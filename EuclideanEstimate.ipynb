{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (\\lambda + 1)} \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "Q^*_{w_n, r_n} &= \\frac{1}{N} \\left(\\frac{2 N}{\\left(\\gamma + \\beta w\\right) \\left(1 + \\lambda\\right)}\\right)^{\\frac{1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(\\beta, \\gamma, Q) \\\\\n",
    "&= -\\beta - \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2^{\\frac{1}{1 + \\lambda}} \\left(1 + \\lambda\\right)^{\\frac{\\lambda}{1 + \\lambda}}}{\\lambda N^\\frac{\\lambda}{1 + \\lambda}} \\sum_n \\left(\\gamma + \\beta w \\right)^{\\frac{\\lambda}{1 + \\lambda}} & (\\lambda > -1, \\lambda \\neq 0)\n",
    "\\end{aligned}\n",
    "$$ Just hit it with a generic convex solver $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": [
     0,
     43,
     50,
     56,
     84,
     92,
     104,
     136,
     163
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** MLE ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(1, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "        factor = (barw - 1) / denom\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "        \n",
    "#         estimate = sum(c*w*r*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n\n",
    "#         missing = max(0, 1 - sum(c*w*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n)\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "\n",
    "class CressieRead:\n",
    "    @staticmethod\n",
    "    def dualobjective(gamma, beta, data, n, fac, scalefac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        scalefaclampow = scalefac ** lampow\n",
    "        dual = -scalefac * gamma - scalefac * beta - (2 * n) / (lam * (1 + lam))\n",
    "        dual += fac * scalefaclampow * sum(c * (gamma + beta * w)**lampow for c, w, _ in data)\n",
    "        return -dual\n",
    "    \n",
    "    @staticmethod\n",
    "    def jacdualobjective(gamma, beta, data, n, fac, scalefac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        scalefaclampow = scalefac ** lampow\n",
    "        j = [-scalefac, -scalefac]\n",
    "        for c, w, _ in data:\n",
    "            dx = fac * scalefaclampow * c * lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            j[0] += dx\n",
    "            j[1] += dx * w\n",
    "            \n",
    "        return -j[0], -j[1]\n",
    "        \n",
    "    @staticmethod\n",
    "    def hessdualobjective(gamma, beta, data, n, fac, scalefac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        scalefaclampow = scalefac ** lampow\n",
    "        h = [ 0, 0, 0 ]\n",
    "        for c, w, _ in data:\n",
    "            dx = fac * scalefaclampow * c *  lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            d2x = fac * scalefaclampow * c * lampow * (lampow - 1) * (gamma + beta * w)**(lampow - 2)\n",
    "            h[0] += d2x \n",
    "            h[1] += d2x * w\n",
    "            h[2] += d2x * w * w\n",
    "            \n",
    "        return [ [ -h[0], -h[1] ], [ -h[1], -h[2] ] ]\n",
    "        \n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, lam, **kwargs):\n",
    "        from cvxopt import matrix, solvers\n",
    "        \n",
    "        rmin = kwargs.pop('rmin', 0)\n",
    "        rmax = kwargs.pop('rmax', 1)\n",
    "        \n",
    "        n = sum(c for c, _, _ in data)\n",
    "        fac = 2**(1/(1+lam)) * ((1+lam)/n)**(lam/(1+lam)) / lam                                           \n",
    "        \n",
    "        scalefac = (2.0 * n) / (1.0 + lam)\n",
    "        \n",
    "        # FOOstarprime = FOOstar / scalefac\n",
    "        \n",
    "        x0 = 1.0, 0.0\n",
    "        \n",
    "        G = matrix([ [ -1.0, -float(w)  ] for w in (wmin, wmax) ])\n",
    "        h = matrix([ 0.0 for w in (wmin, wmax) ])\n",
    "\n",
    "        if False:\n",
    "#         import MLE.MLE\n",
    "#         from numpy import array as arr\n",
    "#         MLE.MLE.gradcheck(f = lambda x: CressieRead.dualobjective(x[0], x[1], data, n, fac, scalefac, lam),\n",
    "#                           jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, fac, scalefac, lam)),\n",
    "#                           x = x0,\n",
    "#                           what='dualobjective',\n",
    "#                           eps = 1e-6)\n",
    "#         MLE.MLE.hesscheck(jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, fac, scalefac, lam)),\n",
    "#                           hess = lambda x: arr(CressieRead.hessdualobjective(x[0], x[1], data, n, fac, scalefac, lam)),\n",
    "#                           x = x0,\n",
    "#                           what='jacdualobjective')\n",
    "            pass\n",
    "        \n",
    "        def F(x=None, z=None):\n",
    "            if x is None: return 0, matrix(x0)\n",
    "            if any(x[0] + x[1] * w <= 0 for _, w, _ in data):\n",
    "                return None\n",
    "            f = CressieRead.dualobjective(x[0], x[1], data, n, fac, scalefac, lam)\n",
    "            jf = CressieRead.jacdualobjective(x[0], x[1], data, n, fac, scalefac, lam)\n",
    "            Df = matrix(jf).T\n",
    "            if z is None: return f, Df\n",
    "            hf = CressieRead.hessdualobjective(x[0], x[1], data, n, fac, scalefac, lam)\n",
    "            H = matrix(hf)\n",
    "            return f, Df, H\n",
    "        \n",
    "        soln = solvers.cp(F, G, h, options={'show_progress': False})\n",
    "        if False:\n",
    "            if soln['status'] != 'optimal':\n",
    "                import sys\n",
    "                print('.', file=sys.stderr)\n",
    "        fstar, (gammastar, betastar) = -soln['primal objective'], soln['x']\n",
    "                \n",
    "        estimate = sum((c/n) * w * r * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, r in data)\n",
    "        missing = max(0, 1 - sum((c/n) * w * 1 * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, _ in data))\n",
    "        \n",
    "        return max(0, min(1, estimate + 0.5 * missing))\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('TwoThirds', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=2/3)),\n",
    "                        ('MinusOneHalf', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=-1/2)),\n",
    "                        ('One', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1)),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=1000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:euc}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:eucsumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:eucsum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases} \\\\\n",
    "g(\\beta^*, \\gamma^*) &= -\\beta^* \\left(-1 + \\frac{1}{N} \\sum_n w_n\\right) + {\\beta^*}^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\ \n",
    "&= \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} - \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} \\\\\n",
    "&= \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:cr}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:crsumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:crsum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (1 + \\lambda)} \\left( \\left( N Q_{w, r} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "0 &= \\gamma + \\beta w - \\frac{2 N}{1 + \\lambda} \\left( N Q_{w, r} \\right)^{-1 - \\lambda} & (c_{w,r} = 1) \\\\\n",
    "\\left( N Q_{w, r} \\right)^{-1 - \\lambda} &= \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\\\\n",
    "Q_{w,r} &= \\frac{1}{N} \\left( \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\right)^{\\frac{-1}{1 + \\lambda}}  & \\left( \\lambda > -1 \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Substitute $\\gamma \\leftarrow \\gamma \\frac{1 + \\lambda}{2 N}$ and $\\beta \\leftarrow \\beta \\frac{1 + \\lambda}{2 N}$ to get\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w, r} &= \\frac{1}{N} \\left(\\gamma + \\beta w\\right)^{\\frac{-1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\sum_n \\left( \\frac{2}{1 + \\lambda} \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} + \\frac{2}{\\lambda (1 + \\lambda)} N Q_{w_n, r_n} \\left( \\gamma + \\beta w_n \\right) \\right) \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda} \\sum_n \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma  - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda}  \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\\\\n",
    "&= - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2 N}{(1 + \\lambda)} \\left( - \\beta - \\gamma +  \\frac{1 + \\lambda}{\\lambda N} \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\right) \\\\\n",
    "&= \\frac{2 N}{(1 + \\lambda)} \\left( -\\frac{1}{\\lambda} - \\beta - \\gamma +  \\frac{1 + \\lambda}{\\lambda N} \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\right) \\\\\n",
    "&= \\frac{2 N}{(1 + \\lambda)} \\left( 1 - \\beta - \\gamma +  \\frac{1}{N} \\sum_n \\frac{1 + \\lambda}{\\lambda} \\left( \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} - 1 \\right) \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Just hit it with a generic convex solver $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CR(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad&  \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{2} - 1 \\right),\\label{eq:crminus2}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:crminus2sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:crminus2sum}\n",
    "\\end{alignat}\n",
    "$$ Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) +  \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^2 - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\left( \\left( N Q_{w, r} \\right)^2 - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "0 &= \\gamma + \\beta w + 2 N^2 Q_{w, r} & (c_{w,r} = 1) \\\\\n",
    "Q_{w,r} &= -\\frac{\\gamma + \\beta w}{2 N^2} \\\\\n",
    "\\end{aligned}\n",
    "$$ does not compute $\\ldots$ $Q = 0$ (?!) $\\ldots$ or $\\gamma + \\beta w = 0$ everywhere and the distribution is whatevs.\n",
    "<!---\n",
    "\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "0 &= \\gamma + \\beta w - 2 N^2 Q_{w, r} & (c_{w,r} = 1) \\\\\n",
    "Q_{w,r} &= \\frac{\\gamma + \\beta w}{2 N^2} \\\\\n",
    "g(\\beta, \\gamma) &= -\\beta - \\gamma - N + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right)^2 \\frac{1}{2 N^2} + \\left( \\gamma + \\beta w \\right)^2 \\frac{1}{4 N^2} \\right)\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "UGH\n",
    "\n",
    "Substitute $\\gamma \\leftarrow \\frac{\\gamma}{2 N}$, $\\beta \\leftarrow \\frac{\\beta}{2 N}$ to get $$\n",
    "\\begin{aligned}\n",
    "Q^*_{w, r} &= \\frac{1}{N} \\left(\\gamma + \\beta w \\right) & (c_{w,r} = 1) \\\\\n",
    "g(\\beta, \\gamma) &= -2 N \\beta - 2 N \\gamma + \\sum_{n} \\left( \\gamma + \\beta w_n \\right)^2 \\left(\\frac{1}{2 N^2} + 1 \\right) \n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -2 N^2 \\beta - 2 N^2 \\beta^2 w_{\\text{sgn}(\\beta)}^2 + \\beta^2 \\frac{2}{N} \\sum_{n} \\left( w_n - w_{\\text{sgn}(\\beta)} \\right)^2 \n",
    "\\end{aligned}\n",
    "$$\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### All on the Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we assign all mass to the sample then we can drop the dual variable constraint and we get $$\n",
    "\\begin{aligned}\n",
    "Q_{w,r} &= \\max\\left\\{ 0, -\\frac{\\gamma + \\beta w}{2 N^2} \\right\\} \\\\\n",
    "g(\\beta, \\gamma) &= -\\beta -\\gamma - N - \\sum_n 1_{\\gamma + \\beta w_n \\leq 0} \\frac{(\\gamma + \\beta w_n)^2}{4 N^2}\n",
    "\\end{aligned}\n",
    "$$ If $\\forall n: w_n < 1$ then we can choose $(\\gamma = -\\beta - \\epsilon)$ with $\\epsilon > 0, \\beta > 0$ s.t. $(-\\epsilon + \\beta (w_n - 1) < 0)$ in which case $g(\\beta, \\gamma) = \\epsilon - N$ which is unbounded.  In this case the primal is infeasible. Similarly if $\\forall n: w_n > 1$ then we can choose $(\\gamma = -\\beta - \\epsilon)$ with $\\epsilon > 0, \\beta < 0$ s.t. $(-\\epsilon + \\beta (w_n - 1) < 0)$ in which case $g(\\beta, \\gamma) = \\epsilon - N$, again unbounded due to primal infeasiblity.\n",
    "\n",
    "We can ensure primal feasibility by creating undata.  Inspired by the Euclidean solution we can place an undata point at $$\n",
    "\\begin{aligned}\n",
    "w_{\\text{fake}} &= \\begin{cases} w_{\\min} & \\frac{1}{N} \\sum_n w_n \\geq 1 \\\\ w_{\\max} & \\text{otherwise} \\end{cases}, \\\\\n",
    "Q_{w,r} &= \\max\\left\\{ 0, -\\frac{\\gamma + \\beta w}{2 (N+1)^2} \\right\\} \\\\\n",
    "g(\\beta, \\gamma) &= -\\beta -\\gamma - (N + 1) - 1_{\\gamma + \\beta w_{\\text{fake}} \\leq 0} \\frac{(\\gamma + \\beta w_{\\text{fake}})^2}{4 (N+1)^2}  - \\sum_n 1_{\\gamma + \\beta w_n \\leq 0} \\frac{(\\gamma + \\beta w_n)^2}{4 (N+1)^2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Idea 1: optimize a lower bound on the dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Equivalent to allowing a signed measure.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(\\beta, \\gamma) &\\geq  -\\beta -\\gamma - (N + 1) - \\frac{(\\gamma + \\beta w_{\\text{fake}})^2}{4 (N+1)^2}  - \\sum_n \\frac{(\\gamma + \\beta w_n)^2}{4 (N+1)^2} \\\\\n",
    "\\end{aligned}\n",
    "$$ Substitute $\\gamma \\leftarrow \\frac{1}{2(N+1)} \\gamma$, $\\beta \\leftarrow \\frac{1}{2(N+1)}$ to get $$\n",
    "\\begin{aligned}\n",
    "Q_{w,r} &= \\max\\left\\{ 0, -\\frac{\\gamma + \\beta w}{N+1} \\right\\} \\\\\n",
    "g(\\beta, \\gamma) &\\geq -2(N+1) \\beta -2(N+1) \\gamma -(N+1) -\\gamma^2 - 2 \\beta \\gamma w_{\\text{fake}} - \\beta^2 w_{\\text{fake}}^2 - \\sum_n \\left( \\gamma^2 + 2 \\beta \\gamma w_n + \\beta^2 w_n^2 \\right) \\\\\n",
    "&= -(N+1) -2(N+1) \\beta -2(N+1) \\gamma - (N+1) \\gamma^2 - 2 \\beta \\gamma \\left(w_{\\text{fake}} + \\sum_n w_n\\right) - \\beta^2 \\left(w_{\\text{fake}}^2 + \\sum_n w_n^2 \\right) \\\\\n",
    "&= (N+1) \\left( -1 -2 \\vec{1}^\\top \\left(\\begin{matrix} \\gamma \\\\ \\beta \\end{matrix} \\right) - \\left(\\begin{matrix} \\gamma \\\\ \\beta \\end{matrix} \\right)^\\top \\left(\\begin{matrix} 1 & \\frac{w_{\\text{fake}} + \\sum_n w_n}{N + 1}  \\\\ \\frac{w_{\\text{fake}} + \\sum_n w_n}{N + 1} & \\frac{w_{\\text{fake}}^2 + \\sum_n w_n^2}{N + 1}  \\end{matrix} \\right) \\left(\\begin{matrix} \\gamma \\\\ \\beta \\end{matrix} \\right) \\right) \\\\\n",
    "&\\doteq (N+1) \\left(-1 - 2 b^\\top x - x^\\top C x \\right) \\\\\n",
    "x^* &= -C^{-1} b \\\\\n",
    "\\left(\\begin{matrix} \\gamma^* \\\\ \\beta^* \\end{matrix} \\right) &^= -\\left(\\begin{matrix} 1 & \\frac{w_{\\text{fake}} + \\sum_n w_n}{N + 1}  \\\\ \\frac{w_{\\text{fake}} + \\sum_n w_n}{N + 1} & \\frac{w_{\\text{fake}}^2 + \\sum_n w_n^2}{N + 1}  \\end{matrix} \\right)^{-1} \\vec{1} \\doteq -\\left(\\begin{matrix} 1 & a  \\\\ a & b  \\end{matrix} \\right)^{-1} \\vec{1} = \\left(\\begin{matrix} \\frac{b - a}{a^2 - b} \\\\ \\frac{1 - a}{a^2 - b} \\end{matrix} \\right) \\\\\n",
    "g(\\beta^*, \\gamma^*) &= (N+1) \\left(-1 - 2 b^\\top x^* - {x^*}^\\top C x^* \\right) = (N+1) \\left(-1 - b^\\top x^*\\right) = (N+1) \\frac{(a-1)^2}{b - a^2} \\\\\n",
    "\\hat{V}(\\pi) &= \\rho \\left(- \\frac{\\gamma^* w_{\\text{fake}}}{N+1} - \\frac{\\beta^* w_{\\text{fake}}^2}{N+1}\\right) - \\gamma^* \\frac{1}{N+1} \\sum_n w_n r_n - \\beta^* \\frac{1}{N+1} \\sum_n w_n^2 r_n\n",
    "\\end{aligned}\n",
    "$$ Note $a^2 - b < 0$ always.\n",
    "\n",
    "<!---\n",
    "Dot[Inverse[{{ -1, -a }, { -a, -b }}], {1,1}] // FullSimplify\n",
    "-1 - Dot[{1, 1}, %] // FullSimplify\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0,
     43,
     50,
     80
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** CrMinus2 ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** Almost MLE ******\n",
      "****** MLE ******\n",
      "****** Lambda Zero ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, minexp=1, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(minexp, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def crminustwo(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        assert n > 0\n",
    "        assert wmax > 1\n",
    "        assert wmin < 1\n",
    "        \n",
    "        sumw = sum(c*w for c, w, _ in data)\n",
    "        sumwsq = sum(c*w*w for c, w, r in data)\n",
    "        sumwr = sum(c*w*r for c, w, r in data)\n",
    "        sumwsqr = sum(c*w*w*r for c, w, r in data)\n",
    "        \n",
    "        wfake = wmax if sumw < n else wmin\n",
    "        \n",
    "        a = (wfake + sumw) / (1 + n)\n",
    "        b = (wfake**2 + sumwsq) / (1 + n)\n",
    "        assert a*a < b\n",
    "        gammastar = (b - a) / (a*a - b)\n",
    "        betastar = (1 - a) / (a*a - b)\n",
    "        gstar = (n + 1) * (a - 1)**2 / (b - a*a)\n",
    "        vhat = (-gammastar * sumwr - betastar * sumwsqr) / (1 + n)\n",
    "        missing = (-gammastar * wfake - betastar * wfake**2) / (1 + n)\n",
    "\n",
    "        if False:\n",
    "            from pprint import pformat\n",
    "            assert False, pformat({\n",
    "                'data': data,\n",
    "                'vhat': vhat,\n",
    "                'missing': missing,\n",
    "                'megamissing': 1 - (-gammastar * sumw - betastar * sumwsq) / (1 + n)\n",
    "                'gammastar': gammastar,\n",
    "                # gammastar = -1 - betastar * a\n",
    "                # (b - a) / (a*a - b) = -1 - a*(a-1)/(a*a - b)\n",
    "                #                     = (-a*a + b - a*a +a)/(a*a - b) \n",
    "                'megagammastar': -1 - betastar * (wfake + sumw) / (1 + n),\n",
    "                'betastar': betastar,\n",
    "                'sumw': sumw,\n",
    "                'n': n,\n",
    "                'sumofone': -gammastar - betastar * (wfake + sumw)/(1+n),\n",
    "                'sumofw': -gammastar * (wfake + sumw)/(1+n) - betastar*(wfake**2 + sumwsq)/(1+n),\n",
    "                'a': a,\n",
    "                'b': b,\n",
    "            })\n",
    "        \n",
    "        return vhat + 0.5 * missing\n",
    "        \n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "from MLE.MLE import CressieRead as CressieRead\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('CrMinus2', Euclidean.crminustwo),\n",
    "                        ('TwoThirds', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=2/3)[0]),\n",
    "                        ('MinusOneHalf', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=-1/2)[0]),\n",
    "                        ('One', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=1)[0]),\n",
    "                        ('Almost MLE', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=1e-3)[0]),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                        ('Lambda Zero', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=0)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=1000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "skip = { 'Almost MLE': 1, 'MinusOneHalf': 1, 'TwoThirds': 1, 'One': 1, 'Lambda Zero': 1, }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    if name in skip:\n",
    "        continue\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

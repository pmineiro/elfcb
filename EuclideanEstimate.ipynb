{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (\\lambda + 1)} \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "Q^*_{w_n, r_n} &= \\frac{1}{N} \\left(\\frac{2 N}{\\left(\\gamma + \\beta w\\right) \\left(1 + \\lambda\\right)}\\right)^{\\frac{1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(\\beta, \\gamma, Q) \\\\\n",
    "&= -\\beta - \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2^{\\frac{1}{1 + \\lambda}} \\left(1 + \\lambda\\right)^{\\frac{\\lambda}{1 + \\lambda}}}{\\lambda N^\\frac{\\lambda}{1 + \\lambda}} \\sum_n \\left(\\gamma + \\beta w \\right)^{\\frac{\\lambda}{1 + \\lambda}} & (\\lambda > -1, \\lambda \\neq 0)\n",
    "\\end{aligned}\n",
    "$$ Substitute $\\gamma \\leftarrow \\gamma \\frac{1 + \\lambda}{2 N}$ and $\\beta \\leftarrow \\beta \\frac{1 + \\lambda}{2 N}$ to get $$\n",
    "\\begin{aligned}\n",
    "Q^*_{w_n, r_n} &= \\frac{1}{N} \\left(\\gamma + \\beta w\\right)^{\\frac{-1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda} \\sum_n \\left(\\gamma + \\beta w \\right)^{\\frac{\\lambda}{1 + \\lambda}} & (\\lambda > -1, \\lambda \\neq 0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Just hit it with a generic convex solver $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     43,
     50,
     56,
     130,
     157
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** MLE ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(1, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "        factor = (barw - 1) / denom\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "        \n",
    "#         estimate = sum(c*w*r*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n\n",
    "#         missing = max(0, 1 - sum(c*w*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n)\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "\n",
    "class CressieRead:\n",
    "    @staticmethod\n",
    "    def dualobjective(gamma, beta, data, n, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        dual = (-2 / (1 + lam)) * (gamma + beta + 1 / lam) \n",
    "        dual += (2 / lam) * sum((c/n) * (gamma + beta * w)**lampow for c, w, _ in data)\n",
    "        return -dual\n",
    "    \n",
    "    @staticmethod\n",
    "    def jacdualobjective(gamma, beta, data, n, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        j = [ -2 / (1 + lam), -2 / (1 + lam) ]\n",
    "        for c, w, _ in data:\n",
    "            dx = (2 / lam) * (c/n) * lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            j[0] += dx\n",
    "            j[1] += dx * w\n",
    "            \n",
    "        return -j[0], -j[1]\n",
    "        \n",
    "    @staticmethod\n",
    "    def hessdualobjective(gamma, beta, data, n, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        h = [ 0, 0, 0 ]\n",
    "        for c, w, _ in data:\n",
    "            dx = (2 / lam) * (c/n) *  lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            d2x = (2 / lam) * (c/n) * lampow * (lampow - 1) * (gamma + beta * w)**(lampow - 2)\n",
    "            h[0] += d2x \n",
    "            h[1] += d2x * w\n",
    "            h[2] += d2x * w * w\n",
    "            \n",
    "        return [ [ -h[0], -h[1] ], [ -h[1], -h[2] ] ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, lam, **kwargs):\n",
    "        from cvxopt import matrix, solvers\n",
    "        \n",
    "        rmin = kwargs.pop('rmin', 0)\n",
    "        rmax = kwargs.pop('rmax', 1)\n",
    "        \n",
    "        n = sum(c for c, _, _ in data)\n",
    "        assert n > 0\n",
    "        \n",
    "        x0 = 1.0, 0.0\n",
    "        \n",
    "        G = matrix([ [ -1.0, -float(w)  ] for w in (wmin, wmax) ])\n",
    "        h = matrix([ 0.0 for w in (wmin, wmax) ])\n",
    "\n",
    "        if False:\n",
    "#             import MLE.MLE\n",
    "#             from numpy import array as arr\n",
    "#             MLE.MLE.gradcheck(f = lambda x: CressieRead.dualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam),\n",
    "#                               jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='dualobjective',\n",
    "#                               eps = 1e-6)\n",
    "#             MLE.MLE.hesscheck(jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)),\n",
    "#                               hess = lambda x: arr(CressieRead.hessdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='jacdualobjective')\n",
    "            pass\n",
    "        \n",
    "        def F(x=None, z=None):\n",
    "            if x is None: return 0, matrix(x0)\n",
    "            if any(x[0] + x[1] * w <= 0 for _, w, _ in data):\n",
    "                return None\n",
    "            f = CressieRead.dualobjective(x[0], x[1], data, n, lam)\n",
    "            jf = CressieRead.jacdualobjective(x[0], x[1], data, n, lam)\n",
    "            Df = matrix(jf).T\n",
    "            if z is None: return f, Df\n",
    "            hf = CressieRead.hessdualobjective(x[0], x[1], data, n, lam)\n",
    "            H = z[0] * matrix(hf)\n",
    "            return f, Df, H\n",
    "        \n",
    "        soln = solvers.cp(F, G, h, options={'show_progress': False })\n",
    "        if False:\n",
    "            if soln['status'] != 'optimal':\n",
    "                import sys\n",
    "                print('.', file=sys.stderr, end='')\n",
    "        fstar, (gammastar, betastar) = -n * soln['primal objective'], soln['x']\n",
    "                \n",
    "        estimate = sum((c/n) * w * r * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, r in data)\n",
    "        missing = max(0, 1 - sum((c/n) * w * 1 * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, _ in data))\n",
    "        \n",
    "        return max(0, min(1, estimate + 0.5 * missing))\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('TwoThirds', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=2/3)),\n",
    "                        ('MinusOneHalf', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=-1/2)),\n",
    "                        ('One', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1)),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=1000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEdCAYAAABOl2PPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3xUVf7/8de5d3pJJpUEAknovYmKKwr2smJdcX8qfnVX7Kugu666KrYFUfmKul91XVFc17qrW+wKCKig9F4DJCG91+lzz++PiRFYSggJk5DzfDzmkcydO/d+bsS8c86ce46QUqIoiqIosaLFugBFURSla1NBpCiKosSUCiJFURQlplQQKYqiKDGlgkhRFEWJKRVEiqIoSkyZYl1AZ5OcnCyzsrJiXYaiKEqnsmrVqgopZcqBXlNBdISysrJYuXJlrMtQFEXpVIQQeQd7TXXNKYqiKDGlgqiFhBAThRCv1NbWxroURVGU44oKohaSUn4kpbwpPj4+1qUoiqIcV1QQKYqiKDGlBisoitLmDMOgoqKCmpoaIpFIrMtRjhGbzUZGRgZms/mI3qeCSFGUNldQUIAQgqysLMxmM0KIWJektDMpJZWVlRQUFJCdnX1E71Vdc4qitLnGxkZ69OiBxWJRIdRFCCFISkrC7/cf8XtVECmK0i40Tf166Wpa+0eH+pdyDH0xZxY7F30a6zIURVE6FBVEx1BVtZ9v565k54KPYl2KoihtaNGiRWRkZDQ/HzJkCIsWLWrRvooKomMmUFkG20tABvh23hp2fvXvWJekKF1WVlYWdrsdl8vV/Ljjjjva7PibNm1iwoQJbXa8450aNddCQoiJwMS+ffu26v0fvfUOVfFX4q7bCHoZ376xFiNi0O/8y9q2UEVRWuSjjz7i7LPPjnUZCqpF1GJHO7PCL+68C5/5K+rjhkIkFQiw7G/ryfn8w7YtVFGUVnvkkUe49tprm5/n5uYihCAcDgNQVVXFDTfcQPfu3UlISODSSy894HGysrKYP38+AD6fj+uvv56EhAQGDx7MihUr9tm3qKiIK664gpSUFLKzs3n++eebX1u+fDmnnHIKHo+H9PR07rjjDoLBYPPrQghefvll+vXrh8fj4fbbb0dK2WY/j2NFBdEx1CPRRqNYQH3cULRIGoIgS/+2gR2f/CPWpSmK0gKTJ0/G6/WyadMmysrKmDZt2mHf8+ijj7Jz50527tzJF198wRtvvNH8mmEYTJw4kREjRlBYWMiCBQuYM2cOX3zxBQC6rvPss89SUVHBsmXLWLBgAS+++OI+x//4449ZsWIF69ev5/33329+b2eiuuaOISkgqbEbFa6FEH8mcbWAXsKydzYhIxH6X3xVrEtUlHbx6Eeb2FxU167nGNw9jukTh7R4/0svvRST6adfgU8//fQh9y8uLuazzz6jsrKShIQEAMaPH3/Y87z//vu8+OKLJCYmkpiYyJ133sljjz0GwIoVKygvL+fhhx8GoHfv3kyZMoV3332X8847jxNOOKH5OFlZWdx8880sXryYqVOnNm+/77778Hg8eDwezjjjDNauXcv555/f4p9DR6BaRMfQuCk3ErbWk9yQSr2+kLr4oYhIGkL6+f69rWz751uxLlFRuox//etf1NTUND+mTJlyyP337NlDYmJicwi1VFFRET179mx+npmZ2fx9Xl4eRUVFzUHi8XiYMWMGpaWlAGzfvp2LLrqItLQ04uLieOCBB6ioqNjn+Glpac3fOxwOGhoajqi+jkC1iI6hXj16cOFjN/Dpw6+TUp9KuftriD+DuFoQWhHL/5EDkTcZ8IvJsS5VUdrUkbRUYsnpdOL1epufl5SUNH/fs2dPqqqqqKmpwePxtPiY6enp7NmzhyFDoj+D/Pz8fY6ZnZ3Njh07DvjeW2+9lVGjRvHOO+/gdruZM2cO//jH8deVr1pEx1jP9O78/LEbCNvqSKlPoa6pZYTRA2Qjy/+5k63vv3H4AymK0uZGjhzJkiVLyM/Pp7a2lpkzZza/lp6ezgUXXMBtt91GdXU1oVCIJUuWHPaYkyZNYubMmVRXV1NQUMALL7zQ/NpJJ52E2+1m1qxZ+Hw+IpEIGzdubB7QUF9fT1xcHC6Xi61bt/LSSy+1/UV3ACqIYiAjvTsTH/s1YVs9qfWp1OlfUxc/BGRPkD5W/Hs32959LdZlKspxbeLEifvcR3TZZZdxzjnncNVVVzF8+HBOOOEELrroon3e8+abb2I2mxk4cCCpqanMmTPnsOeZPn06mZmZZGdnc+655zJ58k89Hrqu8/HHH7N27Vqys7NJTk7mxhtv5McFOJ955hnefvtt3G43U6ZM4aqrjs/PkUVnHOoXS2PGjJErV65sk2MVl5Twn4fnYvK7KYsrJy58BvE1m0DkIzUnJ16YwcBrbmyTcynKsbRlyxYGDRoU6zKUGDjYf3shxCop5ZgDvUe1iGIoPS2NS564kbC9jtS6FOpMX1PrGQKyF8JoYMWnBWz96yuxLlNRFKVdqSCKsbTUblz6xE3NYVRrXtQURpkIo54VXxSx5bUXD38gRVGUTkoFUQfQLSWVS5+4iZC9jm61ydQ0h1EWQtaxcn4ZW179v1iXqSiK0i5UEHUQ3VJSuWLGLYQcdaTVJlNjaQojIxtN1rFyYTmb//xcrMtUFEVpcyqIOpCUpGR+8cdbCTvqSKvZK4xkNpqsYfXiaja99Gysy1QURWlTKog6mOSkJK7YK4yqLYupiR8Esi/CqGHNN7VsfOHQU5EoiqJ0JiqIWkgIMVEI8cqP4/vbU3JSEr+YeRthRx3pNUlUW7+hJn4gyL5okSrWLmtg43Oz2r0ORVGUY0EFUQsd7TIQRyopIZErn7x93zDyDETSPxpGP3jZ+L8zjkktiqIo7UkFUQeW6Elg0qw7CDujYVTVHEYD0COVrF0ZYMMzT8S6TEVRjoDf70cIQUFBwQFfnzt3LhMnTjyiY44dO5a//e1vbVFeTKgg6uAS4j1cNes3hJ11dK9Josr2LTWegRgMRA9Xsm51iA2zHo11mYrSaew9rY+mafssGf7WW0c/A/706dObj2ez2TCZTM3P917W4WB+/etf89FHHx11HZ2JCqJOwBMXz1Wz7oyGUXUiVbZoy8gQA9HD5axbZ7D+j9M75cqMinKsNTQ0ND969erFRx991Pz8mmuuOerjP/roo83HmzNnDhMmTGh+vmrVqqM69o8rxR5vVBB1Ep64OP7fU3cRctXRvTqJSntTy0gMRg+Vs34TrH/iIRVGinIUGhoasNls1NVFF/F76KGHsFqt+Hw+AH73u99x3333AdFlw6+++urmJb6feuqpI/r/79NPP6VPnz4kJCTss9Lryy+/zNlnnw381I330ksv0adPH4YOHQrAJ5980rw8+N13373Pcbdu3cq4ceOIj48nJSWF6667rvU/kGNEBVEnEud2c83TUwm56uhRlfhTGGmDMQXL2LBFZ/3D9yGP07+aFKW9uVwuhg8fzjfffAPA4sWLycjI4Pvvv29+/uOqrLfccguhUIjdu3fz1Vdf8dJLL/H222+3+Fyff/45a9asYfXq1bz++ussWrTooPt+/PHHrFq1ijVr1lBcXMykSZOYPXs25eXlpKSksPdEzPfffz+XXnopNTU15Ofnc/PNN7fiJ3FsqYXxOhm308U1T0/l7Xvn0KMqkcLEb4FxJNSAObiRDTvTEPffzdA/PoNmscS6XEWJ+uw+KNnQvudIGwYXPHnUhxk/fjyLFy/m7LPPZseOHdxzzz0sXryYMWPGsH79ek499VQCgQAffPABO3bswOVy0bdvX6ZOncqbb77Z4u69Bx54gLi4OOLi4jj99NNZu3YtEyZMOOC+f/jDH5oX43vzzTc58cQTufjiiwG49957efbZn250N5vN5ObmUlJSQnp6OqeeeurR/UCOAdUi6oTcThdXPzWVsLuWHlWJVNi/pdozkIg+FHOgiHVFyay/5w6MvVaaVBSlZcaPH8+iRYv44YcfGDNmDGeeeSaLFy/mu+++Y9iwYcTFxVFSUoJhGPTq1av5fZmZmRQWFrb4PEeyxPfeS43vv/S4ruv06NGj+fmzzz6L1+tl1KhRDB8+vFOMplMtok4q2jK6m7/d+ywZVYkUJH4HnEpCjcDhW8mGqt4w7TaGzZqDfgTLGitKu2iDlsqxMm7cONatW8cnn3zC+PHjGTlyJFu3buXLL79s7pZLS0tD0zTy8/Pp3bs3EF0CfO9AaEtCiObv09PT9+nGMwxjnwDs0aMHr732GlJKFi9ezLnnnsvpp5++T2h2NKpF1Im5HE4mP303obg6MqoSKLd/R7VnAH7zycQ3bGZ9Q1/WT7uNUFlZrEtVlE7D4/EwZMgQXnrpJcaPH4+maYwZM4ZXX321OYisViuXXXYZDzzwAI2NjezcuZPnnnuOa6+9tt3ru/jii1mxYgUff/wxoVCIp59+mqqqqubX33vvPYqKihBCNHfn6bre7nUdDRVEnZzT7uC6p+8hHFdHz6oESh3LqPH0oc5xJinVq1kfHMKGe35D8CA3zymK8t/Gjx+PlJLRo0c3P29sbGTcuHHN+/z5z38Gol1yZ555JjfeeGObDP8+nPT0dN59912mTp1KSkoKpaWljBnz08Kny5Yt44QTTsDlcnHllVfyyiuvtFtLra2opcKPUFsuFd6WfD4ff713NqbaOPKSaunmHYu7fg/dq/5JYdIpDDVWMfTRmVj79491qUoXoJYK77rUUuFdmN1u57qn7iGUUEdmZTzFjh+oc/egMOkKMkuXsFGMYeMffo9v3bpYl6ooirIPFUTHEbvdzvVP/Y5ASj3ZlXEU21dS70ojN+0aehcvYKPlRDY+Pp3Gpd/FulRFUZRmKoiOMzarlV8/+Vt86Y30rnZTYl9DvSuZnIxf0W/PfLaaT2TTM09R/9XnsS5VURQFUEF0XLKardz0xG9p7Oknu9pJmWUt9U4PW7Jvol/+V2y3nMCWl16i9sO/x7pURVEUFUTHK4vZzG2P3E1D7wCZtU7KLOtpcLjY2O82+uUtIMc0ki1vvkX1m6+r+ekURYkpFUTHMZPJxB0P3U1j/wiZdQ4q9I002O2sH3gnffMXk6sPZeuH/6bq5T+p+ekURYmZLhlEQogEIcTHQojtQoh1QogvhRB9Y11Xe9A1nd/cfxcNg6Fno4NqfTMNdhNrBt9F9p6lFNCfrV8upOK5ZzCCwViXqyhKF9QlgwiQwBwpZX8p5QjgY+DVGNfUbjRNY+rvfkPjcBM9vHZqxTYa7II1Q++iV8EqSo1Mtn23gvKn/ojR2BjrchVF6WI6RBAJITKEEC8IIZYJIbxCCCmEyDrIvj2FEP8QQtQKIeqEEB8KIY5oEiUpZY2Ucv5em5YCBzzf8UIIwdSpt9I42kqa306D3E6DPcLqEXfRo3gz1aFUtq3aTNnMR4jU1MS6XEWJuVtuuYXHH3881mXE3IQJE3j11Z/+Tn/wwQdJTk7eZ9LWo9UhggjoC0wCqoFvDraTEMIBLAQGAv8DTAb6AV8LIZxHcf6pwL+P4v2dghCCaXfcjO8kB6lBO75IDvX2IKtG3ElyaS6N/ji2bc6j7ImH1fx0ynEtKysLi8VCRUXFPttHjRqFEILc3FxefvllHnrooXavZenSpZx55pm43W7i4+OZOHEimzdvbpNjL1q0iIyMjP/avn+4tFR+fj6zZ89m8+bNlJSUtEWJQMcJoiVSym5SyguBQ40pngL0Bi6VUv5LSvlv4GIgE2he/UkIMV8IUXGQxz6Lcwghpjcd8/42v6oOatrNvyZ4ajyJYTvB0C7q7F7WjPwNnqpSQvUmtu4qo/yJhwgWtHxKe0XpbLKzs3nnnXean2/YsAHvMV46ZdmyZZx77rlccsklFBUVsXv3bkaMGMGpp57Krl27jmktLZGfn09SUhKpqaltetwOEURSSqOFu14MfC+lzNnrvbuB74BL9tp2tpQy+SCP5mkFhBAPAhcCF0gpu9TiPXfdcB2R05PwSAdGMJcaex2rR96Ovb4RURNka0Ed5U/8gUBOzuEPpiid0OTJk/nrX//a/PyNN97YZ1nt66+/ngcffBD4qWUxe/ZsUlNTSU9P5/XXX2/ed/8Wxrx585onSJVSMm3aNFJTU4mLi2PYsGFs3LgRiC5qd91113HXXXfhdrtJTEzkiSeeYOzYsTzyyCMtOncgEOC3v/0tvXr1olu3btxyyy3NS5u3RHV1NRdddBEpKSkkJCRw0UUXUXCASZLnz5/POeecQ1FRES6Xi+uvv77F5zicDhFER2AIsPEA2zcBg4/kQE0toYnAuVLK2jaordO5c/LVcEYqThwI/x6q7dWsGXErJh9YK+rYUhaifOZ0fBvaeWVNRYmBsWPHUldXx5YtW4hEIrz77ruHXMahpKSE2tpaCgsLmTt3LrfffjvV1dWHPc+XX37JkiVL2L59O7W1tbz//vskJSXh9XpZunQpV1555X+9Z9KkSXz11VctOvd9993H9u3bWbt2LTk5ORQWFvLYY4+1+OdgGAY33HADeXl55OfnY7fbueOOO/5rv7PPPpvPPvuM7t2709DQwLx581p8jsPpbAvjJRL9HGl/VUBCSw8ihBgCPALsBBY3LToVPtjMsEKIm4CbgA69uFRr3PHLSbyof4jt6yKC3iKqnAZrh9/MiI2v4i4tZbNMY9DsJ0i89R6cJ58U63KVTmrW8llsrdrarucYmDiQ35/0+yN6z4+tovHjxzNo0KBDLpdgNpt5+OGHMZlMXHjhhbhcLrZt28bYsWMPeQ6z2Ux9fT1bt27lpJNOap6ZuqCgAMMwSE9P/6/3pKen7/P51cHOffLJJ/PKK6+wfv16EhMTgegS5FdffTUzZ84Eoiu6evZbHLOhoaE5dJOSkrjiiiuaX/vDH/7AGWeccchramudLYjahJRyEyAOu+NP+78CvALRZSDaq65Yue3Ky3nZ9B+MBfnQWEKFy2DdsJsYtvE1Eovy2EI2g/40i3DJdcRNnIjQOltDWlEObPLkyZx++uns3r17n265A0lKSsJk+ulX5uGW9/7RmWeeyR133MHtt99OXl4el19+Oc888wwJCQlomkZxcTEDBw7c5z3FxcUkJycf9tzl5eV4vV5OOOGE5teklEQikebn3bt3/6+utgkTJjR/7/V6mTZtGp9//nlzK6u+vp5IJHLMFtTrbEFUzYFbPgdrKSktdMtlF/MX0yfUfLUbZ305ZXEGG4b+iiGb3yBlz3Y2Zwxg8DuvEMjZQdKNU9Dj42NdstKJHGlL5VjJzMwkOzubTz/9lLlz57b6OE6nc5+BDvuPKLvzzju58847KSsrY9KkSTz99NM8/vjjnHLKKfz973//rxbI+++/z1lnnXXY8yYnJ2O329m0aVOrF7+bPXs227Zt44cffiAtLY21a9cyatSoYzr1V2f703YT0c+J9jcYaJvxjl3YlIk/J+mCvkScNuLqKilx7mHT4Oupcg8kLX8TawK9aPjuM0oeeQj/tm2xLldR2sTcuXNZuHAhTmfr7wAZOXIkH374IV6vl5ycnH1CbcWKFfzwww+EQiGcTic2mw2tqVfhySef5I033uD555+nvr6e6upqHnzwQZYtW8b06dMPe15N05gyZQrTpk2jrOmWi8LCQr744osW115fX4/dbsfj8VBVVcWjjz56hFd/9DpbEP0HGCuE6P3jhqYbX09teq3dCCEmCiFeqa09vsc1/OqC80m/cABBl42E2iqKXLlsHXAdZYkj6L1zKevDg6jYvZ3ymY9Q+8knasJUpdPr06fPPkttt8a0adOwWCx069aN//mf/9lnyfC6ujqmTJlCQkICmZmZJCUl8bvf/Q6AcePG8cUXX/Dhhx+Snp5OZmYma9as4dtvv6Vfv34tOvesWbPo27cvY8eOJS4ujrPPPpttR/CH4tSpU/H5fCQnJzN27FjOP//8I7v4NtBhlgoXQvyi6duzgFuA24ByoFxKubhpHyewDvABDxKdqudxwA0Ml1IevsP2KHXUpcLb2lsLv2b3J1uw1/ko97jpWd+XrMIPyNqxkMIeo7HER+hjbcB60lkk3Xgjutsd65KVDkQtFd51tWap8I70GdH+N7K+2PR1MTABQErZKIQ4E3gWeJPogIMFwNRjEUJdyTVnnsG7ms72TzaSWtPA7oRtwBWUJ/Zm9Kq38DfEs6bHIIZ+8zHhgjwSb74NW//+sS5bUZROqMN0zUkpxUEeE/bbL19KeYWUMk5K6ZZSXiqlzI1N1ce3X044nUGXDKcuwUZmpZ+1iauocwxn8bj7kFjonfMd68ODqNy1lfKZj1L32Weqq05RlCPWYYKoo+sqnxHt78px4xh+2SjKU6yMLrOy3bGKRovG8lG/Jb/Xz+i3fQHFdUnsqo1Q+9cXqXjhBSINagZvRVFaTgVRC0kpP5JS3hTfBYctX37KKYy5bAz5aWb6+lyEgrvJj9vJ7owrWXHyFNJKt2IrLGVNoAf1i/9D6aMPE9i5M9ZlK4rSSaggUlrk4pNP4uqbLqSwp47Z5iSr3M/axJXUOYZGu+qEjeycZawPDaQyZxNlM6ZT99VXqqtOUZTDUkGktNiwXpk8ds+v8A2PozzZxuhyG9scq2mwwoqRd5OfOY5+2xdSXJfM7qoQta//iYoXX1SL7SmKckgqiJQjYrdYefRX1zLw4iHkpZnp53MRCeSRF5dDbo8rWD72JrqVbcVaVM7aQDoNC/9FyWOPENi9O9alK4rSQakgaqGuOljhYH552mn86vZLKcgyYbI5yS4PsCZxBfX2wSw59X4MYSdrx/esCw2gcvsGyv44nfqFC2NdtqIoHZAKohbqyoMVDqZ/Wjoz7r6R8BgP5Sk2Tii3s92xmnprhJUjp5KXeRr9diykuC6F3Eo/Na8+R8Wf/4xxjBcfUxSlY1NBpBwVi8nEg9f+P0ZcMYrcdBN9fC4Mfx658TvI63EZP5x8C93KtmIprmBdIJ36rz6g9PFHCeblxbp0pYubN28ew4YNw+FwkJaWxq233kpNTU2sy+qSVBApbeKyk0/m1juvpKiPGZPDRe/yYFNX3QCWnHo/EeEgc8f3rAv1p3LresqemE79okWxLlvpombPns3vf/97nn76aWpra/n+++/Jy8vjnHPOIRgMxrq8LkcFkdJmspJTmDVtCmJsCmUpVkaX28mxr6HOGmL1iKnkZo2n3/aFFNYns7uikZq/zKHilb9g+P2xLl3pQurq6pg+fTovvPAC559/PmazmaysLN5//31yc3P529/+xiOPPMKkSZO47rrrcLvdDBkyhL3nmCwqKuKKK64gJSWF7Oxsnn/++RheUeengkhpU5qm8furfsEpvzyZ3B4megdc4N/Drvht5He/hB9Ovp200m1YiqtY5+tGw5d/j3bV7bdwl6K0l6VLl+L3+7n88sv32e5yubjwwgubl+j+z3/+wy9/+Utqamq4+OKLm5fPNgyDiRMnMmLECAoLC1mwYAFz5sw5oqUXlH11pElPOzQhxERgYt++fWNdSqdw/qjRjMjO5tl5fychT6dPeYA1acsZVj2SJafez+h1r5O58wfWZp5Kv82riTw+Hc+11+M67bRYl660g5IZMwhsad+lwq2DBpL2wAOH3a+iooLk5OR9Vjz9UXp6OqtWrWLAgAGMGzeOCy+8EIiu5Dpnzhwgur5QeXk5Dz/8MAC9e/dmypQpvPvuu5x33nlteEVdh2oRtZAaNXfk0j0JPDX1Jmyndac01cLocgc7bWupswVYM+JOcjPPpN+OrymoT6WwtJLqv7xA5WuvYwQCsS5dOY4lJydTUVFBOBz+r9f2XqI7LS2tebvD4cDv9xMOh8nLy6OoqAiPx9P8mDFjBqWlpcfsGo43qkWktLtpl1/CooEb+fTvi8kuc+ELF7AzoRzBRZQl9uWE1W/gb3Sxi2R6f/Uhwbw8km+agrmVSx8rHU9LWirHyimnnILVauXDDz9k0qRJzdsbGhr47LPPmDFjBgWH6Cru2bMn2dnZ7Nix41iU2yWoFpFyTEwYPJTf330dpYNsCIeTvqURVicsp97Rm29+dj8RnHhyc9hWH0do87eUzvgjDd99F+uyleNQfHw806dP5ze/+Q2ff/45oVCI3NxcJk2aREZGBpMnTz7k+0866STcbjezZs3C5/MRiUTYuHEjK1asOEZXcPxRQaQcM0luN0/ecSNxZ2VS0s3CCRVOdlnXUmvzsXrEbyhPHUr3XWvZUZNIuGgz1a+/SuW8eRhqOK3Sxu69915mzJjBb3/7W+Li4jj55JPp2bMnCxYswGq1HvK9uq7z8ccfs3btWrKzs0lOTubGG29EzbrSeh1mqfDOoqssFd7elm3fzj/f+4r0EkljJIDXFU+Pht70qPiI/hs/pyopi9REL/akHpiyB5B8002Y09NjXbbSQmqp8K6rNUuFqxaREhOn9O/PH+65gYqhTjSHg8TaOnZ4NlGYPJGVP7uahKo91BeGqC8tJLx9DaVPzqLx++9jXbaiKO1ABVELqUlP2168w8Efb72Bbuf3oz7JSnZlmHWJq6i3nMqiM27FaniJ7KmhsrQWWbyFqtdfp+qvbyJVV52iHFdUELWQGr7dfn597tmcfvVplKRZGFJlZVXiD0Rkf7467bcYHhuWghLKiv3Isq00frOI0llPEVJDZRXluKGCSOkQzho+nKunXERhdxND6t1siFuBNBJYMPI+/P1TcZSXUbkniCzeRGj3NkpnzKRx+fJYl60oShtQQaR0GMN6ZTL1zl9SkqHTJ+hht209Qc1gScbvqTwlG2tjDTV5AcIFW5FVhVS99jpVb72luuoUpZNTQaR0KN09iUy/6waqsy2kigQqtRxqrVWssU5j1zmjsZgCePd48efvgpp8Gpd8Q+nTzxAqLYt16YqitJIKIqXDcdlszLzjV0SGuHBYE4hESihw7yYv+GuWn3Ue1pQI4VIvDbuKkeXbCeXtonTmTLyrVsW6dEVRWkEFkdIhaZrGIzdcS8LYbhjOOJyBBrYmrKe+YSKfnXQ9rkEhqPPRsLMCo3ATsr6Kyr+8SvU77yBDoViXryjKEVBBpHRoUy+7hIHn9cfndtHNJ1iTtByt4STezf4dCQcrVL8AACAASURBVKcFIWLQuLuO8O6N4KumYfESSp9+mnB5eaxLVxSlhVQQtZC6jyh2rp0wgbMmnUx9nI3MgJuVSUux+rJ50/FH3OdFEC4df5GXwI4tyNpCQvl7ol11q9fEunSlg8rKysJisVBRUbHP9lGjRiGEIDc3l+uvv54HH3zwgO8XQuB0OnG5XM2Pp5566liUflw66iASQvQVQpwihOjfFgV1VOo+otg6e8QIrv71edTGmcgOJbMuYQVa2Mn7gScxzrZh7qURqgri27YLWbodo7GByldeofq995EHmO5fUbKzs3nnnXean2/YsAGv19vi969bt46Ghobmx7333tseZXYJrQoiIYRJCPGwEKIU2AZ8C9y31+vXCCGWCiGGtlGdisKIzCym3vEL6pI0MiIp5Lg3ENSDzK95jMKTeuEZHSTiM/DuKCaStwHCARq+/pqyZ2YTrq6OdflKBzN58mT++te/Nj9/4403uO6662JYUdd1xEEkhDABnwLTAQ+wBRD77fYdMBa44mgLVJS9ZSQmMf2ua/H30EkhlRLbLirtpWyouIdvMyfQ86xqwroZX14NoR3rkP5agrm5lM6YiX9r+64QqnQuY8eOpa6uji1bthCJRHj33Xe59tprY11Wl9SahfHuAM4G5gP/I6UsFkIYe+8gpcwVQuQA5wKPHn2ZivITt83BjDuu5/E33sa9LRmfqZy8OC+i+hreikvn1+e/yLZvswmUeon4NmLt0xsDKH/+BeIvnoj7vPMQYv+/nZT29M3726nY09Cu50ju6eK0SUf2CcGPraLx48czaNAgehzBYoyjR49G0376W/69995TS4W3Umu65iYDlcAkKWXxIfbbAvRsVVWKchi6pvPIDZNJOikRs5aAWQTYlLSGSN2ZzAk/Qv+z8tH6OQjVhfBt2oFRtBXCIWr/9W8q//xnDJ8v1pegdACTJ0/m7bffZt68eUfcLbd69WpqamqaHyqEWq81LaIBwCIpZc1h9qsHUlpxfEVpsWmXX8bfEhaycf4uksN+VqYs44Tyk5ltPMPtox6kLjmOuuUC744SrA0NmLMH4Vu7jtKiJ0m++Sa1HPkxcqQtlWMlMzOT7OxsPv30U+bOnRvrcrqs1rSIJGAcdi/oDvhbcXxFOSLXnnEmZ105Gr/VSq9gEsu7LcXh78ZLNc8S6a7T/bx6Qk4HgaIG/BvXIutLCJeVUfrU03jV8s5d3ty5c1m4cCFOp/O/XotEIvj9/uZHUM1r2C5aE0S7gRFCiIO+VwhhB4YT7Z5TlHZ3zsjRXPOrs2h0amT501mdsgLdsPCPmhlst/VnyAW7CGd6CNeG8G3YjlG8FenzUjn3NTXEu4vr06cPY8YccOFQnnzySex2e/PjzDPPbH5txIgR+9xHNHXq1GNV8nGnNV1z/wHuB+4Bnj7IPvcCCcC/W1mXohyxEVm9SbrjMp575Z/0qunOVs8Gsuv7sqz6t2xxfc51Y18jLy2LxpURvNtLsNY3Ys4eRMPXXxPMzyN5yhR0jyfWl6EcA7m5uQfcbjKZkFICMG/ePObNm3fA/X7cR2kbrWkR/S9QAjwphHhbCHF50/ZkIcQFQojXgIeBfODFNqpTUVokIzGFh6deSyhdkhZOp8iVS6F7Nw31E5lZ/xQJPWvoeV4NYZeDQFE9/o1rkPWlBHdGJ071b98e60tQlC7niINISlkFnA/kAb8E/k70c6OfAx8D1wMFwEQpZX2bVRpjaoqfzsNtc/DH39yAqb8JTyQVk+ZnRepSXL5s/lz7HFvMwxhy/k6MrLimrrptGCXbidRUU/7c89TPnx/rS1CULqVVMytIKTcAg4HbgU+Ifha0HVhAtMtusJRyY1sV2RGoKX46F13TeeRX15F8UjxSxNMrkMLybksJaQGW197DX3x3Mujk7bjGaoQjAu+2IkI71oK/gZp/fEDFX/6CEQjE+jIUpUto9VxzUkq/lPIlKeXFUsqhUspBUspzpZTPSikb27JIRWmtaZdfzsgLe+OzaWT5e1Lk2k1OwkbCDRN4svY5rD3CZJ1fuV9XXRm+VaspffJJQiUlsb4ERTnuqdm3lePeVaeN56bbLsSfHCY+0g2n1FmatgRHMIU3a2bzvel0hl6QA9lOwrVBfBu2YpTsIFxUROmTs9Qs3q2kPtDvelr737w1c81ZhBCpQgjbfttdQognhBAfCSFeEEKoWRWUDqNftx7MnPYr3CPtGJqD3v4MVqQsp95axdbam3mh8QH6nZRL/ClGU1ddIaGcNcj6aipfeYWaDz5AGi25fU4BMJvN+NTsFV1OKBTCZDrywditaRE9BBQDo37c0HRP0RKiw7p/TvSzo2VCiKRWHF9R2oVZN3P/L3/JhKtG0ugMkxXoSZW9jPXJK9AbT+SZ6j8RSneSfUE5EbedQGFTV11DOfVfzaf8ueeJ1B8342/aVWpqKoWFhXi9XtUy6iIMw6C0tJTWfI4ujvQfiRBiKZAhpey117YriI6e2wA8RzSMLgMeklL+8Yir6sDGjBkjV65cGesylKNUWlvN02/+HUuhGWH42e4s4uSyEzEZFrKdHzDR9j47VvXB2OVDs2rYstPRuvVBT0gk6aabsPbOjvUldHh1dXWUlZURUku3dxlOp5OMjIx9JoP9kRBilZTygHcOtyaIioBNUspz9tr2V+AaYKSUckNTC2kPUHywE3dWKoiOH1JKXvjsP+xZVoo1pJNnLaKnN40e9X0I2Ddzq2sGtQUOKpfb0IwI1u5xmLMHg82J5xe/wD1hQqwvQVE6jUMFUWu65hKB0v22/QzIaxrWjZTSAH4AeqEoHZQQgjsvvIQrrj+dBk+AnoEMGsz1rOj2LWbfAF6o+RM13brT+4JSIm4bgcI6/BtWI2tKqXn3PSpfn4eh5h5TlKPWmiAKAc2dgEKIVKA30VVa9+YFXK0vTVGOjbF9BvL4tBuQAwwceOgRSGNR9yUYwuCrmof4N9cx+LzdmPpYmkbVbcYozcH7/TJKZ87Et2FDrC9BUTq11gTRduDUvUbNXUF0ZoX9gygdKDuK2hTlmHFb7cy84dcMPL8XPluEgd5sNiVsY3fCViobfs7s+tmkjaoj+Wd+IhGBd1sBoZx1hPfkUvF/L1I66yn8mzfH+jIUpVNqzWdE9wEzgJVEw+dGwAxkSinLmvbRgQpglZTy7DatOMbUZ0THv81FBbz07r9wl7nwiXqK7WWcUnwqYS3EeNfLDDO+J2dJT7TaAKZ4C9asDER8d9BMWPv1JW7iRGz9O+b6O4oSK209WMEKfAqc0bQpAkyVUv7fXvtcQHTqn0eklI+1quoOSgVR1+APBZn54Xt4N/ixhAWbHHmcUD2QRF86DucSJjteIG91OsGdEYQuMCdaMffogYhLjwbSwAHEX3wx1t69Y30pitIhtGkQNR1QAOOAbsBqKeWu/V4/g+h6RP+RUu4+8pI7LhVEXcu/1ixn4Sc/4GpwUWgpxWloDKkYg9dSzPXuJ9ArGylcm4qpprEpkGxNgZQGmgnb0KHEXzwRSy81bkfp2to8iLoyFURdT351Bc+88x7OPQ6Cws9uez7jSk5FkzojnG9xhvMjSgtTKFvvwVTbiDAJLIk2TN1/bCHp2EeOIO6iiVgy1NLkStekgqgNCCEmAhP79u07ZceOHbEuRznGDMPgmc/+SckPZViDFrY48hla14v0hmy81nwusrzDQMf3lBWmULY+HlOdNxpISTZM3Xsi3N1AN+E4YTRxF12EOS0t1pekKMdUuwRR01xy44HugO0gu0kp5eOtOkEHpVpEXduiHZt4/8Ov8FR7KDZVErLUMrJsDPawC681j4us7zDQ/gNlBU2BVP9jINkx9chAuNOigXTiicT9/ELMqamxviRFOSbaerCCCfgT0dFy4sfN++0mm7ZJKaV+ZOV2bCqIlMrGBp54/2+Yc0zoEdhqL8RFhBHl0UDyWfP5eVMLqXRPKuUb3JjqfdFASrb/1EIymXGOHUvczy/ElJgY68tSlHbV1kH0BPAAECY6em4H0HCw/aWUjx7RCTo4FUTKj15e9AWblmzD7Y0DwmyxF+IizIjyE5sCKa8pkH6gJD+Vyg1u9AYfwvxjC6kXwpUKZjOuceOIO/98dI8n1pelKO2irYMoj+g0P6dKKde3QX2digoiZW/ri/KZ++WnyNwwLr+LnwJp7xbSXoGU143KDU70Rn80kJId0RaSKxVhseI8/bRoILndsb40RWlTbR1EPmCBlPKitiius1FBpBzImsI85n31GTI3hMvv5rCBlNuNyo0/BpKGJcWOqXsvhDMFYbPjmjAB9znnoLucsb40RWkTbR1E24CNUsor2qK4zkYFkXIoawv3MG/+pxi5QVy+gwfSRZa3GeBYTnFuN6o2ONG9fjSLhjnZ8VMgOew4xozBPmIktgH9EWZzrC9PUVqtrYPoUaIL32VJKQ/62dDxSgWR0hIbigt4bf6nhHf5cfv2/gxpr0Cy5HOR9a1oIO1Ko2qTHd0bQLNoWJId6N17IZzJIDSEzYZt8GDsw4djHzYUzalaSkrn0h5T/CwkOlhhipRy+9GX2HmoIFKOxIEDqQinCDcP+/ZZ8qKBZF9B0a40qjfZ0X3RQDInWjElehAOD9jiwWQDXcfaty/2ESOwjxiOKUkthKx0fO0xxY8TWAYMAvKAAsA4wK5SSnnWEZ+gA1NBpLTGxuJCXlvwKcFdXuK88UjCbD1AIE20vk1/+3KKdqZRvcmG7o+ud6Q7dExOE3q8Hc2VCLY4sLhB0zFnZGAfMRz7iBFqKiGlw2rrFlEy8BXRueT2v39of+o+IkXZy6aSIuYu+JTgzsYWBVJFaRLl+SlESoKYfH4AhNWE2aWhu8xoLjfCFh9tLZkd6ImJ0e67EcOx9uuHMJlifMWKEtXWQfQq8CtgG/AykMOh7yNafEQn6OBUECltYUtZMXPnf4ovp4H4pkDaYi/CtVcg+S35nGb6kuHOr7FqXny1Norz0vAVmtDrvABIixmTU8fiFGgOK8LuAWsc2NxorjhsQ4ZgHzEC25AhaHZ7jK9a6craOoiKiXbDDZZS1rZBfZ2KCiKlLW0tK+HV+Z/g29lAfOOBAykiwgTMJfQUOzjR9i3ZtrVE/FCel0RNgRsqAwgpiehmhNuOzR7B7NAQNgdY46PdePY4bAMGRENp+HBMCQmxvnSli2nrIGoAPpNSXtkWxXU2KoiU9rCtrJRXF3yCN6eO+EZPNJBsxRgmL91CDjLqs0j2RmfuDmp+MO9hoL6BE22L8FBAfZGTij2JhEoMRMTA0HRCrnjMThNOuw/drIPVHQ0mezyW7L7YhgzGlJKCnpCAnpCIKcGDsFhi/JNQjldtHUQrgEop5fltUVxno4JIaU855eW8suBjGnbU4mn0IJBIGaLI0kCJrRyHFHT3x5NR1wd3MDo/nd9Uh928k9GmFQyzLEVW+qnM99BYaEEEIkgEPlcS0u3EaQ/j1OsQJls0mHQL6GbQzKCb0OI86MndMCVGw0lPTMCUkICemBh9xMcjNC3GPyWlM2rrILqe6GdDw7va0G1QQaQcGzsrKvjzgo+o3l1BfEMclrAVAJ0IIRkiz1JDta2W+IhOD18yPer6YItE7y3yW8pINm/lZP07uvu24C3QqC1wYdRH/1/32hPxxSehO82YzRoWEcJMEJMRwCQD6IQQmqkpnMygmaJfdTPolmgLKikVU0o39ORu6AkJ0eBKTIy+pqYnUg6gPYZvPwlcBzwEfCGlLDi6EjsPFUTKseQLhvl612a+37qJ4j0lmGok8b54NENHYKDLCI0izG5rBQGTlwTDSoa3G+l1vTFJMwYRwtYCMi3rGB36AWdJAXV7HAQqteYhrxFNJ2h2ErK4CFqchMwOwmYbEbMFw2RCmjQw6ZgJYpFBzDL61dr01aQZmIRE1wyEZkZYbegJCWjuODSnK/rVFYcW52n6mhDd7nCgOR3Rrw4HQj+uBtgq+2nrFlHkCHaXUsrjavyoCiIllopq6/hy+xrWb8+hrrgaR4MVV8ANEnTCaDJCtQiRa68ALUhK2E5GQwYpjT0RaIS1ALp1JwPFSvrUbsHirUV4AxheSdBnJuzTifjEAe8KjJjMhCwOAhYXfrOboMVJ0OyMhpfFScjsROoaOiFMhh9dhtBkBCEjTbWF0WUEjQiaNNCEgYaBjkQXBrpJQ7PqmK1mdLsZq9OB2eXA6nZhiXNh88SjuePR4hIw9eyDKUUtLtiZtHUQHejG1YOSUh5XHcoqiJSOwjAk64oL+XrbWnJ25hOq9BLf4MYStkZ/wcswggglIkChrQKTBmkhFxn1WXj8+y7IFxEhIpoPdB9C82GWjdiMBuyRRmwhL9agD1vAi8Xvx+z1oXkDaF4/prAfU9iLboQBkAIMi5mI1UbYZCVsshLSbYQ0GyGTg4BmJ6zbCZssRHQrYd1KxGQhrFtAHPpXhUAiZARBGF0GMTsF3YcMpPspP8OZ6MARZ8EZb8FkUS2rjkgtFX4AQoj3iM4MEQFCwP1SygWHe58KIqWj8gbCLM7dyvdbNlK8pwS9xiDe50E3BBoRdBnGMCIUCh9l9hosmsCChllqmA0TVsOM1TBjiVixRmxYw3YsEQfWsB39MB0bkhBCetGlD1PYiyXkwxQOoIfD6JEwplAIzfjxEUZv+rr3NgghtAhoEYQWQeoyesu8BggdqWkYmoWwZqXRnEylLQuEQNc09Lh44npkY3K5sdhNOOIsOOKtTV8tOOOsOOIt2Jxq4thYUUF0AEIIj5Sypun7UcACIFlKecgWnwoipbMoqKlj/o51bNiWQ21JFc4GC86AC10aaDKCTgTNMNANiWaAMCQRQxAyBH4RwWcK4dNCBLQQQT1CWJfI5mAAHQ1daOhSwyxNWAwdi2HBGjFjjdgwGWZM0oxumNANM6amrxpH12IRRgQ94sMSrEDTGgmKOKQAzQigWST2pFTc3TMP+JmTbtKaw8nRFE7RlpQVu9uMph9XHTgdSocPIiFEBvB7YAwwArAD2VLK3APs2xN4FjiH6N9L84GpUsr8ozj/BOBDVBApxynDkKwrKuLrbWvZ2dSN5/Y6MUXM6FIH2dT11fQAiZA/PgeQaNJAj4DeFF66IdEioBugNYWZboAwICQhpEmCQhLWJSERIawZhDWDkDAwNDA0CAuJ1DSkIHpWjegZRVNjSBjoAnQMLBGwRCTmCDgDGvGNHtC7EbB69u3WkyE0I4hmMrDFubHHuzFbNcwWHaEdYlYyIbC7zDg9VnoOTiS9TzxCHG4WM6WlOkMQTQDeA1YBOnAuBwgiIYQDWAcEgAeJ/tt9AnAQHU7eeITnfRa4BIgHfiGl/Ppw71FBpBwPvMEwK/JzqfTV0Rj00egP4Pf78QcC+P1BgsEQoWCYcChEOBTBCEcwwgYybEBEQrQHDd0Ak6FhMnT0iIZJ6miGQJMyGk6y6WHQvE1I9ttGNPQkTd+DJqNfOcSvpzJbA5Xx39J7TxHDchMxTKlUJnWjzpONz5yFoTtA7NulaDJrmKw6ZquO2fLT9yaz9l+h4/RY6T0qhYwBCaql1AY6QxBpP7ZEhBA3An/hwEF0F/C/wAApZU7TtmxgB3CvlPJ/m7bNB0Ye5HSXSCm/2++45wOPE13+PHioWlUQKUpUOGIQCBs0BAM0BHzUB3w0BH3U+700+PwEA2HCkTDhiIERNghHwhgRg3A4gmFIjIhBJGIgDSP6PByCcAQZCSMjYQhH0ELhpkcEEYqghw20kET4wjhrTWiGkwgGOUk7iPf/wPCtjfQthpAuKcp0UBM3jnrrUKRmjT7MdoTJScTQkcZev/sEmC06JouG2arj9Fix2qMhZnOayR6ZQq8hiZjMaiBEa3X4INrbYYJoAWCTUp663/bFAFLK8Udx3hzgKinlqkPtp4JIUTqGFbnrWPrhC1g35pNQ7cLQPZQ5nZQm5ZBVmM+YLQb2IFQmCsp69KTePIGwqWlBQV1DcydgT+4FwkIoECEcNAgFIoSCEZBgd5vxpNqxOqIDHMw2E5lDk8gekYzFdlzdlXJMHE9BVAL8W0p5837bXwSulFKmtPAcdiBNSrm76fkpwKdAbyll9aHeq4JIUTqWGl8D//7iz9Qs+jc9dldjCUoMBJt7JGHW6hi83UfvEgiaoCDLTo37ZPymgSAEQoC0WrAkdSOhexZC0zAiBnWVfuoq/BgRic0VDaQfR9zpZo1eg5PIHpmM3aXm5mup4ymIgsD/Sinv22/7E8B9Lb15VgiRCHwCuImuNNsIPCSlXHiQ/W8CbgLo1avXCXl5eUd0TYqiHBvfbFrCun/9Cfem7SRUR+9tqrNo5GbY6V7awKhtBrYQVCRBSfdEQnIwYXMqYVMCUreCy4k7PQunJwkjIqmv8lNb7osGktOEJ9WBzRUNJKEJevRPoM/oVFwJ1lhedqeggqgNqRaRonR85Q3VfPLhbHxLF5BeUI8W7W0jL8WONIcYsCNAZlm0lZSXZSJgtmAKaERMiUTMifgzzyCp98kIER1xWF/pp67CRyQssf4YSE5TdICDEKT1jqPPqFQ83RyxvvQO61BB1Nk6OquBAy2kktj0mqIoCimuBK6/7gnk5MeZ/8O/yPloLp6t+WSV+wCoidPZMNhOt3Ivw7aHsYXClKZASZqBpaEQ25aN1NV9T8KY2wAz8Sl23Ek2GppaSKW767A6TNEuO5eZkp21lOysJSnDRd8TUknOUBO/HonO1iJaCFiklOP2276I6LW0erBCS6kWkaJ0TnuqCljwzpMYK78nqdQfHR4OFCVYiFgN+uUE6FkOATNsGeTAUecj4nSQcOnDRCJ9CTSGgGgLqaE6QG25j0jIwGLX8aQ6sLvNzUPA41Md9BmdSlrvOHUvUpPjqWtuKvAM0F9KuatpWxbR4dv3SSlnt2NdE4GJffv2nbJjx472Oo2iKO0sbIT5cv48Cj9/l4SdpViC0d+BQSEp6m4jo9DHoN2SnGyNoKZhBsxnnMOAs+4nZ3UZIX/TvHqGpKEmQG2Zj3DIwGLTiU914Ij7KZCcCTb6jE6hRz9Pl78XqVMEkRDiF03fngXcAtwGlAPlUsrFTfs4id7Q6uOnG1ofJzroYLiUsqG961QtIkU5fuTs2cR3bz+FvnY9cTXRFk9QSKqSdH62MkS1B/K7O3F7/YSye3DWI29Sni/ZtbaccCC6EIGUewVS0MBs0/Gk2nHEWZoDyeYy03tkCr0GJ6Gbu2YgdZYgOlghi6WUE/barxf7TvGzgOgUP7ntXSOoIFKU41EwFODLf86h/OuPScivRjMgv7uVE9Z6sQdh4xAncbV+wnE2htw9i94jJ7BrTRm711cQCUVnBZNS0lgTpKbcSzhgYLbqxKfaccb/FEhmm4nB47qTMeBAH3Uf3zpFEHUWKogU5fi2bt1XbJj9B+LKvZQnWOhW6qNPIWwZYIagga5rOCdeztk3P0LQFyZndRl5GysxwnsFUm2Q2jIfoUAEk1XDk+LA6fkpkDKHJTP41PQu1V2ngqgNqM+IFKXraGis4YNH/h8JG/fgM4PPDievi1CUCmVJDpz+AKGBfZj4xFtYnC78jSFyVpWRv6myeeogKSXeuiA1ZT5C/ggmi0Z8ih1XghUhBAlpTkafn9lllqZQQdSGVItIUbqOD16+B+3TL9FCkoJ0C2NX+ZACNg904an1EUx0Mub+P5E19EQAfPVBtq8opXBb9T6B5KsPUVPqJeiPYLbqpGa6MVt1rE4zo8/LJDHdGcvLPCZUELUhFUSK0rUs/+YDdv3pCRx1IUqTrWTv9pJeARuGWbHWhxBWE0mTbuC0a6Y2v6exNsD25aUU7agBuW8gVRQ0ICWk9HRFBzRogsHjupM1LDlWl3hMqCBqQyqIFKXrKS3ezReP/Q+e3CoabBpCRhi51SC3p6DObsEeDhMZOYyLH33z/7d359Fxlecdx7/P7DPaJcu2sGXLK8bG8iJjbGPK4rA0hH1JE5oTaNOG0DQklDRtQgIhKQmEbFCWkByaNKQ5KYQthKRgFhMw4AXbGG+yvCDJlm3t0uzb2z9mZGQh2xpZozuSn885c0a6c9973/t4PD/d7R3szg/HCehuC1O75gAHdncdDqR4NMGhD7qJhhMUj/NRVO5BRJg4q5TTz5mA3TE6zxsdK4hG5xYrpdQQGlcxhb998FX8Zy/AF03iCRtWnelmwn5DZVOEjgIv9g2befpz57B/9/bD7QpKPdRcXMXya2cwtqoQAIfLzvhpReQVueg4GKS5wU8yaWjc3sbqp+oIdh3zm2hGJd0jGiC9WEEpBbDyqZ/S/vhjuEMJ9o/1MGdbgGI/bKr2UdARJul1csqNX+bMy274SNv2AwE2r9pHd0sIYwxdLWHaDwRxetLnjVx2nB4HCy6cRHnl6BomSA/NDSE9NKeUqtv+Du98758pOBSkI99BYXeUU/cadky3E08KLoBlS/jE13+GzXbkgad4LMHGlQ0c3N0JpC5waK73g0D5pAK8+U4Q4dQzxzO9Zuzwb1yW6KE5pZQaQtNnncmVD6+k8/TJFPvjxG3C6oVOZtQlGNMZp8vnhjff4vefX0HrgYYj2jqcdmounnw4ZLwFLiqmF2F32Di4p4vOlhAmmWTH202s//Ne4tGEFZs4rDSIlFJqEHy+Qj7zgz+SvOISbA4b5e1JXl/qwxuEOTsCtJV4cTc2s+qLl7HxlaePaCsinLqkggUXTsLmSH09ecW0InyFLtqbgrQ0BkgmDQd2dfLmk3X42yMWbeXw0ENzGdJDc0qpvta/8RR1D3wXb1eU1mIX4w+EqGqCzbOdOIJJ7A7BseICLvnKjz7StuNgkHV/2kskEMMYQ+ehEB2HQri8qfNGDqcdh8vOvBWVjJ9aZMHWDQ09NKeUUllUs/wqzn/gGbqqyijriNJdYGfdXDtzt8bIjyQJOF0kXnyRJ754Ed0drUe0LR7nY/m1Myga60NEKB7nY+zkAmKRgInzKAAAFf9JREFUBPvrOgkHYsSjCdb/+QN2vN3EaNx50CAaIBG5VEQe7ezstLorSqkcVD52Etc/9BrBsxfiSkChH1Yt8VLeYpj2QZC2Eh/uXfv4v5suonb9qiPaevKcLL1yGhUzigHwFbqomFaEzSYc2NNFd2sYjKFu/SHWPL+HaPqrKEYLPTSXIT00p5Q6nleffoDWX/8CVyjBoTI3U3cHGdcOG6o95HVGEZedgqv+hhU3fP0jbXeuPUjt2oNgDIlEkpZ6PyF/jPxSN2UVeYhN8Ba6qLm4iqJyrwVbNzh6+fYQ0iBSSg3Entr1vPUfN5N/KEDAI9jjSebuTFJXZSPocuKNxYnMO43Lv/04Lrf7iLZNuzrZ9HI9iVgSYwwdB4N0Nodx+xyUTyrA4bRhc9iYe+7EEfOVEnqOSCmlhtmUmTVc+fBKuudOIS9scMYMb57hoqo+ySkHI7QV+nBv2sYz/3ge+3ZvPaJtxbQill45HU+B6/BI3eWV+URDcZrqOokE4yTjSTatrOf91/eRTCQt2sqhoUGklFJZ4vUVcP29f0CuuhRcdsa2JHjrDB+OOMzf0k1rqRfPoS7eue3TvPX8r45oW1TuZfk10ykenxqZO6/YTcW0IhBo2t15+JLuDza38M5zuwkHYsO+fUNFD81lSA/NKaUGY8Pq56j96Z14u6K0FzgZ0xpm6j54b7YTRziJQwSzfBmX/9sjh79ADyCRSPL+a/to3N6W+j2epLm+m3AgTmGZh5KK1NV27jwnNRdPpmR8bn6lhB6aGwJ61ZxS6kQsWHYZH/vPZ+muKqOkO0bQY2fdXAfVW2MUhBJ0e9zYXn+T//3CCjpaDxxuZ7fbmLeiklnLKkAEu8PGuCmFFJR56GoNc3BPF4l4kkggxroX9o7IPSMNogEyxvzBGPOPRUUj94YypZS1ysor+fSDrxI6az4uoMhveGOJh/IWmLEnQEtJHt4PDrHyC5ewefWfj2g7bcFYFn28CrvTjohQdkoeZRPzCAdT542ioTjRUJyNL9WPuHuNNIiUUmoYic3GJ29/nIIb/o6Y28a4gzE2VHsJuWHxhi46il04ukLs+t5X+dOj3zqi7biqQs66Zjq+QhcABSUeKqYWYkzqSrtAR4TWfX52vdtsxaYNmgaRUkpZ4Pxrb2Xudx4kWOphTFuUA+Md1E4RztgQAruNCHZiTz/F7/7lckKBwOF2BaUezrp2BqWn5APg9jmpmF6Ey+ugucFPoCNC7ZoDtB8IHG3VOUeDSCmlLDJz7tl84uH/wz9jPL6wQQy8vdDJjN0JxrVGaC3Mw7t1F8/dtII9W9893M7lcXDmZVOonF0GgMNpY/yUQtw+By37AkTDcTa8WE8sMjJG7tYgUkopC+UXlvHp+1eSWHEWNpuNsvYkqxd78YVg3rYuDpX58Lb42fD1G1n1xEOH29nsNqrPm8jssyeACGITxlSm9pKa6/0EuyJsXtVo1WZlRINIKaVywJW3/YzyL9xCzGtn3KEo22e4aS6Fs9Z148+3k4wk6Pyvh3jy2zeQTH54A+uU6jEsvnQKDrcdp8vOmIl5RENxOg4GadrZQcPWNgu3amA0iJRSKkcs+8Q/cMY9vyRQnkdxd5zuPDsbZ9tYsDmKL5aky+3G9fY6/vfz53Go6YPD7corCzjr6hnkFbvJK3KTX+KmszlMyB9lyxv78LeHLdyq49MgUkqpHDJpxgKufOQlAnMm40qCLySsPsPNhCbD1MYgh0ry8TW28vqXrmT9a88cbpdf4mbxpVNxuO2UnpKH022npcFPNJQ6X5TI4WGANIgGSG9oVUoNF4+vkE/d90fkExdh7EJ5S5z18z0kbLB4UyetpR6c3REa7vsmzz3wr4fb+QpdVJ87EZtNKJ+UTzJhaGnw09kcZPvqJgu36Ng0iAZIb2hVSg23S//ph1Te+g2ieQ7GtMVonOBg70RhyfoAMZcQxgYvvMBvb/k43d2pP5IrphczaU4ZLo+Dkoo8Qv4YXS1h9r7XwsG9XRZvUf80iJRSKofVnP83nP3j3xGoKCQvZIjbYN18B3NqE5R3Rmkp8JFXW88LN13A9vdWAzB7+Snkl3ooKHXjK3TRfjBIJBhn08sNOTkEkAaRUkrluHGVp3LNz14lvGAmDmwUdRreWuyhqAuqa7s5OCYPb1uQLXfczHvvrMTusLHwosnYnXbKJuRhd9hobugmEojl5BBAGkRKKTUCOJ1urrv7KbzXXU3CKYxpjrFtlpuOAli2tovOQieE4uy85zbeW7OSglIPs5efgt1ho7wyn3g0Set+P637/NStP2T15hxBg0gppUaQC278NjO/8X0ihS6KuuJ0FNvZNtPGok1hXMYQTtio/f5tbF7zMpPnlFExvRhPnpPisV4CHVH87RF2rj2YU0MAaRAppdQIM+fMS7jggWcJVpbiioMzanhnkYsp9UnKukLE4jZ23PMvbF7zCnPPnYi3wEXRWC/uPAet+z+8pDtXhgDSIFJKqRGoeGwl1z38KrGl8xGbjdLWBGsWeZh4AIr9IWJxofaeW9nx3ussuHASNnvqEJ2I0FzfTbAzwubXcmMIIA0ipZQaoWx2O1d/63GKP3sDcZeNMS0x1i7yUNkExf4Ikbiw/ftfYX/jWmYuHo/DaWfMxHyi4QTtB4I01eXGEEAaREopNcL91SdvY95dDxAu9lCWDqNJ+w3FgQjxuLDte18mwjbGVBbgK3Qd/nbXYFeULX+xfgggDaIB0pEVlFK5bNq8c1jxkycIlXopa4mxpsbD5H2GgkCEWAy23X0LnnGNuHwOSsb7cHrstDT6iYTivGvxEEAaRAOkIysopXJd2bgprLjvd4RLvJS1xlhT46aq0VAYihKPwM57v0RhZWv6fFEBJmloaeimy+IhgDSIlFJqFBlTMZXz7v1t6jBda5y1i9xUNRjyI6kwqn/kS3hK23F5UoOjhgNxOptDqSGA9lgzBJAGkVJKjTLlE6dz7g9+Q7jYQ2k6jKY0GPKiURJhQ+uTt5KwdZBf4iavyEXHwRDhQIxNr1gzBJAGkVJKjUJjJ57Kufc8TqTInQqjGjfT6w2+WIxkOE7oldsJhTsom5CHw2mjucFPOBC1ZAggDSKllBqlxk6axdnff5xIoZvStjhratzM+MDgjccg1EV0/f0EA52UT8onEUvS2higpbF72IcA0iBSSqlRbHzVaSz73mNEC12UtMVZU+Ni5l6DJxHD4d9FeOvviSX8lIz3Eez6cAigtqbhGwJIg0gppUa5CVPnseQ/HiNa4KKkLcGahW5m7jG443G87asJbP8LNncIT76Ttv0BIsHUKN3DNQSQBpFSSp0EJk6fz5nf/TnRfBcl7XHWLnRx6p4kzmQYT9tLdO/chK84gdiF5no/gWEcAkiDSCmlThKVM2tY/J1HieY5KW5PsG6hi1m7k3gSrbg7VuPfvZn8MiEWSdDeFKCproP6La1Z75cGkVJKnUQmzVrEGXc9RCzPSVF7gnULXJy2K0l+dAsSrCXcsBlvodDdFiHQGWHrG/vpbsvuEEAaREopdZKZPHspNXc+SMznpKgjHUZ1SUr8r5CItRBv2YLDBa2NqfNFG7I8BJAGkVJKnYSqTl9GzZ33E/M5KOpIsH6+k9l1ScZ0/J5kPIj4d2KMobnBT1dLkG1vZm8IIA0ipZQ6SVXNPZv5t/+YmNdBYWeS9fOdzNmZoKzjv0nEu7HFGokE43QcCrGvtj1r/dAgUkqpk9i0Becx7xs/JO61U9iZ5N15Tk6vTTCm81cQPYQkOug8FCLUHc1aHzSIBki/BkIpNVpNr1nB3H//AXGPnYKuJO9WOzl9R5zSrl8h4XowEZp2dRD2Z2ccOg2iAdKvgVBKjWYzzriQ2f9+D3G3nYLuJBuqnczdHqe0+zFs4T0kY0le+fW2rKxbg0gppRQAsxZfzOx/u5u4205+d5IN1Q6qt8co6XoEie5l3vmVWVmvBpFSSqnDZi25hFm33UXCZSO/K8nG0x3M2xajpPN+SiZ7srJODSKllFJHmL38cmZ+9S4Sbjt5/iQb5zqZvzXGc59dlpX1ObKyVKWUUiPanOVXkIzHqPvxd8jrTrBxrgtv1ZSsrEuDSCmlVL/mnnstJA27fvJdPFHDFfc8lZX1aBAppZQ6qrnnX0cyEaPukfuytg4NIqWUUsc074LrcXnzs7Z8vVhBKaXUcZ22/PKsLVuDSCmllKU0iJRSSllKg0gppZSlNIiUUkpZSoNIKaWUpTSIlFJKWUqDSCmllKU0iJRSSllKg0gppZSlxBhjdR9GFBFpBj7oNakI6Mzg9zFASxa61nc9Q9nuePMc7fX+pudKvfpb11C10Xpl3uZY82m9MpvvROrVd9pQ1muyMaa831eMMfo4gQfwaIa/rxuOfgxlu+PNc7TX+5ueK/UabM20Xtlpc6z5tF7DV6++04arXnpo7sT9IcPfh6sfQ9nuePMc7fX+pudKvQa7Lq1Xdtocaz6tV2bznUi9+k4blnrpoblhJiLrjDGLrO7HSKH1yozWKzNar8xkq166RzT8HrW6AyOM1iszWq/MaL0yk5V66R6RUkopS+kekVJKKUtpEOUwESkRkedFpFZENonIiyIy3ep+5TIR+Wa6XkkRucLq/uQSEZkmIm+k67NBRPTcyHHo+2ngTuTzSoMotxngJ8aYmcaYecDzwC8s7lOuewm4GHjd6o7koEeAXxljZgL/CvxGRMTiPuU6fT8N3KA/rzSIMiAiE0XkARF5S0SCImJEpOoo81aKyJMi0ikiXSLylIhMymR9xpgOY8zKXpNWA/2uLxcNd70AjDFvG2N2n2jfc8FQ1k9EyoElwC8BjDEvAQLUZH1DhtFQv+dG0/upP0NZrxP5vNIgysx04DqgHfjL0WYSER/wCjAL+CzwGWAG8KqI5J3A+r8MPHsC7Yeb1fUa6YayfpOAJmNMrFfTvenpo4m+5zKTzXoN/PMqW3cVj8YHYOv18+dI7YpW9TPfLUACmN5r2hQgDtzaa9pKUsNl9Pc4q88y7yD1F4bP6jqMkHq9BlxhdQ1ypX6k9nxq+7R7EbjK6u3M1ZqNtvfTMNcro88r3SPKgDEmOcBZLwPeNsbU9Wq7B3gTuLzXtI8ZY8Yc5fFmz3wicjvwceCvjTHBodma7LOqXqPFENevHqgQEWevdlXp6aPGUL/nRrts1Gswn1caRNkxB3i/n+lbgNmZLEhE7gAuBS40xgxmUNORYMjqdZI6bv2MMc3AGuAGABG5gNQ5ovXD08Wco++5zAyoXoP9vNIgyo5SUsdc+2oDSga6EBGZA9wJlAGrRGSjiKwbkh7mliGpF4CI3CkijcBS4Bci0igiE4egj7lsoPW7CbhRRGqBHwDXm/RxlJPQgGp2kr6f+nPcep3I55VjiDqpssAYs4XUX61qgIwxd5L6z6D6MMbsBJZZ3Y+RRN9PA3cin1e6R5Qd7fT/l/zR/qo42Wm9TozWL3Nas8xktV4aRNmxhdQx1b5mA1uHuS8jgdbrxGj9Mqc1y0xW66VBlB3PAUtEZGrPhPRNYmelX1NH0nqdGK1f5rRmmclqvXT07QyJyDXpH1eQOvl7M9AMNBtjVqXnyQM2ASHgdlLX5n8HKACqjTH+4e63VbReJ0brlzmtWWZyol5W31A10h7pf4D+Hq/1mW8S8HugC+gGnqGfG8VG+0PrpfXTmuX2IxfqpXtESimlLKXniJRSSllKg0gppZSlNIiUUkpZSoNIKaWUpTSIlFJKWUqDSCmllKU0iJRSSllKg0gppZSlNIiUUkpZSoNIKYuJyE9FxIjIOVb3RSkr6BA/SllMROoBDzDeGJO0uj9KDTfdI1LKQiJyBlAJPKshpE5WGkRKWeuq9PPTlvZCKQtpECk1AOlzOCb98ydF5C0R8YtIt4i8LCLLB7noK0kNq79ygP0Yl+5LbT+v3dTTTxGZ3ue1uenp7wyyn0pljQaRUhkQkbuA/wGiwB+BRuB84GURWZrhsmYDpwIvGGOiA2zWnn4u6LMsO3Bbr0mlfdp9Jf18byZ9VGo4aBAplZl/AhYbY84xxnwSmAP8HHABd2W4rJ7Dck8NtEE6sIL0CSLgamAasDr9++EgEpGxwKeBOvQQoMpBGkRKZeYOY8z6nl/SFxh8M/3r2SLizGBZVwFh4E8Z9qENyBOR3v9/vwYcAu5P/957j+hmwA38SC+IULlIg0ipzDzfd4Ix5iCpQ2ZuoGwgCxGRKmAB8JIxxp9hH444PCciFwALSYXQgfRrpenX3MAXgGbglxmuR6lhoUGkVGbqjzK9K/3sGeByMj4s10vf80RfA/zAQ0BnelpJ+vl6YCzwn8aY0CDWpVTWaRAplYEhPLR1FRAH/jCItm3p5wIRWQSsAH5ujGkHOtKv9Rya+zKpc0oP9l2IiDSJyLdE5HYRaUhfAfhzEbGLyDIRWSUiARHZJCLVfdpeLSIrReSgiIRE5H0RubbX6zXpq/Q+22takYhsFpG/iMhAA1udBDSIlBpmIjIOWAq8boxpHcQievaICkntDcWAH6en9ewRlYrIx4C5wGN915O+gGE88PfAKennB4DPpZ8fBh4BPkVq7+pHffpQDTwJ/C1wKfAa8D8icipA+jzaM8A30sHmTM/vBC43xoQHsd1qlHJY3QGlTkJXkPojcDCH5eDDIFpIas/qN8aYhvS0LsCQ2iP6CpDgoyECMC/9/GtjzO3pn18UkS+SCpbq9B4W6b2uv+/d2BhzR8/P6UvHX0vPsxjYkX7pDmAjqTA7n1R4LTHGtKFULxpESg2/q0iFxTODbN/zQf51QOh1b5AxJiEiflIhVQE8YYzZ088yqoEIcE/PBBFxkLrg4qGeEEor4MNDfj0XQHweuBGYSmrPrEewV1/eE5EngJ+lJ517lL6ok5wGkVLDSESKgfOANcaYfYNcTE9ITCR1M+z7fV7vTL8GR7+BdR6w1hjT3WvabFL3Q73cZ95q4H0AERHgWVJB9xPgXaAVODe9ru192tYBPuBeY8za422YOjnpOSKlhtelpM6TnMiNpb33VvoLmp7zRK/2vuepj3mkDpv1nZYANvczfVP656XARcD1xpi7jTF/TgfMNFL3RB0OIhH5FKlzWGuBz4iI95hbpU5aGkRKDYAxRowxcozXq9Lz7D3Ook7ksu2edf22pz/GmFX9vH56+rXz+2ufvnBgFh8NovnAjt6XeYvIBFL3RvXMW5l+7h04s4EbgPeNMYn0tHNI3bf0NeCa9DJuznBT1UlCg0ip4fUW8FVjzE4L+3AaqUNw/QXRhj7Tei5q6Nkjepf0BRAisiJ9ccOzpPaGNgKIyGmkzn/9whjzQ2NMPfBfwNdEJG+oN0aNfBpESg0jY8y9xpj7LO7GPFL3MPU9t3S0w3UtPeez0gH6D6QO0T0LXEJqnDsPsFFExpMasugN4Eu9lnM3UATcMqRbokYF/YZWpZRSltI9IqWUUpbSIFJKKWUpDSKllFKW0iBSSillKQ0ipZRSltIgUkopZSkNIqWUUpbSIFJKKWUpDSKllFKW+n/+ay4FAfgIqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

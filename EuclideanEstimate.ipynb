{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases} \\\\\n",
    "g(\\beta^*, \\gamma^*) &= -\\beta^* \\left(-1 + \\frac{1}{N} \\sum_n w_n\\right) + {\\beta^*}^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\ \n",
    "&= \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} - \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} \\\\\n",
    "&= \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (1 + \\lambda)} \\left( \\left( N Q_{w, r} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "0 &= \\gamma + \\beta w - \\frac{2 N}{1 + \\lambda} \\left( N Q_{w, r} \\right)^{-1 - \\lambda} & (c_{w,r} = 1) \\\\\n",
    "\\left( N Q_{w, r} \\right)^{-1 - \\lambda} &= \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\\\\n",
    "Q_{w,r} &= \\frac{1}{N} \\left( \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\right)^{\\frac{-1}{1 + \\lambda}}  & \\left( \\lambda > -1 \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Substitute $\\gamma \\leftarrow \\gamma \\frac{1 + \\lambda}{2 N}$ and $\\beta \\leftarrow \\beta \\frac{1 + \\lambda}{2 N}$ to get\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w, r} &= \\frac{1}{N} \\left(\\gamma + \\beta w\\right)^{\\frac{-1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\sum_n \\left( \\frac{2}{1 + \\lambda} \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} + \\frac{2}{\\lambda (1 + \\lambda)} N Q_{w_n, r_n} \\left( \\gamma + \\beta w_n \\right) \\right) \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda} \\sum_n \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma  - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda}  \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\\\\n",
    "&= - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2 N}{(1 + \\lambda)} \\left( - \\beta - \\gamma +  \\frac{1 + \\lambda}{\\lambda N} \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\right) \\\\\n",
    "&= \\frac{2 N}{(1 + \\lambda)} \\left( -\\frac{1}{\\lambda} - \\beta - \\gamma +  \\frac{1 + \\lambda}{\\lambda N} \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\right) \\\\\n",
    "&= \\frac{2 N}{(1 + \\lambda)} \\left( 1 - \\beta - \\gamma +  \\frac{1}{N} \\sum_n \\frac{1 + \\lambda}{\\lambda} \\left( \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} - 1 \\right) \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Just hit it with a generic convex solver $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": [
     0,
     43,
     50,
     58,
     140,
     167
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** Almost MLE ******\n",
      "****** MLE ******\n",
      "****** Lambda Zero ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, minexp=1, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(minexp, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "        factor = (barw - 1) / denom\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "        \n",
    "#         estimate = sum(c*w*r*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n\n",
    "#         missing = max(0, 1 - sum(c*w*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n)\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "\n",
    "class CressieRead:\n",
    "    @staticmethod\n",
    "    def dualobjective(gamma, beta, data, n, lam):\n",
    "        from math import log\n",
    "        \n",
    "        lampow = lam / (1 + lam)\n",
    "\n",
    "        def approxpow(loga):\n",
    "            z = loga * lampow\n",
    "            return loga * (1 + 1/2 * z * (1 + 1/3 * z * (1 + 1/4 * z * (1 + 1/5 * z * (1 + 1/6 * z * (1 + 1/7 * z))))))\n",
    "        \n",
    "        dual = - gamma - beta\n",
    "        if -1e-2 <= lampow and lampow <= 1e-2:\n",
    "            dual += sum((c/n) * approxpow(log((gamma + beta * w))) for c, w, _ in data)\n",
    "        else:\n",
    "            dual += sum((c/n) * ((gamma + beta * w)**lampow - 1) / lampow for c, w, _ in data)\n",
    "            \n",
    "        return -dual\n",
    "    \n",
    "    @staticmethod\n",
    "    def jacdualobjective(gamma, beta, data, n, lam):       \n",
    "        lampow = lam / (1 + lam)\n",
    "        j = [ -1, -1 ]\n",
    "        for c, w, _ in data:\n",
    "            dx = (c/n) * (gamma + beta * w)**(lampow - 1)\n",
    "            j[0] += dx\n",
    "            j[1] += dx * w\n",
    "            \n",
    "        return -j[0], -j[1]\n",
    "        \n",
    "    @staticmethod\n",
    "    def hessdualobjective(gamma, beta, data, n, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        h = [ 0, 0, 0 ]\n",
    "        for c, w, _ in data:\n",
    "            d2x = (c/n) * (lampow - 1) * (gamma + beta * w)**(lampow - 2)\n",
    "            h[0] += d2x \n",
    "            h[1] += d2x * w\n",
    "            h[2] += d2x * w * w\n",
    "            \n",
    "        return [ [ -h[0], -h[1] ], [ -h[1], -h[2] ] ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, lam, **kwargs):\n",
    "        from cvxopt import matrix, solvers\n",
    "        \n",
    "        rmin = kwargs.pop('rmin', 0)\n",
    "        rmax = kwargs.pop('rmax', 1)\n",
    "        \n",
    "        n = sum(c for c, _, _ in data)\n",
    "        assert n > 0\n",
    "        \n",
    "        x0 = 1.0, 0.0\n",
    "        \n",
    "        G = matrix([ [ -1.0, -float(w)  ] for w in (wmin, wmax) ])\n",
    "        h = matrix([ 0.0 for w in (wmin, wmax) ])\n",
    "\n",
    "        if False:\n",
    "#             import MLE.MLE\n",
    "#             from numpy import array as arr\n",
    "#             MLE.MLE.gradcheck(f = lambda x: CressieRead.dualobjective(x[0], x[1], data, n, lam),\n",
    "#                               jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='dualobjective',\n",
    "#                               eps = 1e-6)\n",
    "#             MLE.MLE.hesscheck(jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, lam)),\n",
    "#                               hess = lambda x: arr(CressieRead.hessdualobjective(x[0], x[1], data, n, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='jacdualobjective')\n",
    "            pass\n",
    "        \n",
    "        def F(x=None, z=None):\n",
    "            if x is None: return 0, matrix(x0)\n",
    "            if any(x[0] + x[1] * w <= 0 for _, w, _ in data):\n",
    "                return None\n",
    "            f = CressieRead.dualobjective(x[0], x[1], data, n, lam)\n",
    "            jf = CressieRead.jacdualobjective(x[0], x[1], data, n, lam)\n",
    "            Df = matrix(jf).T\n",
    "            if z is None: return f, Df\n",
    "            hf = CressieRead.hessdualobjective(x[0], x[1], data, n, lam)\n",
    "            H = z[0] * matrix(hf)\n",
    "            return f, Df, H\n",
    "        \n",
    "        soln = solvers.cp(F, G, h, options={'show_progress': False })\n",
    "        if False:\n",
    "            if soln['status'] != 'optimal':\n",
    "                import sys\n",
    "                print('.', file=sys.stderr, end='')\n",
    "#             else:\n",
    "#                 (gammastar, betastar) = soln['x']\n",
    "#                 import numpy as np\n",
    "#                 if not np.allclose(gammastar + betastar, 1, atol=1e-3):\n",
    "#                     import sys\n",
    "#                     from pprint import pformat\n",
    "#                     print(pformat({\n",
    "#                                 'gammastar': gammastar,\n",
    "#                                 'betastar': betastar,\n",
    "#                                 'n': n,\n",
    "#                                 'gammastar + betastar': gammastar + betastar,\n",
    "#                             }), file=sys.stderr)\n",
    "            pass\n",
    "\n",
    "        fstar = -(2 * n / (1 + lam)) * (soln['primal objective'] - 1)\n",
    "        (gammastar, betastar) = soln['x']\n",
    "                \n",
    "        estimate = sum((c/n) * w * r * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, r in data)\n",
    "        missing = max(0, 1 - sum((c/n) * w * 1 * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, _ in data))\n",
    "        \n",
    "        return max(0, min(1, estimate + 0.5 * missing)), { \n",
    "            'fstar': fstar, \n",
    "            'gammastar': gammastar, \n",
    "            'betastar': betastar,\n",
    "            'n': n\n",
    "        }\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('TwoThirds', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=2/3)[0]),\n",
    "                        ('MinusOneHalf', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=-1/2)[0]),\n",
    "                        ('One', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1)[0]),\n",
    "                        ('Almost MLE', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1e-3)[0]),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                        ('Lambda Zero', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=0)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=10000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "skip = { 'TwoThirds': 1, 'MinusOneHalf': 1, 'One': 1, 'Almost MLE': 1 }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    if name in skip:\n",
    "        continue\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

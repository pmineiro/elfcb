{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases} \\\\\n",
    "g(\\beta^*, \\gamma^*) &= -\\beta^* \\left(-1 + \\frac{1}{N} \\sum_n w_n\\right) + {\\beta^*}^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\ \n",
    "&= \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} - \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} \\\\\n",
    "&= \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (1 + \\lambda)} \\left( \\left( N Q_{w, r} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "0 &= \\gamma + \\beta w - \\frac{2 N}{1 + \\lambda} \\left( N Q_{w, r} \\right)^{-1 - \\lambda} & (c_{w,r} = 1) \\\\\n",
    "\\left( N Q_{w, r} \\right)^{-1 - \\lambda} &= \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\\\\n",
    "Q_{w,r} &= \\frac{1}{N} \\left( \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\right)^{\\frac{-1}{1 + \\lambda}}  & \\left( \\lambda > -1 \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Substitute $\\gamma \\leftarrow \\gamma \\frac{1 + \\lambda}{2 N}$ and $\\beta \\leftarrow \\beta \\frac{1 + \\lambda}{2 N}$ to get\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w, r} &= \\frac{1}{N} \\left(\\gamma + \\beta w\\right)^{\\frac{-1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\sum_n \\left( \\frac{2}{1 + \\lambda} \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} + \\frac{2}{\\lambda (1 + \\lambda)} N Q_{w_n, r_n} \\left( \\gamma + \\beta w_n \\right) \\right) \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda} \\sum_n \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda}  \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Just hit it with a generic convex solver $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0,
     43,
     50,
     58,
     129,
     156
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** Almost MLE ******\n",
      "****** MLE ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(1, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "        factor = (barw - 1) / denom\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "        \n",
    "#         estimate = sum(c*w*r*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n\n",
    "#         missing = max(0, 1 - sum(c*w*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n)\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "\n",
    "class CressieRead:\n",
    "    @staticmethod\n",
    "    def dualobjective(gamma, beta, data, n, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        dual = (-2 / (1 + lam)) * (gamma + beta + 1 / lam) \n",
    "        dual += (2 / lam) * sum((c/n) * (gamma + beta * w)**lampow for c, w, _ in data)\n",
    "        return -dual\n",
    "    \n",
    "    @staticmethod\n",
    "    def jacdualobjective(gamma, beta, data, n, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        j = [ -2 / (1 + lam), -2 / (1 + lam) ]\n",
    "        for c, w, _ in data:\n",
    "            dx = (2 / lam) * (c/n) * lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            j[0] += dx\n",
    "            j[1] += dx * w\n",
    "            \n",
    "        return -j[0], -j[1]\n",
    "        \n",
    "    @staticmethod\n",
    "    def hessdualobjective(gamma, beta, data, n, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        h = [ 0, 0, 0 ]\n",
    "        for c, w, _ in data:\n",
    "            dx = (2 / lam) * (c/n) *  lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            d2x = (2 / lam) * (c/n) * lampow * (lampow - 1) * (gamma + beta * w)**(lampow - 2)\n",
    "            h[0] += d2x \n",
    "            h[1] += d2x * w\n",
    "            h[2] += d2x * w * w\n",
    "            \n",
    "        return [ [ -h[0], -h[1] ], [ -h[1], -h[2] ] ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, lam, **kwargs):\n",
    "        from cvxopt import matrix, solvers\n",
    "        \n",
    "        rmin = kwargs.pop('rmin', 0)\n",
    "        rmax = kwargs.pop('rmax', 1)\n",
    "        \n",
    "        n = sum(c for c, _, _ in data)\n",
    "        assert n > 0\n",
    "        \n",
    "        x0 = 1.0, 0.0\n",
    "        \n",
    "        G = matrix([ [ -1.0, -float(w)  ] for w in (wmin, wmax) ])\n",
    "        h = matrix([ 0.0 for w in (wmin, wmax) ])\n",
    "\n",
    "        if False:\n",
    "#             import MLE.MLE\n",
    "#             from numpy import array as arr\n",
    "#             MLE.MLE.gradcheck(f = lambda x: CressieRead.dualobjective(x[0], x[1], data, n, lam),\n",
    "#                               jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='dualobjective',\n",
    "#                               eps = 1e-6)\n",
    "#             MLE.MLE.hesscheck(jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, lam)),\n",
    "#                               hess = lambda x: arr(CressieRead.hessdualobjective(x[0], x[1], data, n, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='jacdualobjective')\n",
    "            pass\n",
    "        \n",
    "        def F(x=None, z=None):\n",
    "            if x is None: return 0, matrix(x0)\n",
    "            if any(x[0] + x[1] * w <= 0 for _, w, _ in data):\n",
    "                return None\n",
    "            f = CressieRead.dualobjective(x[0], x[1], data, n, lam)\n",
    "            jf = CressieRead.jacdualobjective(x[0], x[1], data, n, lam)\n",
    "            Df = matrix(jf).T\n",
    "            if z is None: return f, Df\n",
    "            hf = CressieRead.hessdualobjective(x[0], x[1], data, n, lam)\n",
    "            H = z[0] * matrix(hf)\n",
    "            return f, Df, H\n",
    "        \n",
    "        soln = solvers.cp(F, G, h, options={'show_progress': False })\n",
    "        if False:\n",
    "            if soln['status'] != 'optimal':\n",
    "                import sys\n",
    "                print('.', file=sys.stderr, end='')\n",
    "        fstar, (gammastar, betastar) = -n * soln['primal objective'], soln['x']\n",
    "                \n",
    "        estimate = sum((c/n) * w * r * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, r in data)\n",
    "        missing = max(0, 1 - sum((c/n) * w * 1 * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, _ in data))\n",
    "        \n",
    "        return max(0, min(1, estimate + 0.5 * missing))\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('TwoThirds', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=2/3)),\n",
    "                        ('MinusOneHalf', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=-1/2)),\n",
    "                        ('One', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1)),\n",
    "                        ('Almost MLE', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1e-2)),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=10000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "skip = { 'MinusOneHalf': 1, 'One': 1 }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    if name in skip:\n",
    "        continue\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

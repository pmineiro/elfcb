{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (\\lambda + 1)} \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "Q^*_{w_n, r_n} &= \\frac{1}{N} \\left(\\frac{2 N}{\\left(\\gamma + \\beta w\\right) \\left(1 + \\lambda\\right)}\\right)^{\\frac{1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(\\beta, \\gamma, Q) \\\\\n",
    "&= -\\beta - \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2^{\\frac{1}{1 + \\lambda}} \\left(1 + \\lambda\\right)^{\\frac{\\lambda}{1 + \\lambda}}}{\\lambda N^\\frac{\\lambda}{1 + \\lambda}} \\sum_n \\left(\\gamma + \\beta w \\right)^{\\frac{\\lambda}{1 + \\lambda}} & (\\lambda > -1, \\lambda \\neq 0)\n",
    "\\end{aligned}\n",
    "$$ Just hit it with a generic convex solver $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0,
     43,
     50,
     56
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** MLE ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(1, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "        factor = (barw - 1) / denom\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "        \n",
    "#         estimate = sum(c*w*r*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n\n",
    "#         missing = max(0, 1 - sum(c*w*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n)\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "\n",
    "class CressieRead:\n",
    "    @staticmethod\n",
    "    def dualobjective(gamma, beta, data, n, fac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        dual = -gamma - beta - (2 * n) / (lam * (1 + lam))\n",
    "        dual += fac * sum(c * (gamma + beta * w)**lampow for c, w, _ in data)\n",
    "        return -dual\n",
    "    \n",
    "    @staticmethod\n",
    "    def jacdualobjective(gamma, beta, data, n, fac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        j = [-1, -1]\n",
    "        for c, w, _ in data:\n",
    "            dx = fac * c * lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            j[0] += dx\n",
    "            j[1] += dx * w\n",
    "            \n",
    "        return -j[0], -j[1]\n",
    "        \n",
    "    @staticmethod\n",
    "    def hessdualobjective(gamma, beta, data, n, fac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        h = [ 0, 0, 0 ]\n",
    "        for c, w, _ in data:\n",
    "            dx = fac * c * lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            d2x = fac * c * lampow * (lampow - 1) * (gamma + beta * w)**(lampow - 2)\n",
    "            h[0] += d2x \n",
    "            h[1] += d2x * w\n",
    "            h[2] += d2x * w * w\n",
    "            \n",
    "        return [ [ -h[0], -h[1] ], [ -h[1], -h[2] ] ]\n",
    "        \n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, lam, **kwargs):\n",
    "        from cvxopt import matrix, solvers\n",
    "        \n",
    "        rmin = kwargs.pop('rmin', 0)\n",
    "        rmax = kwargs.pop('rmax', 1)\n",
    "        \n",
    "        n = sum(c for c, _, _ in data)\n",
    "        fac = 2**(1/(1+lam)) * ((1+lam)/(n))**(lam/(1+lam)) / lam                                           \n",
    "        \n",
    "        x0 = (2 * n) / (1 + lam), 0\n",
    "        \n",
    "        G = matrix([ [ -1.0, -float(w) ] for w in (wmin, wmax) ])\n",
    "        h = matrix([ 0.0 for w in (wmin, wmax) ])\n",
    "        \n",
    "        def F(x=None, z=None):\n",
    "            if x is None: return 0, matrix(x0)\n",
    "            if any(x[0] + x[1] * w <= 0 for _, w, _ in data):\n",
    "                return None\n",
    "            f = CressieRead.dualobjective(x[0], x[1], data, n, fac, lam)\n",
    "            jf = CressieRead.jacdualobjective(x[0], x[1], data, n, fac, lam)\n",
    "            Df = matrix(jf).T\n",
    "            if z is None: return f, Df\n",
    "            hf = CressieRead.hessdualobjective(x[0], x[1], data, n, fac, lam)\n",
    "            H = matrix(hf)\n",
    "            return f, Df, H\n",
    "        \n",
    "        soln = solvers.cp(F, G, h, options={'show_progress': False})\n",
    "        fstar, (gammastar, betastar) = -soln['primal objective'], soln['x']\n",
    "        \n",
    "        qfac = (1.0 / n) * ((2 * n) / (1 + lam))**(1 / (1 + lam))\n",
    "        \n",
    "        estimate = sum(c * w * r * qfac * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, r in data)\n",
    "        missing = max(0, 1 - sum(c * w * 1 * qfac * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, _ in data))\n",
    "        \n",
    "        return max(0, min(1, estimate + 0.5 * missing))\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('TwoThirds', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=2/3)),\n",
    "                        ('MinusOneHalf', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=-1/2)),\n",
    "                        ('One', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1)),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=10000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEdCAYAAABOl2PPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd3RU1drA4d+ekmTSeyEdgvTeFQEpUpSiXrGBlSLCVZDeVRRRRFCvgCgqdtHr51XsgoBKMVRpoSeQ3sukTdvfH8FIkJKECZOyn7VmMXPmnH3eGdbKO/ucvfcrpJQoiqIoiqNoHB2AoiiK0rCpRKQoiqI4lEpEiqIoikOpRKQoiqI4lEpEiqIoikOpRKQoiqI4lM7RAdQ1/v7+MioqytFhKIqi1Cm7d+/OlFIGXOw9lYiqKCoqil27djk6DEVRlDpFCJFwqffUpTlFURTFoVQiUhRFURxKJSJFURTFoVQiUhRFURxKDVZQFMXubDYbmZmZ5ObmYrVaHR2Oco24uLgQFhaGXq+v0nEqESmKYneJiYkIIYiKikKv1yOEcHRISg2TUpKVlUViYiLR0dFVOlZdmlMUxe4KCwsJDQ3FyclJJaEGQgiBn58fJSUlVT5WJaJrSNpsjg5BUa4ZjUb9eWloqvujQ12au0YObf6eXWuO4NU6nRFznnd0OIqiKLWG+slSSUKIoUKINXl5edU6PjmrgGLXJmQc78SGZ2fZOTpFURxp8+bNhIWFlb9u1aoVmzdvrtS+ikpElSal/FpKOc7Ly6taxw+4405cWh7HonUh9URHvls03c4RKopSWVFRURgMBtzd3csfkyZNslv7hw4dok+fPnZrr75TiegaevDJqehbxGHWu5J0sgs/PD3N0SEpSoP19ddfYzQayx//+c9/HB1Sg6US0TU2Zuo0RLM4zHo3Ek914eenpjo6JEVRznnqqacYNWpU+ev4+HiEEFgsFgCys7N56KGHaNSoET4+PowYMeKi7URFRfHzzz8DUFxczIMPPoiPjw8tW7YkNja2wr7JycnccccdBAQEEB0dzauvvlr+3h9//EGPHj3w9vYmJCSESZMmYTKZyt8XQrB69WqaNm2Kt7c3EydOREppt+/jWlGJyAEenTYNW7PDmJzcSTjdmV/mTXZ0SIqiVMLo0aMpKiri0KFDpKenM2XKlCse8/TTT3Py5ElOnjzJDz/8wLp168rfs9lsDB06lHbt2pGUlMTGjRtZsWIFP/zwAwBarZbly5eTmZnJ9u3b2bhxIytXrqzQ/oYNG4iNjeXPP/9k/fr15cfWJWrUnINMnDadV196CXm0JafPdEMz53F6L371ygcqSh309NeHOJycX6PnaNnIk4VDW1V6/xEjRqDT/f0ncOnSpZfdPyUlhe+++46srCx8fHwA6N279xXPs379elauXImvry++vr48/vjjPPPMMwDExsaSkZHBggULAGjcuDFjx47lk08+YeDAgXTq1Km8naioKMaPH8+WLVuYPPnvH6+zZs3C29sbb29vbrrpJvbt28egQYMq/T3UBqpH5ECPT5tG6XVHKHHx5mRid36bbb+bpYqiXN6XX35Jbm5u+WPs2LGX3f/s2bP4+vqWJ6HKSk5OJjw8vPx1ZGRk+fOEhASSk5PLE4m3tzeLFy8mLS0NgGPHjnHrrbcSHByMp6cnc+bMITMzs0L7wcHB5c9dXV0xGo1Viq82UD0iB5syfSpLX3oJjrXieGIPxIwJ3PDiKkeHpSh2VZWeiiO5ublRVFRU/jo1NbX8eXh4ONnZ2eTm5uLt7V3pNkNCQjh79iytWpV9B2fOnKnQZnR0NMePH7/osRMmTKBDhw58/PHHeHh4sGLFCj7//POqfqxaT/WIaoHp06ZR0PQgxQY/jqXcwI6p4x0dkqI0SO3bt2fr1q2cOXOGvLw8nn/+78nnISEhDB48mMcee4ycnBzMZjNbt269YpsjR47k+eefJycnh8TERF577bXy97p27YqHhwcvvPACxcXFWK1WDh48WD6goaCgAE9PT9zd3YmLi2PVqvr5I1Ulolpi5vTp5MYcpMjgT1xaT/6YfPnLBIqiXJ2hQ4dWmEd02223MWDAAO666y7atm1Lp06duPXWWysc8/7776PX62nevDmBgYGsWLHiiudZuHAhkZGRREdHc/PNNzN69Ojy97RaLRs2bGDfvn1ER0fj7+/PmDFj+Gvi/EsvvcRHH32Eh4cHY8eO5a677rLvl1BLiLo41M+ROnfuLHft2lVj7T/7wlJ8TrbBrSidVj6b6fLa2zV2LkWpKUeOHKFFixaODkNxgEv93wshdkspO1/sGNUjqmXmzZxOVpP9FLoFcTi7F3see9DRISmKotQolYhqoQUzZ5LeeD9G9xAO5vVmz6Ojr3yQoihKHaUSUS319MwZpEbvo8A9lIP5N7Fv7L2ODklRFKVGqERUiy2aNZPk6L0UeIRxoLAffz58V51cvkNRFOVyVCKqpKstA1Fdz82aSWLUXvI9IthfPICDD9ypkpGiKPWKSkSVdLVlIK7G87NnkhC1h3zPSPabB3Jo1B0qGSmKUm+oRFRHvDh7Jqcjd5PnGcU+6yAO3XMbtnMrAiuKotRlKhHVIS/NmcWpiN3keTVmP4OIu+c2bGazo8NSFEW5KioR1THL5s7iRMQucrxi2KsZQtxdw7GdV59EUZTaraSkBCEEiYmJF31/7dq1DB06tEptdu/enQ8++MAe4TmESkR10PK5szgZHkuOd1P26W7h6J3DsJWWOjosRakTzl/WR6PRVCgZ/uGHH151+wsXLixvz8XFBZ1OV/76/LIOl/LII4/w9ddfX3UcdYlKRHXU8nmzOR4WS7ZPM/Y538qxO27FWlLi6LAUpdY7vzx4REREhZLh991331W3//TTT5e3t2LFCvr06VP+evfu3VfVtqWe3hdWiagOe2X+bI6FxZLl05y9hmEcu/0WrOctYa8oStUYjUZcXFzIzy8r4jd//nycnZ0pLi4GYPr06cyaNQsoKxt+7733lpf4fvHFF6s0mvXbb7+lSZMm+Pj4VKj0unr1avr37w/8fRlv1apVNGnShNatWwPwzTfflJcHf/LJJyu0GxcXR8+ePfHy8iIgIID777+/+l/INaISUR336vzZHA39g2y/Vux3G86J227Bkpvr6LAUpU5yd3enbdu2/PrrrwBs2bKFsLAwduzYUf76r6qsjz76KGazmdOnT/PTTz+xatUqPvroo0qf6/vvv2fv3r3s2bOHd955h82bN19y3w0bNrB792727t1LSkoKI0eOZNmyZWRkZBAQEMD5CzHPnj2bESNGkJuby5kzZxg/vvaXlVGF8eqB1xbMYdLTi2lBd/Yisd52CzEf/xen8yo3KopDfTcLUg/U7DmC28DgJVfdTO/evdmyZQv9+/fn+PHjTJ06lS1bttC5c2f+/PNPbrjhBkpLS/nvf//L8ePHcXd3JyYmhsmTJ/P+++9X+vLenDlz8PT0xNPTk169erFv3z769Olz0X3nzp1bXozv/fffp0uXLgwbNgyAGTNmsHz58vJ99Xo98fHxpKamEhISwg033HB1X8g1oHpE9cR/Fs7hcKOdZPq1Ym/gKI7+awSlp045OixFqXN69+7N5s2b2blzJ507d6Zv375s2bKF33//nTZt2uDp6Ulqaio2m42IiIjy4yIjI0lKSqr0eapS4vv8UuMXlh7XarWEhoaWv16+fDlFRUV06NCBtm3b1onRdKpHVI+8vmA2jz27hJZ0Yq92LLb77uW61W9gaNfO0aEpDZ0deirXSs+ePdm/fz/ffPMNvXv3pn379sTFxfHjjz+WX5YLDg5Go9Fw5swZGjduDJSVAD8/IdiTEKL8eUhISIXLeDabrUICDA0N5e2330ZKyZYtW7j55pvp1atXhaRZ26geUT2zct4sjjTeR55nGHtiJnJk/ASMv/3u6LAUpc7w9vamVatWrFq1it69e6PRaOjcuTNvvfVWeSJydnbmtttuY86cORQWFnLy5EleeeUVRo0aVePxDRs2jNjYWDZs2IDZbGbp0qVkZ2eXv//pp5+SnJyMEKL8cp5Wq63xuK6GSkT10OszpnOi+VEK3P3Z0/wJ4qbOIm/DBkeHpSh1Ru/evZFS0rFjx/LXhYWF9OzZs3yfN954Ayi7JNe3b1/GjBljl+HfVxISEsInn3zC5MmTCQgIIC0tjc6d/y58un37djp16oS7uzt33nkna9asqbGemr2oUuFVVNOlwu1p2uo3CPuzEa4lJtoffI2Yx8fgew1+sSmKKhXecKlS4UoFLz06nqxuORQYNOxp+yTHX3uPjNdecXRYiqIoFahEVM8teuB+zL2tFBhM7Gn7BMc/+pnUpxc4OixFUZRyKhE1ALNG/gu3Ia7kGvLZ12Yix388RNKUiaqmkaIotYJKRA3EY4OHEHlnKFlu6fzZ+lGO78ol8ZHRyHq6dpWiKHVHg0xEQggfIcQGIcQxIcR+IcSPQogYR8dV0+69sRcdR7Uk3f0sh1o+xLHTzpy55zasauVuRVEcqEEmIkACK6SU10kp2wEbgLccHNM1cUunzgwc151kz5PENbuPuJxIztw+GOu5RR4VRVGutVqRiIQQYUKI14QQ24UQRUIIKYSIusS+4UKIz4UQeUKIfCHEF0KIKk0ZllLmSil/Pm/TNuCi56uPrm/WknufGEyC1xFOxNzOYWtHTo8YhDkjw9GhKYrSANWKRATEACOBHODXS+0khHAFNgHNgQeA0UBT4BchhNtVnH8y8L+rOL7OaRUWwaTp/+KEzwHio4ZwyLU/8bcNxpSQ4OjQFEVpYGpLItoqpQySUg4BPrvMfmOBxsAIKeWXUsr/AcOASKB8rXMhxM9CiMxLPCosRSuEWHiuzdl2/1S1XIR/EPPmPESc314Sw/qw3/92To0cQfGhw44OTVEc7tFHH2XRokWODsPh+vTpw1tv/X3nYt68efj7+1dYtPVq1YpEJKW0VXLXYcAOKeWJ8449DfwODD9vW38ppf8lHuULrwkh5gFDgMFSygZZUc7Pw5Ml8ydwIHAXacHd2R8+mvgH7qVw505Hh6YoNSYqKgonJycyMzMrbO/QoQNCCOLj41m9ejXz58+v8Vi2bdtG37598fDwwMvLi6FDh3L4sH1+DG7evJmwsLB/bL8wuVTWmTNnWLZsGYcPHyY1NdUeIQK1JBFVQSvg4EW2HwJaVqWhcz2hocDNUso8O8RWZ7m5uPKfBU+yL2QXmf7t2dNkLPETxpP/40+ODk1Rakx0dDQff/xx+esDBw5QdI0rHG/fvp2bb76Z4cOHk5yczOnTp2nXrh033HADp2phGZczZ87g5+dHYGCgXduta4nIl7L7SBfKBnwq24gQohXwFOAHbBFC7BNCXHIBOSHEOCHELiHErox6ekNfp9Px5sIZ7AnbRY5PM3a1mETCjJnkrr/clVJFqbtGjx7Ne++9V/563bp1FcpqP/jgg8ybNw/4u2exbNkyAgMDCQkJ4Z133inf98Iexrvvvlu+QKqUkilTphAYGIinpydt2rTh4MGy39MzZszg/vvv54knnsDDwwNfX1+effZZunfvzlNPPVWpc5eWljJt2jQiIiIICgri0UcfLS9tXhk5OTnceuutBAQE4OPjw6233kpiYuI/9vv5558ZMGAAycnJuLu78+CDD1b6HFdS1xKRXUgpD0kphZQyRkrZ/tzjoovxndt/jZSys5Syc0BAwLUM9ZpbO28Ge6P2k+8RQWzbJ0h4fimZa95wdFiKYnfdu3cnPz+fI0eOYLVa+eSTTy5bxiE1NZW8vDySkpJYu3YtEydOJCfnYr+LK/rxxx/ZunUrx44dIy8vj/Xr1+Pn50dRURHbtm3jzjvv/McxI0eO5Kef/r4icblzz5o1i2PHjrFv3z5OnDhBUlISzzzzTKW/B5vNxkMPPURCQgJnzpzBYDAwadKkf+zXv39/vvvuOxo1aoTRaOTdd9+t9DmupK4Vxsvh4j2fS/WUlGpYO2sq415+hVbHr+OP9lPovOpVrFmZBM2e6+jQlDrqhT9eIC47rkbP0dy3OTO7zqzSMX/1inr37k2LFi0uWy5Br9ezYMECdDodQ4YMwd3dnaNHj9K9e/fLnkOv11NQUEBcXBxdu3YtX5k6MTERm81GSEjIP44JCQmpcP/qUufu1q0ba9as4c8//8TX1xcoK0F+77338vzzzwNlFV3/qkv0F6PRWJ50/fz8uOOOO8rfmzt3LjfddNNlP5O91bUe0SHK7hNdqCWghnrZ0Zonn+BE63iMBg/+6DiVs5/9QPL0KWp9OqVeGT16NB999BHvvvtuhctyF+Pn54dO9/dv9yuV9/5L3759mTRpEhMnTiQwMJBx48aRn5+Pj48PGo2GlJSUfxyTkpKCv7//Fc+dkZFBUVERnTp1wtvbG29vbwYNGsT5txAaNWpEbm5uhcf5dZWKiooYP348kZGReHp60qtXL3Jzc7FarVf8bPZS13pEXwEvCSEaSylPAZyb+HoDMKsmTyyEGAoMjYmp9ysBlXtt4gSmv/02wXv8iO00lU6//AfLg/cStuZdNM7Ojg5PqUOq2lO5ViIjI4mOjubbb79l7dq11W7Hzc2twkCHC0eUPf744zz++OOkp6czcuRIli5dyqJFi+jRowefffbZP3og69evp1+/flc8r7+/PwaDgUOHDlW7+N2yZcs4evQoO3fuJDg4mH379tGhQ4dr+qOz1vSIhBD/EkL8C+h0btPgc9t6n7fbm0A88D8hxHAhxDDKJqKeBWr0RoaU8msp5TgvL6+aPE2ts/Thh8m/voh8FwuxHSeTGGck4c5hakkgpd5Yu3YtmzZtws2t+nPi27dvzxdffEFRUREnTpyokNRiY2PZuXMnZrMZNzc3XFxc0GjK/vQuWbKEdevW8eqrr1JQUEBOTg7z5s1j+/btLFy48Irn1Wg0jB07lilTppCeng5AUlISP/zwQ6VjLygowGAw4O3tTXZ2Nk8//XQVP/3VqzWJiLKJrJ8Bj557vfLc6/JvRUpZCPQFjgHvAx8Cp4G+Usor95GVann63nvQDdCS61rAnvaTOJvlTfzwQZhS7DePQFEcpUmTJhVKbVfHlClTcHJyIigoiAceeKBCyfD8/HzGjh2Lj48PkZGR+Pn5MX36dAB69uzJDz/8wBdffEFISAiRkZHs3buX3377jaZNm1bq3C+88AIxMTF0794dT09P+vfvz9GjRysd++TJkykuLsbf35/u3bszaNCgqn14O1ClwquoLpUKt7f3tmzi5FdnCSgMpXncR0SW7iNs7YcYWqqS0EpFqlR4w6VKhdcgIcRQIcSavLyGO/f1/t596Xl/O5I8TnO0+SiOe/bkzOiRGH/7/coHK4qiXIJKRJXUUO8RXWhAu/bcNakfJ72PcDp6BEdCh5H42Dhy/+//HB2aoih1VF0bNafUAu0jG+M3fSRLXv4A6IfJyR3b/HmYk5IJmDTR0eEpilLHqB6RUi3hfgE8P3ccewN3k+Hfjd1tx5O2ag0p8+aquUaKolSJSkRKtXm6urF6wRR2he4m17MlOzv/m4wvfyBx3BikxeLo8BRFqSNUIqokNVjh4nQ6He/Mn86eqP0YXcPZ3m0KmX8cJv6uf2EtLHR0eIqi1AEqEVWSGqxwee/MmsqR5scpcvFmW7epZJ3OJX7ErZjPTbJTFEW5FJWIFLtZ/cQkEjukUuSsY3uXJ8nIdSH+tqGUHD/u6NAURanFVCJS7GrZmEco7VmK0aWU2E6Pk0ojztxzp6r4qijKJalEpNjdvLvuwucWD3IM2exr8xiJHq1JHPcIuV997ejQFKXcu+++S5s2bXB1dSU4OJgJEyaQm5vr6LAaJJWIlBrx6M2DaHNPDGnuiRxp9jCnQ3qSOmcmmW++6ejQFIVly5Yxc+ZMli5dSl5eHjt27CAhIYEBAwZgMpkcHV6DoxJRJalRc1V3W9duDBnfg3ivY5yMGsnRmMGkL3+ZlGcWOTo0pQHLz89n4cKFvPbaawwaNAi9Xk9UVBTr168nPj6eDz74gKeeeoqRI0dy//334+HhQatWrTh/jcnk5GTuuOMOAgICiI6O5tVXX3XgJ6r7VCKqJDVqrnqub9qcsZOHc9T3IInBt3CgzV3kfPQxZydOUnONFIfYtm0bJSUl3H777RW2u7u7M2TIkPIS3V999RV33303ubm5DBs2rLx8ts1mY+jQobRr146kpCQ2btzIihUrqlR6QalILfGj1LjrQkJYOPtBZr/4Fu3oRWwXdzr9sg7rffcR/u67aA0GR4eo1LDUxYspPVKzpcKdWzQneM6cK+6XmZmJv79/hYqnfwkJCWH37t00a9aMnj17MmTIEKCskuuKFSuAsvpCGRkZLFiwAIDGjRszduxYPvnkEwYOHGjHT9RwqB6Rck34eXjy+sJ/szt4D0a3jmzv8RgFB48Rf9twLFlZjg5PaUD8/f3JzMzEcpEe+fkluoODg8u3u7q6UlJSgsViISEhgeTk5PLS3N7e3ixevJi0tLRr9hnqG9UjUq4ZvU7P209N46HFy+hwti1bb3iCnttXcnrEMMLfehuXZs0cHaJSQyrTU7lWevTogbOzM1988QUjR44s3240Gvnuu+9YvHgxiYmJlzw+PDyc6Ohojqv5cXajekTKNffOnKn8GXMEsy6ILTc8SYEREu65m5zPPnN0aEoD4OXlxcKFC/n3v//N999/j9lsJj4+npEjRxIWFsbo0aMve3zXrl3x8PDghRdeoLi4GKvVysGDB4mNjb1Gn6D+UYlIcYi3pj7OyTZnMOvc+LXHdPKcfUlduJCkmTORavisUsNmzJjB4sWLmTZtGp6ennTr1o3w8HA2btyIs7PzZY/VarVs2LCBffv2ER0djb+/P2PGjEGNqK0+VSq8koQQQ4GhMTExY1WX3H7mfvABnn+44mI20PbMOwSfPIBTTAxhK1/HOSLC0eEp1aRKhTdcqlR4DVLDt2vGc6NG4TrQiSzXdA6HP8rBbgMpPXGS+DvuIO+77xwdnqIo14BKRIrD/fuWW+n3SCeO+hwm3TCMLX0fxmwqJXnaNFKefkbNN1KUek4lIqVW6N2yNXNnj2JX8B4stvb82Gs6Jn8Pcj/+mPi771HlJBSlHlOJSKk1gjy9eXvhVPbEHEDafNjUei7ZrSMpOXiQ08NHULD1V0eHqChKDVCJSKlVhBC8M20KWT1yMOoL2eM/lWO9u2PNzyZp4mOkL1uGtNkcHaaiKHakEpFSKy0eNZomIxtxxvMUiWI0vw64C43BQtabb5HwwINY1HL9ilJvqESk1Fr3Xt+LMVNuZV/AHswlvfn6hiloo50ojo3l9LDhFJ63GrKiKHWXSkRKrdYsJJT/zH+cHRG70JZG8FXMAkzdfLFkpHP2kTFkrn4DNRdOUeo2lYgqSdUjchxnJyfenzODY21OYRGw2XU+STc3R6spJuOVFZwdNx6r0ejoMBVFqSaViCpJTWh1vJUTHsN5oI4MtxSOlv6bbf2H4BpsovDXXzk1fDjFhw45OkSljoiKisLJyYnMzMwK2zt06IAQgvj4eB588EHmzZt30eOFELi5ueHu7l7+ePHFF69F6PXSVSciIUSMEKKHEOI6ewSkKJcz5ZbhDBjThSO+BzAWDOWLjv/Gs50ZS1IyCaNGk/XBB44OUakjoqOj+fjjj8tfHzhwgKKiokofv3//foxGY/ljxowZNRFmg1CtRCSE0AkhFggh0oCjwG/ArPPev08IsU0I0dpOcSpKud7NW7Ng9gPsDPkDbVErPgpehFM/HcJWRPri50mcPBlbaamjw1RqudGjR/Pee++Vv163bh3333+/AyNquKqciIQQOuBbYCHgDRwBxAW7/Q50B+642gAV5WICPbxZt2Amu5vuQ1jd+IrnyR3YCIOviYLvf+D0iBGUnjrl6DCVWqx79+7k5+dz5MgRrFYrn3zyCaNGjXJ0WA1SdQrjTQL6Az8DD0gpU4QQFWYYSinjhRAngJuBp68+TEX5JyEE656czuyP1uESq+OP/DmE9PyYnkd/IudIPPEj7yJo3ly8R4xwdKgN3q/rj5F5tmYHlPiHu3PjyKrdIfirV9S7d29atGhBaGhopY/t2LEjGs3fv+U//fRTVSq8mqqTiEYDWcBIKeXlZhUeATpUKypFqYLn732AjxtvJfbLg2hy7+Pzpo0ZFbCS1G2QMnceRTt2ELxgARpXV0eHqtQyo0ePplevXpw+fbrKl+X27NlDTExMDUXWsFQnETUDNl8hCQEUAAHVaF9Rquye7r3oEBnNi6s+olN6D9a4NeKhQYvI/82JvC//R1HsLoLmz8ejT29Hh9ogVbWncq1ERkYSHR3Nt99+y9q1ax0dToNVncEKEqjMYl+NgJJqtK8o1dI8JJxV86bwe9QOnEqDWVfyMqV9vfBpUYI5OYnESZNInDxFLQ+kVLB27Vo2bdqEm5vbP96zWq2UlJSUP0yqenCNqE4iOg20E0Jc8lghhAFoS9nlOUW5ZpydnPho1hyOtDuBWWvmp5xn2NeiC9EDM9C7Win4/ntODx1G3ldfOTpUpZZo0qQJnTtftHAoS5YswWAwlD/69u1b/l67du0qzCOaPHnytQq53qlyqXAhxLPAbGCWlHLpuW024F0p5cPnXi8EFgBzpJQv2Ddkx1ClwuueZd9+SdYvmYQWNKbUYy/jnJdTdExH1kFXQOB6fQ8aLV6MPjDQ0aHWO6pUeMN1rUqFvwykAkuEEB8JIW4/t91fCDFYCPE2ZUnoDLCyGu3XSmplhbpn6pARDBzTlV2BO9EZ27I6dyXHmramyZA0nH1sFP32O6eGDSf7ww/VenWK4kBVTkRSymxgEJAA3A18Rtl9o1uADcCDQCIwVEpZYLdIFaUaejdry+r5U/mjxR6MTnnsz3qS18UcgvrmE9i5EGnMJ+3Z50i49z5MZ844OlylEqSUWAsKVF2qeqRaKytIKQ8ALYGJwDeU3Qs6BmwEpgItpZQH7RWkolwNZ70THzw+k+vuCmd34E60xrasylnJsai2NLk1GUOQpHjvXk7ffgcZK1chLRZHh6xchjU3F2t2Nubk5LKEpHqzdV6V7xE1dJ07d4fXOuAAACAASURBVJa7VB2cOquwtJgxq5bTIiESv+IQzO57Ge+6jNIkLamxXkiTFefmzQl5fjEGdY+j2mrqHpGtuBhLenrFjTodWm9vNK6uCHHhIi/KtXat7hEpSp3l5mzg48lzCL0jkN1BO9Aa2/J69iqOBrch5tZE3MKhNC6OhHvuJXXJEmxquG612ftHrrRasWRlIW0SaTb/3b7FgjUzE0tqKraSq58xon6cV191v7vqrDXnJIQIFEK4XLDdXQjxrBDiayHEa0KI8GpFpCjXwCM9B7Bi1mS2XreTIr2RXVnTeN08j8AeuYT1yUMIMznvriurBLt7t6PDrXP0ej3FxcV2a09KiSUzE2mxIIuLsJWUYCssxGYylf/xkyYTlrQ0zGlp1f4BYZM20orSMFvNdou9ITGbzeh0VV8noTo9ovlACuct33NuTtFWyoZ130LZvaPtQgi/arSvKNeEp8GVz56ch99wL/YE7kBT0I7XsldzzL8DTW85i2cTiSk+nrMPPkTynLnYqlAioKELDAwkKSmJoqIiu/QwbPn52EpKkCUlSJsN4eyM0GiQpaX/TEglJVhSUsoTV1VkF2dTYCogozjjqmNuaGw2G2lpaVRnZHF15hFtA8KklBHnbbuDstFzB4BXKEtGtwHzpZTPVTmqWkzdI6qfsgrzGbd6Oe0Tm+JbHIzFYy/jDC8hcyFxmz9WowVdSAjBC+bjcdNNjg63TsjPzyc9PR2z+ep6F9JiwZqfX5ZULBbQ6hA6bdmbNom0WuCvEXQ6HUKr/ftgIdA4OyMMBoTm8r+7Sywl5JnysNqsaIQGHxcfnLXOVxV7Q+Pm5kZYWFiFxWD/crl7RNVJRMnAISnlgPO2vQfcB7SXUh4410M6C6Rc6sR1lUpE9dvSH78g7rc42qd3pURvpJ/Xm3TQ/EbawWBy4rQgNLj37UvIs4vQqTllNc5WVETqc89ReuIkJfv3o/X3x6VVq38MSrDm5WFKSMCanQ06HU7h4ehDQxHnLhMJgwueAwbg3r8/Gienf5znVN4ppm+ezrHcY3g5eZFnyqONfxvWDlyLQWe4Jp+1vrP3YAVfIO2CbdcDCeeGdSOltAE7gQgUpQ6ZfvPtLJ42gZ9jtlKqLWZ75lReK16IT+t8ogdnoffQYPzpJ04NuYXcL790dLj1XvYHH2JOSqbk8GE0rq64NG+OEAJ9eMVb0FovLwxt22Lo2BGtpyem06cp3LEDU3z8uftKJeR99TWp8xdg3LoVabWWH5tvymfJziUcyz2Gr4svfSP64uPsw+Gsw3wc9/GFISk1oDqJyAyU/xQUQgQCjSmr0nq+IsC9+qEpimMEefrwv2nPYLtZw97AHYiCtrySs4o4Q2eaDEzAr70Fa24OKXPmkvDQQ5gSEx0dcr1k3LqVothYSg4eBJsNl9atETodHgMHEjx3DkHz5uLWozvo/r4Up/X0/DsheXlhio//OyGZzVjz8sj56GNSn1lE0Z49WG1WVuxeQWxaLK46V/qE9WFW11kMiByATdr48MiHnC0468BvoWGozqW53UA00EhKWSKEmAD8B5ggpVxz3n4bgSZSyig7xutw6tJcw3I2J5OJb62ga1IrfEqCsHnsY5zhRUSJjcRtIZRmmtC4ueE1fBj+U6ag8/BwdMj1gikxibQlSyj5808s6em4tGmDzs8P1+7d8HvwwQr7WvPyMG7dinHLVmzGisX3rAUFmOLjsWZlgVaLPiwMp7AwhF4PwFHfUhY0O0CxsNArrBcLeizAz+BHWmEaY34cQ3x+PP0j+vNyn5fVHKWrZO9Lc59RViJ8qxDiZeAFwASUX6cQQmiBjsCJarSvKLVGuI8/X01/lsKbzOwr7x2tJs65C9H94gnuYQariZyPPubUoMFkvfOuWpnhKtlMJrLWvoXp9Gks6ek4RUWh8/PDpWVLfC9Sylvr5YXX0KE0WvwcPqNGoQsJ/vs9Dw8Mbdpg6NQJrY8P5oQECnfsoPTUKXKMmbzptZdCWwl3nPRjSqP78DOUDfQNcgviodYPYdAZ+D35d34588s1+/wNUXV6RM7At8BfQ4eswGQp5evn7TOYsqV/npJSPmOnWGsF1SNquI5npvDk2/+he3IbvEsCkZ77GGdYis5WTNqRRuTFaZAWK05RUQRMn4Znv36ODrlOyn7vffK++abC4ASnyAgCp05F41y5UWzFhw5h3LiJksOHK2y3Go1lgxoyMjDp4JsuAquHK7cbm+Hp7IVrt654DRuGztcXs9XM45se57fk34jxjuG9we/h4aR6vNVl11Fz5xoUQE8gCNgjpTx1wfs3UVaP6Csp5emqh1x7qUSkTPnvWgr3p9E2vStFeiM9vb+iu+a/2Mw6UvaEYEywgRAY2rcjeMECXJo3d3TIdUZRbCwZr6+kaNcuNE5OGDp2RBccRNCMGWg9Pavcnjk5mYKNmyj64w/kuWHkVmnlO8t+nLIL6HFEgkaDU2goTuHhCCcn0GkxtG6D2/U9OBUM/948mcziTB5u/TCTO6maQ9Vl90TUEKl6RMr5DqaeZeZ7r9MjuR0+JUHkO2dwi+entNVupKTAheTYQErTLQgnJ9z79SNo7hz0/v6ODrtWM6enk/rssxRt246tuBjXTp3QBQYSOH06+qCrqxllLSgou4+0eQtbCvaytGsq4bla5mwPxi/t3Pp1Gg360FD04eHlQ7y1Xp5sCs1jmdd2PJy9ePPmN2nm28weH7fBUYnIjlSPSDnflC/WknToFJ0yOuJu8iHfkMxIj3U00fyBMd2dlFhfLAUWNO7u+NxzN/6TJlX68lJDIi0W0l5cSsH335cPTtCHhBAweTLOjaPtdp7Pj3zKizuW4GSyMXdbMK0MjRGArbAQU0JCeULSBQSgCwhA6+ODRdhY6/0nX7UqZrC5Oc/evRYn96r3zhq6GklE59aS6w00AlwusZuUUi6q1glqKZWIlAsVmkp57LPVmE9m0jG9MwaLB0a3eEa7rSFEHCEn3oeMfe7YSq3ogoLwnzQR73/9S43COk/u55+T9c67mE6exCk6GqfoaPwnPIqhTRu7neNAxgHG/zyeQnMhQ6KHMMfvPkp+2ULJwUNw7u+gragI09mzWDIyzq3ioEXn54fR25l5fdLIdZW8sOc6WnQbjNv11+N8bl6TcmX2XllBR9lw7THAX/8DF/5PyHPbpJRSSz2iEpFyKdmFRiZ88jrOicW0T++Ck9UFk/sxHnBdhY9MIONwANlxTmCVOF13HcFz5+DWrZujw3a44gMHSX3uOUr27SsfnOA7ejTuPW+w2zmyirO479v7SDIm0S24G8tvWl4+8MCcloZx0yYKt+9AnlssVdpsWHNzsaSnY8nMBIsFi1awoxnk+TgxgvY46w1ofXxw69Ed1+7dVcn5K7B3InoWmANYKBs9dxwwXmp/KeXTVTpBLacSkXIlSXk5TPr4dXxTJG0yuqC1adF4HGSUYSWu5kzS9gZSEK8BrRa3bl0JeuppnCMa5mL11txckufOw7hlS/ngBK/hw/EaeqvdzmG2mRn741h2p+0mxjuGVf1XEewW/I/9rMZCCn/7DePmzVhzc8u3/5WUStJSKM7JwGACq1bg7Hfu8p2vL0KnwzkmBrfre2Do2FFdfr0IeyeiBMqW+blBSvmnHeKrU1QiUirrWHoqUz5dRXi6Cy0zOyGRuHvu5h7n1WiMRaTsCqAkHYSLC5633ELQrJloG9CEWCkl6UtfInf9+vLBCR4DBuA7+p9zha7Gou2LWH9sPQGGAFb2X0lz38uPYpQ2G6b4BEoOH6bk0CFMCQnli6oeK0rgw/Az9Dws6X5cg7BYyu4p+fmhCwxE6+uLxtUV104dcevRA+emTe36WeoyeyeiYmCjlNJ+P1nqEJWIlKranXia+Z+vpUmGN9dltceqMRPo8Rt3OK/FnC5J2e2HpUCi8fLC75GH8Xv44fLFOuuz3A0byHhxafngBPc+vfGfMOGKq2RXxRfHv+CZ7c/gonNh6Y1LuTH8xiq3YSsspCQujpJDhzEeOsAHpl/5uL2RkfvcuC8pAmtGJtbMzLLh4X8lpYAAtH5+6IODcbu+B67du6Pz8bHb56qL7J2IjgIHpZR32CO4ukYlIqW6Nh47wtKvPqBVVhCNc1pTqi2micePDHH6CGO8M+n7PbGVSvShoQROn4bnoEGODrnGlB4/TuKUJzGdOIFTdDTuffoQMGXyRVfGrq5DmYd4+IeHMdvMTO00lfta3meXdnfs28C8XYvIEyU8vzOaZpqQsst3eXlYMjKwZmSUJyXtuaSk8/fH0LoVbj16YOjQoUH80LiQvRPR05QVvouSUl7y3lB9pRKRcrW+OLCbN7/7nLZZEUTkNaNYV0B7j6/orfmSnDgPso4YQApcWrUiYMpk3Lp0KZtoWU9YjYUkTZ5M4W+/ofX3x/2mmwiaOQOtu/3WSM4rzeP2r24noyiDe5rfw+xus+3WtpSShdsW8uWJLwk2BLK28TzcT2dQcvgwlpRUpJRlAx0uTEq+vugCAvAcOBC/8eMa3Gg7eyciZ2ATZYMVxkopj119iHWHSkSKvazduYUvfvmeDpkxhBijKdbncoPHp3S2/ETafh8KEs5NqvTxwaVVKzwGDsRzQH+03t4OjvzqpL3wAtnvf4DGyQm3Pr0Jnj0bXUCA3dqXUnLft/dxIPMAN4beyKt9X0WnsW8PJMWYwtgfx5JQkMDg6MG8cOMLCCGw5OSU3Vs6fJjSI3FYCwux5eWVj76TJhPCxYWAJ6fgd//9do2ptquJJX7cgO1ACyABSARsF9lVSinr1YJbKhEp9rbsl+/YvP1XOme2wL8olGKnDAa4v0ezoh3knPLCmOSMpbBsX2EwlF3K6t0Lr2HDcI6232TPayHv+x9InTcPW3Exbj16EDx/Hk6RkXY9x1PbnuK/x/9LU++mfDDkA1z1rnZt/y/rj67nxdgX0Qoty/oso2dozwrvXzjooTQ+HmtmJiWHDqFxdSXk+cV4DhhwidbrH3v3iPyBnyhbS+5KfUs1j0hRKkFKycLvvuDA3j10zihbVLXYJYl+rv9HS912NMWl5Ce6kn/WlZIsbdlMPa0WfXAwrl264DlsGG5dOpeXN6iNSs+c4cz9D2BJTcWlfTuC58/H0KqVXc/x+bHPeWb7M/gZ/Pjklk8Icguya/vnK7WWMnHjRHam7KSZTzPWDV6Hm97tkvv/Neghc80aCrdsRevrS9jr/8G1Q4cai7E2sXciegt4GDgKrKas1MPl5hFtqdIJajmViJSaZLVJpnzxPilHjtM5oz3uJh9s2ChxTiPIOY7rtdsIFwcoTRPkJ7pRmOKMrWwOJlpvb1xatcT95oF4DhpYq0qZ20pLSbj/AUr278cpOprghQtw697druc4mHmQB757AK1GyzsD36GVv32T3MX8mfEnj296nJzSHB5t+ygT2k+44jGW7GwSH5tI8b596ENDCX/rzTrXs60OeyeiFMouw7WUUubZIb46RSUi5VooMVmY+PlaUhISCCx1IawwhEBjFFqpxSqsmFySiHQ6QDfN7/gaT1OU5ERBogFTftkFCOHijFN0Y9x73YjX8BF2Xa+tOlIWPkXup5+i9fcnaO4cvAYPtmv7eaV5DP2/oeSb8lncczFDGg+xa/uX81LsS7x/5H18nH14e9DbNPZqfMVjTImJnB3/KKaTJ3Fu1oyItW+hq+eL4to7ERmB76SUd9ojuLpGJSLlWkvLN7Ju1+9sPhKLJq+Y4BJXQgtDCSgMQ6DBIkzYDAk0ddpLe/MuDGnJGBNdKE53QtpE2dyW4GBcO3fGa/gw3Lp1u6bDh/O+/prk2XPQODnh/8Tj+D3wgF3bl1Jy59d3cjTnKOPbjmdSh0l2bf9KsoqzGPvjWI7nHqdXaC9e6/caGnHluVDFhw+TOHESlpQUDF27Er5yJVr3S1/aq+vsnYhigSwpZf2d5HAZKhEpjpaQk8O6P35j29G9OOWbCCl1I8wYgW9xCABmTQlawwmaa3bRLO8guqRMjEkGrCVlfxw1nh64tGqNR9++uLRuhb5RI7Q+Pnadw/OX0oQE4v91J7aiInxGjSJo1ky7D1uetXUW35z+hv4R/Vl+03K7tl1Z3536jqe2P4VVWll0wyIGR1eux2f89VeSZ8zEmpuLx8CbCX3xxXo1VP989k5ED1J2b6htQxu6DSoRKbXP8Yws1u7YzN4TB3EttNCoxJPQgki8Sssu9Zi0RgwucbQ07yU8LQ55toDSbB1/jTUSOi3CxRmNm3vZXJegYPSNQnCKiMCpcWP0QUFovX3Q+XhX6Y+kzWTi9LDhmOLjcb95AGEvvWT3P7IfHv6QJbFLaObTjPW3rkdjx1UZqsJiszB181Q2nd1ElGcU7w9+H2+Xyg2zz/nvf0lf/Dy2khJ87r2HoDlz6uUco5oYvr0EuB+YD/wgpUy8uhDrDpWIlNpMSsmBpFTW/rGZo6fj8CiEkFIvwvKjcTOXDV4w6XPw0h2iqfEQ/oVpOBlz0eQVYC3UYC7Sgq3iH0Gh16Bx1iOcXdB4eKLz90fXKAynyEicIqPQ+fuj9fVB5+OD1sen/LJf4uNPUPDjj7i0bUPkO++gcbPvZac9aXt4+IeH8Xb25qsRX+Hp7NgaQcdzjjPh5wmkF6VzX4v7mNl1ZqWPzXh9JVlr1oCUBDz+b/zGjKnBSB3D3j0iaxV2l1LKerWWhUpESl1is0liz5zlnR2/EH/2FF5FgkYlPoQWRGOw/L2SgcSGSZ+P0OZgIBNvSzo+pZn4FGXjbsxBn58H+SVYirQgKyYqjRNonHVlvR1nA1pPLzSubhTtOYAuKJCo99eg9w8ArR40etA6wVX2XDKKMhj25TAsNguf3vopjb2vPEDgWnhj/xus3r8aN70bqwesprV/60odJ61WUhc9S+5nn6Fxdib4mWfwuvWWGo722rJ3IrrYxNVLklI6pq98BUKITymbkGsFzMBsKeXGKx2nEpFSl1msNn49dZr3dm4iJSUJvUlisGhws+pxt7jgYXHDvdQbd5MP2gt+Q1o0Jiy6XLQiB3dLBj6mTLxLMvE05mDIz0Wfm4etyFaeqPTuVqKGWtC5akGIsgeac881SKFBavRIoQOhw4oWi9Bh/utfqaXU5kyR1FFg02O06TFKHYU2HV+4xXPKJY2lgf3pE9oJjas36F3KkpzWqSzpXfZfp3Px2FeBqYBxP47jYNZBOgd1Zs2ANei1lZvbZSstJXn6DAp++gmtlxehr76CW9eudo/RUVSp8IsQQnhLKXPPPe8AbAT8pZSXTbQqESn1UWGphbO52RxOTePPpFOcykgmNy8XWWxGbwKDVYubxQk3iwseZnfcTd7ll/rOV6o1IjW5ONuysbqYsGp0SKkBqUWiA6n9+3HutZBahNSd+7fsuUZq0VRiLryLyMdNm4W7JguDNh9XbT4uOiOu+iLc9EW4uRRicLGg12vQ6nVlCUjndK5npq+YoDR60DmD3hVajYCQdtX6LrcmbmX2r7MpthQzq+ssRjYbWeljrfn5nJ04keLYXeiCgwl/601cYmKqFUdtU+sTkRAiDJgJdAbaAQYgWkoZf5F9w4HlwADK7rb+DEyWUp65ivP3Ab5AJSJFuaQSs5XkvFwOpSWzLyGe0xmJ5OUUQLEZnRkMFt25ZGXAw+yBVuqwCis2YSv/1yZsWLGVP7cJiY2yf6WQ2Cj7VwoJ2Ci7ACMRQqIREiFsaIUNg8UNZ6MPOpsGV5sWg02PxuaKRf5zOR+9KMJdk42bNhN3bRZummzctFm4agsw6ApxdSrEoC9F66RDq9MidE7gHgSNe0PHB8DVt0rfk5SSub/N5etTX9PIrRHvDX6vSis8mFNTOTv+UUqPHsWpSRMi3n4bfVDdr/5aFxJRH+BTYDegBW7mIolICOEK7AdKgXmULXTyLOBK2Si+wiqedzkwHPAC/iWl/OVKx6hEpCiXZrHaSCnIIy4lmcJSM96uBtxd9Lg6O2PQO+Gic8Kg1+Oic8JZp72q0WEWq40fD6fy0c6z7D2TQ6HJik5KWuqt9HCx0lprgSJJSbEFk1mH2arHbHPCZHMBKt4x0GDGTZuDuyYDd20mEc77iWyUhyEkAtrdDU0HVum+1tmCs4z7cRyJxkSGNRnGcz2fq9JnKzl+nMQJj2FOTMTQsSPhb6yu80UT60Ii0vzVExFCjAHe5OKJ6AngZaCZlPLEuW3RlJUrnyGlfPnctp+B9pc43XAp5e8XtDsIWERZ1VnT5WJViUhRap9ik4Uv9iTx3z2JHErOp9RiQysEkX6uDG4TzP2d/Amw5VKSkUphegZ5qQXkZZZSkCcxFukpMhkoMbtgtuqxWDVYpDMGTQ5NXXcQHZpDaDN/RLex4Nek0jF9dOQjlu1ehl6j55U+r9CtUbcqfabC2FiSJk/BmpWFe79+hC5/uUbmel0rtT4Rne8KiWgj4CKlvOGC7VsApJS9r+K8J4C7pJS7L7efSkSKUrvlFJr4cOcZNvyZzIl0IxabRK8VNA3yYHj7RtzTJQJPw3kDCMwlYEyDglRs+akUpaZyYEch8ekBZJtCEEiCnU8QEZRNm17BOHe9G5yuPBS9xFLChJ8nsCttF638WvH2wLervBJ43rffkrrwKWyFhXjf+S+Cn3qqzs4xqk+JKBX4n5Ry/AXbVwJ3SikrVdRECGEAgqWUp8+97gF8CzSWUuZc7liViBSl7jibXcS67fH8eCiNxJwibBJc9Bpah3pxZ6dwhrdvhIv+IoMi8pKw7XyTU4cKOJXow5miNpRKDwzafBr559JpcDQB3a/8u3dP2h4m/zKZfFM+kzpMYkybqs8PynznHTJfeRWsVvwmPErAY49VuY3aoD4lIhPwspRy1gXbnwVmVXbOkhDCF/gG8KCswF8hMF9KuekS+48DxgFERER0SkhIqNJnUhTF8Q4l5/Hetng2H8sgPb8UCbg76+gU6cO9XcPp1yIInfaC+0CnNsOe98lNzSfhrIG4whvJtDRBI6z4ehXTdkAM1/VujlZ36ftHz+98nk/iPsHP4Me7g94lwjOiSnFLKUl74UVyPvgAodcTtGA+PrfdVvUvwMFUIrIj1SNSlLpNSsn2k1m8tyOBnaeyyCkyA+DjqqdHYz8euiGKLtF+fx9QWgB73odTv2DOPEtGmuBA0SBOllyPRIurwUqTLmF0GByNh4/LP86XUZTBmB/HcCrvFP0i+rG8z/IqX16TJhNJc+ZQ8O13aNzdCV2xHPfrr7+q7+Faq0+JKA348movzV0NlYgUpf4wWWz8dDiVT/44y56zORSWWhFAvxZBvDGqI9rze0hph+CPNyH7FDL9CEX5JvYX38bBooGYbc7odBAU40OHmyOIaOFbIdl8ffJrFu1YVDa0u9tcRjQdUeVYrcZCEp94gqLff0cXGEDYG29gaNHCDt/CtVGfEtEmwElK2fOC7Zsp+yzVHqxQWSoRKUr9VGSy8H97knh3WzzH0400CXDj03E98Pdw/nsnqwUOfwkHv4CCZEg/gsVs5oB5JHsLhlBsdkcI8Aww0OL6EFr3DsPZoMNis/DEL0+wNXErkR6RLOm1pNLL/5zPkpXFmfHjKT14CKfISMLfeRunRo3s+C3UnPqUiCYDLwHXSSlPndsWRdnw7VlSymU1GNdQYGhMTMzY48eP19RpFEVxMCkl0z/fz393J+Fp0PP2A13oFOVTcaf8FIh9C1L2QdYJyElAavUcF8OJzRpCXokXUgqcXXWEt/Sl48BIst2SeWzjY6QXpdM1uCtLblxCgGvVL+KYEhI4M2485oQEXNq0IeKtN9HWomq8l1InEpEQ4l/nnvYDHgUeAzKAjL/KjQsh3Cib0FrM3xNaF1E26KCtlPKSJcvtRfWIFKVhePf30zz37REAFg5txajukf/c6fTWsvtHeYmQfghK8pAGX5Jd+rE782ZScgOwmCUancAv1J2MJnGsM72GRVoYEj2EBT0W4KSt+tygov1/kvTvf2NJT8et142EvfYaGmfnKx/oQHUlEV0qkC1Syj7n7RdBxSV+NlK2xE98TccIKhEpSkOy83QWY9ftoqDEwsguYSy5ve0/BxqUGmHfh3BiE+SdgcxjIK3g05g813bsNw7lZHIARfllgyIKfNP5tNmLGJxceKT1IzzS5pFqxVaw6ReSZ8/Glp+P14gRhDz3LMJB9Zgqo04korpCJSJFaVhS80q4e8124rOKaB/uzQePdMPd5SIDdNPj4I81kH0KMuKgIKVsAdXAlpg8r+OYfiR/HjCQk1JElu8ZPrvuZULcg5nTbQ59wvtUK7acTz4h7YUXkWYzfmPGEDj5iav7sDXocomo9qbPWkYIMVQIsSYvL8/RoSiKcg0Fe7nw05Te9L4ugH1nc+n38mZOpl/kLkBgcxj8InS8H8I6Q+i5v7lJu3BK/IXWxle45/ofiIoR+GVHcMupR0gxpvD67hWcStsHlsuuLnZRPnffjd+DDyCEIPudd8hZv/4qP61jqB5RFakekaI0XEu+O8Karadw1ml5+a52DG4dcvEdC9Jg11pI2lPWQ8o5BUIL/tdh8wjnfweHkpwXyuGQX/g16ktuxI3FujC8NE5ldZV0BtAbyp7rXcuel28zVHgtNXpSVqwj7/vNaFwNhK1chVvXLtf2i6kEdWnOjlQiUpSGbcOfyUz7bD8mi43H+sQwbWCzS+8c/zvseQ/yk8rmIRVng9YJm9Dxc9YEjhf3ZH/4FxwL2cTLWUbaCxc0Qlu20rf4//buPDyq8u7/+Ps72fcQQogEkFUNIIiyBxG3R6vyoLhVwa1SRURFf/rQVlv9Wa/aqu3jUkEFqZW6FaUiuAIFBEVFZFMQUFHZJSwhBEhIcj9/zISGEGBmMksSPq/rmmsyZ859zn2+jvPhbPfEgHm8j5qvD8zjfVRWGJve38quFaXE5jTj+H/+k/jc3MgVxQ8KohBSEInIqs27GPb8Z2wtLuXMtO/x3QAAF31JREFUE5sx/poexB5umJ+yEljyMqyZ6T1vtKcQXCUVFY7pG0ewfl83Pmz3MvEZ83iicCdZzsBVei94cJXeh5+2fZ3CT0sySOzSmeMnTcKTlBSiLa47BVEIKYhEBGDX3v0MnfApyzcU0TY7hVdv6kPz9EOH+Dlg62pYOB52/uc3PEvL45i06FL2laYz44S/sb/pCv5/TAt6eqqN7u2c71FxcEBV1girykoqCn9g84eV7PohmbSLLiLv0UfqzWjdCqIQ0A2tIlJTZWUld09exr8WbyA9KZbnru1B7+rj1B3SoMJ7zqgqPJyjZNd+Xhy7mfK9Ht7Jf46s1mU8cvIt5CVmH5jHu1fkDmp38Gu8zzvWUj79AX6cmUJpUTzN7rqL7OGBj/gdDgqiENIekYjUNHH+Wh5+13vz630X5nNdv7YBtd+xtYSXHppPRRm80+kZOndpy0P9Hwr494sA+PAxSqf9iR8+yMJZAnlPjyO1oODo7cJMl2+LiITRL/q35cUbepMUF8MDb63g/01eSmWl/+d2mjRLYfBd3SHWcf7Xv2TlyrU8t/w5gtpR6H8X8f0uJ69gO5WlZWy8527K1q8PfDkRpCASEQmBvh2a8v6dA2jdNJk3Fq3n4rEfs2vvfr/btzo+h4E3t8PMOHvF9cxetIB31r4TeEc8HuzCx0jq0Zvc04qo2L6T9SNHULlnT+DLihAFkYhIiByXkcSM0QMY0DGbZeuLOOcvc1m1eZff7bt1OYFOV2YQ4+Lot/wKJs2bzFfbvgq8I/HJeK6YSMZpeWS2L6F09bdsHPM/we1hRYCCSEQkhOLjYnjxxt7cPKAdhbtLGfz0R0xfttHv9v91ej+anl9GQkUyXRf/jCdnPUPh3sLAO5Kag2foJJr1jiMpu5TiWbMoHDsu8OVEgILITxriR0QC8esL8nniyu4Yxu2vLOaPvosZ/DH0wkFU9tlESlkGLT/pzZNzxrG/wv/DfAfk5BP787Ec128fsQkVbHt2LMWzZwe+nDBTEPnJOTfNOXdTRgP43Q8RqR8GndKCKbf0pWlqAs/M/Y5rn/+Usv0VR20X44lh+JWXUXjySjL25hAzszV/+2xScJ044TwSLrmPvIIiKC9n06/uofT7H4JbVpgoiEREwii/RQYz7zyDLi3S+XBNIec/MY+de44+wGl6fDq/uPpivj/hc7JK8tg0JYZZq4Lcm+lzC0nnXkVuzyIqikpYf8twKnaXBLesMFAQiYiEWUZyHG+NKmDwKS34rrCE8x7/kI079x61XduMtgy5YgCr2s0ju7gVHz+/nlWbg7ih3gy78E+kD+hN1om7KVu7no133o4L4BLzcFIQiYhEgMfj4Ymfd+fWge35aVcpFzw5j1Wbi4/arn9ef/pc1JFlrWeRtSuPKU98zvbdOwLvQGwCnqsm0rR/LinNS9k9/2O2/u9fgtiS0FMQiYhE0D3nn8RvL+pE8b5yhoz9iM+/337UNleedCWtz0xmSctZpO9ozguPzKZsfxAXLyRnEfuLV2ne30Nccjk7XpjIrvc/CGIrQktBJCISYb/o35Y/X96NsopKhk74lBkrNh9x/jhPHCO7jyS+106WHTeHhJ8y+dujM4O7L6hpexKGT+S4gj24ygo233cPpd9+G+SWhIaCyE+6fFtEQuni7nlMuLYHHoNb/vEFry388YjzN0lswuieo9l5ympW5iyg/McEXnlyXnArbzuA5GEPkturmIriMtbfdD3lxUc/TBguCiI/6fJtEQm1M07M4bWb+5IUH8Ovpyxn7Jxvjjh/xyYdubn7TXx78sd823QxO1aWM+354AZhtl43kn7JMJrmF1O2oZANI4fjKo5+aXk4KIhERKKoa8tM3hpVQJPkeB59bxUPTjvykD5ntT6LK/IvY1H+NNZlrOSHhUXMfm1FUOv2XPgHmlzQi9QW+9jz+VK2PPxQUMupKwWRiEiUtc1O5d07TqdFZhITP/qe219ZfMT5h+YPZWDbM5id/w9+Sv2Br2Zv4rPp3wW+4phY4q79OzlnNSM+rZyi116laOqbQW5F8BREIiL1QE56Iu+NPp2OOam8tXQjwyZ8etifkoiPiee2U2+jW97JvJP/DEVJP7Hw7bUsm70u8BUnpJEw6g1y+wNU8tOD97Jvpf/DEYWCgkhEpJ5IS4xj+m39ObV1JvO/KWTw0x+zr6y81nmzk7IZfepojm/Wijc7P8meuGLmT17DupVHvxz8EJmtSLnzZZr32kt5SQUbRlxD+S7/Rw2vKwWRiEg9khAXw+sj+nLWSTks31DEz56cf9ghgTo17cRNXW+iSWYab3T5M/utjPcnfElpAL+DdECrXqSP/CPZXUoo21LCxht/jiuvPQRDTUEkIlLPeDweJl7fk8tPa8nawhLOf3wem4pqHxLovDbnMaTjEDypFczo+HdKS8p5Z+zy4NZ72tVkXnMNaa32UrL8O7b87p66bIb/643IWhoB3UckIpH26OXdGHFGO7bs2scFT8xj9Zba7/W5vvP1DGw1kA1ZX7MqdwEb1+zkiw+CG2E7btBDNBvcg4TMcna+9S47Xv57XTbBLwoiP+k+IhGJhl/9LJ97L8ynaO9+Lhn7EYu+P3ScucTYRO449Q46Z3dm7vGT2Z20nU+nfse29bsDX6HHQ8JNL5JzVhaemEoKH/sje5cuCcGWHGGVYV26iIjU2fDT2/Ho5d0oK6/k6gmfMGPFlkPmaZ7SnLt73E12alOmnziO8soKpj+9lIqKIEbYjk8m9dfTyOntKN/r2HjrNZTvCGKgVT8piEREGoBLT23Jc9f0wAxu+ceiWocEOiXnFIblD2NvahEft5vC7h2lzJwY3M2upDUn43eTyT55H2WF5Wy8/r/ruAWHpyASEWkgzjwph1d+2YekOO+QQONqGRJoWP4w+rXox5fN5rEpaw3fLPqJVZ8eeVDVw7GWp9Dknj+R3mYvJasK2fXma3XdhFopiEREGpDurZvw5q0FZCbH88h7q/j9tIP3eOJi4hjTawyt01vzTvvx7I/bx5yXVrF7R2lQ64vteSXNbhxGYtMydr3wWCg24RAKIhGRBqZ9jndIoOMyE3n+o7Xc8erBQwLlpeYx+rTRxCfF8t5J4ykvK2f6X5cE97MRQPxlv6f51f3JeyY8V9ApiEREGqDm6Ym8d8cAOjRLYeqSjVzz/MFDAp17/LkMajeIjanf8lXreWzbUML8yUH8zDiAGckjnsdyO4Wo9wdTEImINFDpSXG8ffvpdG+Vybw1hVwy9mP2l//npxzuOPUOOjXtxLwWU9idto3ls9ezflUQQwABxMSGqNeHUhCJiDRgCXExvHFLXwae2Iyl64sYMm4B5b5LtlPjU7m/7/1kJmbw5olPUump5P3nvqJ0XxBDAIWRgkhEpIHzeDy8cEMvLjg5l+Ubirj82QUH7h/Kb5rP8JOHsze+mHknvMq+kv28Oy64IYDCRUHkJw3xIyL13dihp3FOfnMW/7iTqyZ8euDihKpLuldkfsrm41azYdVOFs848k+TR5KCyE8a4kdEGoLx157GGSdk89na7QzzhVGMJ4YHCx4kNyWXaa2fpTKpjE+mfsu2jUEMARQGCiIRkUbEzHjhhl70bdeUj77dxg0vLMQ5R3ZSNvf2vpeYOA/T8p+mssLx9l+DHAIoxBREIiKNjJnx0vBe9GzThDmrtnLzpEUADGw1kMHtB7Mp4Xu+7jiX4u11GAIohBREIiKNkMfj4dVf9qF7q0w+WLGFUS9/AcCYXmPokNmBOVlTKGu2k28W/cTqz4IbAihkfY3q2kVEJGxiYjy8PqIvXVqkM33ZJu58bQkJMQk8MuARUuJSeK3dn/HEU6chgEJBQSQi0ojFxHiYMrKAk3LT+NfiDYx5fRkdm3RkRLcR7PEUM6/LK+wvrajTEEB1pSASEWnk4mM9/GtkPzrmpPLa5+v47dQvua7zdfTM7cnyhE8o6vA92zaU8NHrh47mHQkKIhGRY0BSfCxTby2gbXYKkxb8wENvr+TRAY+SlZjFP5s9RWxmJctmrwt+CKA6UBCJiBwjkhNimTaqgNZZyTw/fy3j527hgb4PgDmm5j+JeYz3x0d+CCAFkYjIMSQ1MY5ptxWQl5nEuDnfsXR1HoPaD2Ida1nX7TP27Y78EEAKIhGRY0xGUjzTbisgNz2RJ2auIXvvUFqltWJ6/EvEtSmN+BBACiIRkWNQVkoC02/rT7O0BB6f+S39U39FXEwcL7Z8mISUmIgOAaQgEhE5RmWnJfDWqAKyUuIZP7uYnmnXUVSxg0U9puIqIzcEkIJIROQYlpuRxFujCshMiuODT9rTIr4b88tmENN9J8XbS5n1wsqw90FBJCJyjMtrksybtxaQlhjHmuWXkmDpTEh8mLTj4lizcAtrFoZ3CCAFkZ/0e0Qi0pgd3zSFKbf0JTU+mZ1rr2VfZSnvdn6WuAQPs/+xipKi8A0BpCDyk36PSEQau/Y5aUwe0ZfEyjaUbT2HL0uWsu+MtewvrWDaU0vDNgSQgkhERA44MTedV2/qQ+yus6nY25qJxU/RrGsC29bv5usFm8KyTgWRiIgcpEteBi8N70fllp9TXmmMy/wDaU0T2LB6Z1jWpyASEZFDnNK6CZOuvZDyrRdRuP9HPu/9b86+Lj8s61IQiYhIrXq2zeL5IaOo2J3Pv7e9wey1S8OyHgWRiIgcVkGHZjzc/wGoTOI3cx8Kyzpiw7JUERFpNAZ368S6Pb/hzHbdw7J8BZGIiBzVqL6DwrZsHZoTEZGoUhCJiEhUKYhERCSqFEQiIhJVCiIREYkqBZGIiESVgkhERKJKQSQiIlGlIBIRkaiycP3QUWNlZluBH6pNygCKAnidDRSGoWs11xPKdkeb53Dv1za9vtSrtnWFqo3qFXibI82negU2X13qVXNaKOt1vHOuWa3vOOf0qMMDeC7A159Hoh+hbHe0eQ73fm3T60u9gq2Z6hWeNkeaT/WKXL1qTotUvXRoru6mBfg6Uv0IZbujzXO492ubXl/qFey6VK/wtDnSfKpXYPPVpV41p0WkXjo0F2Fm9rlzrke0+9FQqF6BUb0Co3oFJlz10h5R5D0X7Q40MKpXYFSvwKhegQlLvbRHJCIiUaU9IhERiSoFUT1mZk3MbLqZrTazpWb2gZl1iHa/6jMz+62vXpVmdnG0+1OfmFl7M5vvq89iM9O5kaPQ58l/dfm+UhDVbw543Dl3gnOuGzAdmBDlPtV3M4DzgQ+j3ZF66Bng7865E4D/AV4yM4tyn+o7fZ78F/T3lYIoAGbW0syeMrMFZrbHzJyZtTnMvK3M7HUzKzKzXWY2xcxaB7I+59xO59zMapM+BmpdX30U6XoBOOc+cc59V9e+1wehrJ+ZNQP6AC8AOOdmAAacFvYNiaBQf+Ya0+epNqGsV12+rxREgekAXAHsAOYdbiYzSwb+DZwEXAdcA3QEZptZSh3WPxqYWof2kRbtejV0oaxfa2CTc25/tabf+6Y3JvrMBSac9fL/+ypcdxU3xgfgqfb3cLy7om1qme8OoALoUG1aW6AcuKvatJl4h8uo7VFQY5n34/0XRnK069BA6jUHuDjaNagv9cO757O6RrsPgCHR3s76WrPG9nmKcL0C+r7SHlEAnHOVfs7638AnzrlvqrVdC3wEDK427RznXPZhHh9VzWdm9wEXAD9zzu0JzdaEX7Tq1ViEuH4/AseZWVy1dm180xuNUH/mGrtw1CuY7ysFUXh0Br6sZfpXQKdAFmRm9wODgP9yzgUzqGlDELJ6HaOOWj/n3FbgM+B6ADM7F+85okWR6WK9o89cYPyqV7DfVwqi8MjCe8y1pu1AE38XYmadgQeApsBcM1tiZp+HpIf1S0jqBWBmD5jZeqAvMMHM1ptZyxD0sT7zt34jgBvMbDXwKDDU+Y6jHIP8qtkx+nmqzVHrVZfvq9gQdVLCwDn3Fd5/tYqfnHMP4P2fQWpwzq0B+kW7Hw2JPk/+q8v3lfaIwmMHtf9L/nD/qjjWqV51o/oFTjULTFjrpSAKj6/wHlOtqROwIsJ9aQhUr7pR/QKnmgUmrPVSEIXHW0AfM2tXNcF3k1iB7z05mOpVN6pf4FSzwIS1Xhp9O0Bmdpnvz7PxnvwdCWwFtjrn5vrmSQGWAnuB+/Bem/97IA3o6pzbHel+R4vqVTeqX+BUs8DUi3pF+4aqhvbw/Qeo7TGnxnytgTeAXUAx8Ca13CjW2B+ql+qnmtXvR32ol/aIREQkqnSOSEREokpBJCIiUaUgEhGRqFIQiYhIVCmIREQkqhREIiISVQoiERGJKgWRiIhElYJIRESiSkEkEmVm9oSZOTM7I9p9EYkGDfEjEmVm9iOQCOQ65yqj3R+RSNMekUgUmVlPoBUwVSEkxyoFkUh0DfE9/yuqvRCJIgWRiB9853Cc7+8rzWyBme02s2Izm2Vm/YNc9CV4h9Wf6Wc/mvv6srqW90ZU9dPMOtR472Tf9E+D7KdI2CiIRAJgZg8CLwNlwNvAeuAsYJaZ9Q1wWZ2AE4F3nHNlfjbb4XtOq7GsGODuapOyarS70/f8SCB9FIkEBZFIYG4FejnnznDOXQl0BsYD8cCDAS6r6rDcFH8b+AJrDzWCCLgUaA987Ht9IIjMLAe4GvgGHQKUekhBJBKY+51zi6pe+C4w+K3v5elmFhfAsoYA+4B3A+zDdiDFzKr//zsG+Al40ve6+h7RSCAB+IsuiJD6SEEkEpjpNSc457bgPWSWADT1ZyFm1gboDsxwzu0OsA8HHZ4zs3OBU/GG0Gbfe1m+9xKAW4CtwAsBrkckIhREIoH58TDTd/meE/1cTsCH5aqpeZ5oDLAbGAsU+aY18T0PBXKAvzrn9gaxLpGwUxCJBCCEh7aGAOXAtCDabvc9p5lZD+BsYLxzbgew0/de1aG50XjPKT1dcyFmtsnMfmdm95nZOt8VgOPNLMbM+pnZXDMrMbOlZta1RttLzWymmW0xs71m9qWZXV7t/dN8V+ldV21ahpktN7N5ZuZvYMsxQEEkEmFm1hzoC3zonNsWxCKq9ojS8e4N7Qf+1zetao8oy8zOAU4GJtZcj+8ChlzgRqCF7/kpYLjveRzwDHAV3r2rv9ToQ1fgdWAYMAiYA7xsZicC+M6jvQnc6wu2ON/8ccBg59y+ILZbGqnYaHdA5Bh0Md5/BAZzWA7+E0Sn4t2zesk5t843bRfg8O4R3QlUcGiIAHTzPU9yzt3n+/sDMxuFN1i6+vaw8O113Vi9sXPu/qq/fZeOz/HN0wtY5XvrfmAJ3jA7C2949XHObUekGgWRSOQNwRsWbwbZvuqL/DeAUe3eIOdchZntxhtSxwGTnXNra1lGV6AU+FPVBDOLxXvBxdiqEPJJ4z+H/KougLgZuAFoh3fPrMqean1ZZmaTgWd9kwYepi9yjFMQiUSQmWUCZwKfOec2BLmYqpBoifdm2C9rvF/kew8OfwNrN2Chc6642rROeO+HmlVj3q7AlwBmZsBUvEH3OPAFsA0Y6FvX1zXafgMkA4845xYebcPk2KRzRCKRNQjveZK63FhafW+ltqCpOk80u/o9TzV0w3vYrOa0CmB5LdOX+v7uC5wHDHXO/cE5954vYNrjvSfqQBCZ2VV4z2EtBK4xs6QjbpUcsxREIn5wzplzzo7wfhvfPN8fZVF1uWy7al2vVPXHOTe3lve7+N47q7b2vgsHTuLQIDoFWFX9Mm8zy8N7b1TVvK18z9UDpxNwPfClc67CN+0MvPctjQEu8y1jZICbKscIBZFIZC0A7nHOrYliH/LxHoKrLYgW15hWdVFD1R7RF/gugDCzs30XN0zFuze0BMDM8vGe/5rgnPuzc+5H4G/AGDNLCfXGSMOnIBKJIOfcI865x6LcjW5472GqeW7pcIfrCqvOZ/kC9Jd4D9FNBS7EO85dIrDEzHLxDlk0H7i92nL+AGQAd4R0S6RR0C+0iohIVGmPSEREokpBJCIiUaUgEhGRqFIQiYhIVCmIREQkqhREIiISVQoiERGJKgWRiIhElYJIRESi6v8A36tNeS2HaaAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

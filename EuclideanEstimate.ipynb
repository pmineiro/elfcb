{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{\\left(\\frac{1}{N} \\sum_n w_n\\right) - 1}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (\\lambda + 1)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (\\lambda + 1)} \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "Q^*_{w_n, r_n} &= \\frac{1}{N} \\left(\\frac{2 N}{\\left(\\gamma + \\beta w\\right) \\left(1 + \\lambda\\right)}\\right)^{\\frac{1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(\\beta, \\gamma, Q) \\\\\n",
    "&= -\\beta - \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2^{\\frac{1}{1 + \\lambda}} \\left(1 + \\lambda\\right)^{\\frac{\\lambda}{1 + \\lambda}}}{\\lambda N^\\frac{\\lambda}{1 + \\lambda}} \\sum_n \\left(\\gamma + \\beta w \\right)^{\\frac{\\lambda}{1 + \\lambda}} & (\\lambda > -1, \\lambda \\neq 0)\n",
    "\\end{aligned}\n",
    "$$ Just hit it with a generic convex solver $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "code_folding": [
     0,
     43,
     50,
     56,
     134,
     161
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** MLE ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(1, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "        factor = (barw - 1) / denom\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "        \n",
    "#         estimate = sum(c*w*r*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n\n",
    "#         missing = max(0, 1 - sum(c*w*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n)\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "\n",
    "class CressieRead:\n",
    "    @staticmethod\n",
    "    def dualobjective(gamma, beta, data, n, facscalefaclampow, scalefac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        dual = -scalefac * gamma - scalefac * beta - (2 * n) / (lam * (1 + lam))\n",
    "        dual += facscalefaclampow * sum(c * (gamma + beta * w)**lampow for c, w, _ in data)\n",
    "        return -dual\n",
    "    \n",
    "    @staticmethod\n",
    "    def jacdualobjective(gamma, beta, data, n, facscalefaclampow, scalefac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        j = [-scalefac, -scalefac]\n",
    "        for c, w, _ in data:\n",
    "            dx = facscalefaclampow * c * lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            j[0] += dx\n",
    "            j[1] += dx * w\n",
    "            \n",
    "        return -j[0], -j[1]\n",
    "        \n",
    "    @staticmethod\n",
    "    def hessdualobjective(gamma, beta, data, n, facscalefaclampow, scalefac, lam):\n",
    "        lampow = lam / (1 + lam)\n",
    "        h = [ 0, 0, 0 ]\n",
    "        for c, w, _ in data:\n",
    "            dx = facscalefaclampow * c *  lampow * (gamma + beta * w)**(lampow - 1)\n",
    "            d2x = facscalefaclampow * c * lampow * (lampow - 1) * (gamma + beta * w)**(lampow - 2)\n",
    "            h[0] += d2x \n",
    "            h[1] += d2x * w\n",
    "            h[2] += d2x * w * w\n",
    "            \n",
    "        return [ [ -h[0], -h[1] ], [ -h[1], -h[2] ] ]\n",
    "        \n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, lam, **kwargs):\n",
    "        from cvxopt import matrix, solvers\n",
    "        \n",
    "        rmin = kwargs.pop('rmin', 0)\n",
    "        rmax = kwargs.pop('rmax', 1)\n",
    "        \n",
    "        n = sum(c for c, _, _ in data)\n",
    "        #fac = 2**(1/(1+lam)) * ((1+lam)/n)**(lam/(1+lam)) / lam\n",
    "        facscalefaclampow = 2 / lam\n",
    "        \n",
    "        scalefac = (2.0 * n) / (1.0 + lam)\n",
    "        \n",
    "        # FOOstarprime = FOOstar / scalefac\n",
    "        \n",
    "        x0 = 1.0, 0.0\n",
    "        \n",
    "        G = matrix([ [ -1.0, -float(w)  ] for w in (wmin, wmax) ])\n",
    "        h = matrix([ 0.0 for w in (wmin, wmax) ])\n",
    "\n",
    "        if False:\n",
    "#             import MLE.MLE\n",
    "#             from numpy import array as arr\n",
    "#             MLE.MLE.gradcheck(f = lambda x: CressieRead.dualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam),\n",
    "#                               jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='dualobjective',\n",
    "#                               eps = 1e-6)\n",
    "#             MLE.MLE.hesscheck(jac = lambda x: arr(CressieRead.jacdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)),\n",
    "#                               hess = lambda x: arr(CressieRead.hessdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)),\n",
    "#                               x = x0,\n",
    "#                               what='jacdualobjective')\n",
    "            pass\n",
    "        \n",
    "        def F(x=None, z=None):\n",
    "            if x is None: return 0, matrix(x0)\n",
    "            if any(x[0] + x[1] * w <= 0 for _, w, _ in data):\n",
    "                return None\n",
    "            f = CressieRead.dualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)\n",
    "            jf = CressieRead.jacdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)\n",
    "            Df = matrix(jf).T\n",
    "            if z is None: return f, Df\n",
    "            hf = CressieRead.hessdualobjective(x[0], x[1], data, n, facscalefaclampow, scalefac, lam)\n",
    "            H = z[0] * matrix(hf)\n",
    "            return f, Df, H\n",
    "        \n",
    "        soln = solvers.cp(F, G, h, options={'show_progress': False})\n",
    "        if False:\n",
    "            if soln['status'] != 'optimal':\n",
    "                import sys\n",
    "                print('.', file=sys.stderr, end='')\n",
    "        fstar, (gammastar, betastar) = -soln['primal objective'], soln['x']\n",
    "                \n",
    "        estimate = sum((c/n) * w * r * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, r in data)\n",
    "        missing = max(0, 1 - sum((c/n) * w * 1 * (gammastar + betastar * w)**(-1 / (1 + lam)) for c, w, _ in data))\n",
    "        \n",
    "        return max(0, min(1, estimate + 0.5 * missing))\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('TwoThirds', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=2/3)),\n",
    "                        ('MinusOneHalf', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=-1/2)),\n",
    "                        ('One', lambda *args, **kwargs: CressieRead.estimate(*args, **kwargs, lam=1)),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=10000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEdCAYAAABOl2PPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd3gU1f7H8ffZTe+dBAJJ6L0GhAtXEBBBaYJiA8WCDVRQkCLFdrGiWEF/IiiK7eq1YEFFARWkhw6hhZBKQnpPds/vj8QIGCAJm8wm+b6eZx+ys3NmvnuM+ezMzpyjtNYIIYQQRjEZXYAQQoiGTYJICCGEoSSIhBBCGEqCSAghhKEkiIQQQhhKgkgIIYShHIwuoK4JCAjQ4eHhRpchhBB1yvbt21O11oEVvSZBVEXh4eFs27bN6DKEEKJOUUqdON9rcmpOCCGEoSSIhBBCGEqCSAghhKEkiIQQQhhKLlYQQtic1WolNTWVjIwMLBaL0eWIWuLi4kJoaCiOjo5VaidBJISwubi4OJRShIeH4+joiFLK6JJEDdNac/r0aeLi4oiIiKhSWzk1J4SwudzcXJo0aYKTk5OEUAOhlMLf35+CgoIqt5UgqkXaajW6BCFqjckkf14amup+6JBTc7Xk4O8/s+XN3fh1TmP4rKeNLkcIIeyGfGSpJKXUCKXU25mZmdVqH5uYSp5bK5IOduG7hbNtXJ0Qwkjr1q0jNDS0/HmHDh1Yt25dpdYVEkSVprX+Rmt9t7e3d7XaD7n+RhzbHaTYwZWE6G78+LSEkRBGCQ8Px9XVFQ8Pj/LHlClTbLb9ffv2MWDAAJttr76TIKpFdz4yA1Ob/RQ7unHycFd+fmKm0SUJ0WB988035OTklD9ef/11o0tqsCSIatk9Mx7F2mYfRU4enDjWjV8fn250SUKIMo8//jjjx48vfx4TE4NSipKSEgDS0tK4/fbbady4Mb6+vowePbrC7YSHh/Pzzz8DkJ+fz8SJE/H19aV9+/Zs3br1rHUTEhIYO3YsgYGBRERE8Oqrr5a/tmXLFvr06YOPjw8hISFMmTKFoqKi8teVUixdupRWrVrh4+PD5MmT0VrbrD9qiwSRASZPn0lJ670UOnlx/FgP1s99xOiShBCVMGHCBPLy8ti3bx+nTp1i2rRpF23zxBNPcPToUY4ePcqaNWt47733yl+zWq2MGDGCLl26EB8fz9q1a1m8eDFr1qwBwGw28/LLL5OamsqmTZtYu3Ytb7755lnbX716NVu3bmX37t18+umn5W3rErlqziAPzJjJyy8+D9GdOBrbA9Ocqfx74WKjyxKiRjzxzT72J2TV6D7aN/ZiwYgOlV5/9OjRODj8/SfwhRdeuOD6iYmJfP/995w+fRpfX18A+vfvf9H9fPrpp7z55pv4+fnh5+fHgw8+yJNPPgnA1q1bSUlJYf78+QA0b96cSZMm8fHHH3PVVVfRo0eP8u2Eh4dzzz33sH79eqZOnVq+fNasWfj4+ODj48MVV1xBVFQUQ4cOrXQ/2AM5IjLQtOmPkt9qDwUuvhw52ZM/Zj5gdElCNBhffvklGRkZ5Y9JkyZdcP2TJ0/i5+dXHkKVlZCQQNOmTcufh4WFlf984sQJEhISyoPEx8eHhQsXkpycDEB0dDTDhw8nODgYLy8v5syZQ2pq6lnbDw4OLv/Zzc2NnJycKtVnD+SIyGCPzHiU5154Hg53IjrhMszT76f3i29evKEQdUhVjlSM5O7uTl5eXvnzpKSk8p+bNm1KWloaGRkZ+Pj4VHqbISEhnDx5kg4dSvsgNjb2rG1GRERw+PDhCtved999dOvWjY8++ghPT08WL17Mf//736q+LbsnR0R2YOaMR8lstYd81wAOJvVhy7R7jC5JiAapa9eubNiwgdjYWDIzM3nmmWfKXwsJCWHYsGHcf//9pKenU1xczIYNGy66zXHjxvHMM8+Qnp5OXFwcr732WvlrvXr1wtPTk+eee478/HwsFgt79+4tv6AhOzsbLy8vPDw8OHjwIEuWLLH9m7YDEkR2YvaMR0lruYtct0D2p/Rl64MXPk0ghLg0I0aMOOs+omuvvZYrr7ySG264gc6dO9OjRw+GDx9+VpuVK1fi6OhI27ZtCQoKYvHii3+vu2DBAsLCwoiIiGDIkCFMmDCh/DWz2czq1auJiooiIiKCgIAA7rrrLv66cf7FF19k1apVeHp6MmnSJG644QbbdoKdUHXxUj8jRUZG6m3bttXY9p987jn8j3XFIyeJTj7r6P7G8hrblxA15cCBA7Rr187oMoQBzvffXim1XWsdWVEbOSKyM/NnziSleRQ5HsHszejPjntvNbokIYSoURJEdujxmTNJithJtkcIe7OuIOquW4wuSQghaowEkZ16atYsEprvJNuzCXvyBrHr9pvq5B3TQghxMRJEduw/M2cRF76DLM+m7C4YxN5bx0kYCSHqHQmiSrrUaSCq65nZs4gN306WVxi7Soaw9+axWC2WWq1BCCFqkgRRJV3qNBCX4rnZs4gJ206mVzi79FD23zQWa9kgjEIIUddJENURL8yZxbGwbWR4R7DLNIyDN4zGWlxsdFlCCHHJJIjqkEVzZnOs2VbSvVuw0+EaDl4/CusZQ8ILIURdJEFUx7z02ByONN1Cuk8ropyHc2jsCKwFBUaXJYSopIKCApRSxMXFVfj6smXLGDFiRJW22bt3bz744ANblGcICaI6aPHcORwJ3UKaT2t2uo7k0NgRWPLzjS5LiDrhzGF9TCbTWVOGf/jhh5e8/QULFpRvz8XFBQcHh/LnZ07rcD533nkn33zzzSXXUZdIENVRi+fN4XDoFtJ82xDlPoroMddgqYPDvwtR286cHrxZs2ZnTRl+yy2XfvP4E088Ub69xYsXM2DAgPLn27dvv6Rtl9TTi5QkiOqwV+bNITp0K2l+7dnlMZrD1w7HUsuXlwtRn+Tk5ODi4kJWVukkfvPmzcPZ2Zn8sjMOM2bMYNasWUDptOE333xz+RTfzz//fJXu8/vuu+9o0aIFvr6+Z830unTpUgYPHgz8fRpvyZIltGjRgo4dOwLw7bfflk8P/vDDD5+13YMHD9KvXz+8vb0JDAzk1lvtf5gwCaI67tV5sznU+E9O+3ckyvtaDo8ZQfGpU0aXJUSd5OHhQefOnfntt98AWL9+PaGhofz555/lz/+alfXee++luLiY48eP89NPP7FkyRJWrVpV6X398MMP7Ny5kx07drB8+XLWrVt33nVXr17N9u3b2blzJ4mJiYwbN45FixaRkpJCYGAgZw7EPHv2bEaPHk1GRgaxsbHcc4/9TysjE+PVA6/On8MDTy6kLb3ZhcY6dhQtPvgY5zNmghTCUN/PgqQ9NbuP4E4w7NlL3kz//v1Zv349gwcP5vDhwzzyyCOsX7+eyMhIdu/eTd++fSksLOTzzz/n8OHDeHh40LJlS6ZOncrKlSsrfXpvzpw5eHl54eXlxeWXX05UVBQDBgyocN3HHnusfDK+lStX0rNnT0aOHAnAo48+yssvv1y+rqOjIzExMSQlJRESEkLfvn0vrUNqgRwR1ROvzZ/DgZDNpAZ0ZmfjCRy+aRz5+w8YXZYQdU7//v1Zt24dmzdvJjIykoEDB7J+/Xr++OMPOnXqhJeXF0lJSVitVpo1a1beLiwsjPj4+ErvpypTfJ851fi5U4+bzWaaNGlS/vzll18mLy+Pbt260blz5zpxNZ0cEdUjry+Yzf1PP0N7ItlhvhvL7RNp+frruPfsaXRpoqGzwZFKbenXrx+7du3i22+/pX///nTt2pWDBw/y448/lp+WCw4OxmQyERsbS/PmzYHSKcDPDARbUkqV/xwSEnLWaTyr1XpWADZp0oR3330XrTXr169nyJAhXH755WeFpr2RI6J65s25szkQHkWmVyg7Wk8hevIDZK1da3RZQtQZPj4+dOjQgSVLltC/f39MJhORkZG888475UHk7OzMtddey5w5c8jNzeXo0aO88sorjB8/vsbrGzlyJFu3bmX16tUUFxfzwgsvkJaWVv76J598QkJCAkqp8tN5ZrO5xuu6FBJE9dAbs2YQ3eYgWR4BbG8/lejZC8j44gujyxKizujfvz9aa7p3717+PDc3l379+pWv89ZbbwGlp+QGDhzIXXfdZZPLvy8mJCSEjz/+mKlTpxIYGEhycjKRkX9PfLpp0yZ69OiBh4cH119/PW+//XaNHanZikwVXkU1PVW4LU1/cwmhe5viVlBItz2v0fzeCfjfeafRZYkGQKYKb7hkqnBxlhfvv4/UXqlku5rZ3mUaR976hFOLXjS6LCGEOIsEUT339MSJFPcvJsu1mB1dHuLIp+tInDvb6LKEEKKcBFEDMGvc9bgPdSPDLZuoLlM48ks0cVPuRVutRpcmhBASRA3F5GuuptmYENLcUtjd6R6O7sri5MRbZE4jIYThGmQQKaV8lVKrlVLRSqldSqkflVItja6rpt3Svz9dJ7TllHsce9vfweFYV2JvHI1FppEQQhioQQYRoIHFWuvWWusuwGrgHYNrqhXDe/Tkykm9SfA8xsG24zmUGUbstcMoyZDBUoUQxrCLIFJKhSqlXlNKbVJK5SmltFIq/DzrNlVK/VcplamUylJKfaGUqtItw1rrDK31z2cs2ghUuL/6qF+79tz44BBOeB/gSMvr2K+7E3PtUIqTk40uTQjRANlFEAEtgXFAOvDb+VZSSrkBvwBtgduACUAr4FellPsl7H8q8NUltK9zOjUL575HxnLUdy8x4dewz30Qx8dcQ2FMjNGlCSEaGHsJog1a60Za66uBzy6w3iSgOTBaa/2l1vorYCQQBpSPda6U+lkplXqex1lD0SqlFpRts8Fd0xwRFMzsWbdx0D+KuNCB7Am8luPjRpO/b7/RpQlhuHvvvZennnrK6DIMN2DAAN555+9vLubOnUtAQMBZg7ZeKrsIIq11Za8jHgn8qbU+ckbb48AfwKgzlg3WWgec5/HHX+sppeYCVwPDtNZ5tnk3dUugtzf/eewe9gRtJym4D7uajSfmtpvJLZt/RYj6KDw8HCcnJ1JTU89a3q1bN5RSxMTEsHTpUubNm1fjtWzcuJGBAwfi6emJt7c3I0aMYP9+23wYXLduHaGhof9Yfm64VFZsbCyLFi1i//79JCUl2aJEwE6CqAo6AHsrWL4PaF+VDZUdCY0AhmitG/Q39V5u7rw+fxpRwdtIDejOjhZ3EXP/vWT9+JPRpQlRYyIiIvjoo4/Kn+/Zs4e8vNr9PLpp0yaGDBnCqFGjSEhI4Pjx43Tp0oW+ffty7NixWq2lMmJjY/H39ycoKMim261rQeRH6fdI50oDfCu7EaVUB+BxwB9Yr5SKUkqddwA5pdTdSqltSqltKSkpVSy5bnBwcODtBTPYEbqddN+2bG83mROPziT900+NLk2IGjFhwgTef//98ufvvffeWdNqT5w4kblz5wJ/H1ksWrSIoKAgQkJCWL58efm65x5hrFixonyAVK0106ZNIygoCC8vLzp16sTevaWfpx999FFuvfVWHnroITw9PfHz8+Ppp5+md+/ePP7445Xad2FhIdOnT6dZs2Y0atSIe++9t3xq88pIT09n+PDhBAYG4uvry/Dhw4mLi/vHej///DNXXnklCQkJeHh4MHHixErv42LqWhDZhNZ6n9Zaaa1baq27lj0qHIyvbP23tdaRWuvIwMDA2iy1VimlWDZ3BlHhu8j0DGNr54eIfeYFUt9+y+jShLC53r17k5WVxYEDB7BYLHz88ccXnMYhKSmJzMxM4uPjWbZsGZMnTyY9vaLPxWf78ccf2bBhA9HR0WRmZvLpp5/i7+9PXl4eGzdu5Prrr/9Hm3HjxvHTT3+fkbjQvmfNmkV0dDRRUVEcOXKE+Ph4nnzyyUr3g9Vq5fbbb+fEiRPExsbi6urKlClT/rHe4MGD+f7772ncuDE5OTmsWLGi0vu4mLo2MV46FR/5nO9ISVTDO7Me4e6XXqHD4TZs6fowkUtexZJ2mkaz5hhdmqijntvyHAfTDtboPtr6tWVmr5lVavPXUVH//v1p167dBadLcHR0ZP78+Tg4OHD11Vfj4eHBoUOH6N279wX34ejoSHZ2NgcPHqRXr17lI1PHxcVhtVoJCQn5R5uQkJCzvr86374vu+wy3n77bXbv3o2fnx9QOgX5zTffzDPPPAOUzuj617xEf8nJySkPXX9/f8aOHVv+2mOPPcYVV1xxwfdka3XtiGgfpd8Tnas9IJd62dDbDz/E4Y7HyHH1Ykv3h4n79AcSHn0YmTZE1CcTJkxg1apVrFix4qzTchXx9/fHweHvz+4Xm977LwMHDmTKlClMnjyZoKAg7r77brKysvD19cVkMpGYmPiPNomJiQQEBFx03ykpKeTl5dGjRw98fHzw8fFh6NChnPkVQuPGjcnIyDjrcea8Snl5edxzzz2EhYXh5eXF5ZdfTkZGBhaL5aLvzVbq2hHR18CLSqnmWutjAGU3vvYFZtXkjpVSI4ARLVvW+5GAyr0++X6mL1tGyI5AtnR/mB5rX6fkjgmEvvUuJicno8sTdUhVj1RqS1hYGBEREXz33XcsW7as2ttxd3c/60KHc68oe/DBB3nwwQc5deoU48aN44UXXuCpp56iT58+fPbZZ/84Avn0008ZNGjQRfcbEBCAq6sr+/btq/bkd4sWLeLQoUNs3ryZ4OBgoqKi6NatW61+6LSbIyKl1HVKqeuAHmWLhpUt63/Gav8HxABfKaVGKaVGUnoj6kmgRr/I0Fp/o7W+29vbuyZ3Y3devPNOsvpmk+1qYWuPqcTvzyD2+pFYsrONLk0Im1i2bBm//PIL7u7Vvye+a9eufPHFF+Tl5XHkyJGzQm3r1q1s3ryZ4uJi3N3dcXFxwWQq/dP77LPP8t577/Hqq6+SnZ1Neno6c+fOZdOmTSxYsOCi+zWZTEyaNIlp06Zx6tQpAOLj41mzZk2la8/OzsbV1RUfHx/S0tJ44oknqvjuL53dBBGlN7J+Btxb9vzNsuflvaK1zgUGAtHASuBD4DgwUGt98WNkUS1P3HwL5sGKDLcctnedQmyqFzGjh1Jkw/sIhDBKixYtzppquzqmTZuGk5MTjRo14rbbbjtryvCsrCwmTZqEr68vYWFh+Pv7M2PGDAD69evHmjVr+OKLLwgJCSEsLIydO3fy+++/06pVq0rt+7nnnqNly5b07t0bLy8vBg8ezKFDhypd+9SpU8nPzycgIIDevXszdOjQqr15G5CpwquoLk0VbmvvrfuF41/HEZAXQruDq2hWuIumy1fh0rat0aUJOyNThTdcMlV4DVJKjVBKvZ2Z2XDvfb1twED63NqRBM8YDradwBHPvpy45Xpy/vjj4o2FEOI8JIgqqaF+R3Suq7p25/opgzjmc4Bjza/lYOMRxN1/Nxn/+9Lo0oQQdVRdu2pO2IFuYc3xn349z7+8ChhMobMH1nmPUZyUSOB99xldnhCijpEjIlEtzQKCeHrOnUQF7SAloDfbO99N8utLSJw3T+41EkJUiQSRqDYfd0+WzJ/KtsbbyfDqwJbIB0j533fE3Xs3uqTE6PKEEHWEBFElycUKFXNwcGD5/BnsCN9FjlsYmy6bRuqfe4m58TosublGlyeEqAMkiCpJLla4sOWzHmFfm2jynP3YeNnDnD6aTsy1wykuu8lOCCHOR4JI2MxbU6cQ1z2RPCcnNvV6mNR0J2KuHUHBkSMXbyyEaLAkiIRNLbrrTvL/nU+OSxFbekwlWYcQe+N15G7ebHRpQgg7JUEkbG7+DTfic7UH6a5p7Ox8P/GeHYi7+04yv/3W6NKEKLdixQo6deqEm5sbwcHB3HfffWRkZBhdVoMkQSRqxH1XDaPDDc1J9ohnX5vbOR7yLxJnzuD0sneNLk0IFi1axMyZM3nhhRfIzMzkzz//5MSJE1x55ZUUFRUZXV6DI0FUSXLVXNWN7d2HYZN6E+t1hKPhN3KoxVCSF71A0n/+Y3RpogHLyspiwYIFvPbaawwdOhRHR0fCw8P59NNPiYmJ4YMPPuDxxx9n3Lhx3HrrrXh6etKhQwfOHGMyISGBsWPHEhgYSEREBK+++qqB76jukyCqJLlqrnr6tmnHndNGEu27j7iQ4ezpdANpKz8kbsoD6FqceEuIv2zcuJGCggLGjBlz1nIPDw+uvvrq8im6v/76a2688UYyMjIYOXJk+fTZVquVESNG0KVLF+Lj41m7di2LFy+u0tQL4mwyxI+ocW1CGjNv9q089vwyOnM523p60GPte5SMH0/T5csxu7gYXaKoYUkLF1J4oGanCndu15bgORefzj41NZWAgICzZjz9S0hICNu3b6dNmzb069ePq6++GiidyXXx4sVA6fxCKSkpzJ8/H4DmzZszadIkPv74Y6666iobvqOGQ46IRK0I8PLmlfmT2R68g2z37mzscx/Zuw8Sc+1oStLSjC5PNCABAQGkpqZSUsHoH2dO0R0cHFy+3M3NjYKCAkpKSjhx4gQJCQnlU3P7+PiwcOFCkpOTa+091DdyRCRqjYuTM8sWPMLtz7xE95Od+a3vQ/Td9CbHR4+k6TvLcWlduYnARN1TmSOV2tKnTx+cnZ354osvGDduXPnynJwcvv/+exYuXEhcXNx52zdt2pSIiAgOHz5cG+U2CHJEJGqVUooVcx5hd8v9FDkEs77vw2Rna07cNI6ML74wujzRAHh7e7NgwQIeeOABfvjhB4qLi4mJiWHcuHGEhoYyYcKEC7bv1asXnp6ePPfcc+Tn52OxWNi7dy9bt26tpXdQ/0gQCUO888hDHO0QQ7HZg996TyfTyZfEuXOJnzkLLZfPihr26KOPsnDhQqZPn46XlxeXXXYZTZs2Ze3atTg7O1+wrdlsZvXq1URFRREREUFAQAB33XUXckVt9clU4ZWklBoBjGjZsuUkOSS3ncdWrsRrizsuJa50iV1Go6P7cG7VktAlS3AKDTW6PFFNMlV4wyVThdcguXy7ZvxnwgRchjqR5prCvqb3s6/XEAoOH+X4mDFkrfnR6PKEELVAgkgY7qFrhjPwru5E++4n2W0UGwZOpLigkPhHppH45FMyt5EQ9ZwEkbAL/dt3ZPbsW9gWvJ1ia3d+7P8IRf6eZKxaRcxNN1OckmJ0iUKIGiJBJOxGsJcv7y6Yzo6We8Dqxy8d55DeIYyCPXs4PmoU2b/9bnSJQogaIEEk7IpSiuXTp3Gq92lyHPPYHvgIhy/vjSUzjfj77+PUSy8hF9gIUb9IEAm79OyE22hxfQixXsc5aZrAb1eOw+RSwum3/48Tt95GiQzXL0S9IUEk7NbNfftzx7RhRAXupLhgAN/0m4o53IH8rVs5PnIUedu3G12iEMIGJIiEXWsX0pTX5k3hz6ZbMReG8XWrBRRf5ktJyili77iT1LfellN1QtRxEkSVJPMRGcfFyZmVj83kUKcjWJSJX93nkzCkDWZTPimLF3Pynnux5OQYXaYQopokiCpJbmg13pL7pmAeYiLFNYmDhQ+yafBQ3IILyd2wgeOjRlOwf7/RJYo6Ijw8HCcnJ1JTU89a3q1bN5RSxMTEMHHiRObOnVthe6UU7u7ueHh4lD+ef/752ii9XrrkIFJKtVRK9VFKtbZFQUJcyPThoxk0qQcH/PaQnT2KL3pMwbNzEcXx8cSMn0Dahx8aXaKoIyIiIvjoo4/Kn+/Zs4e8vLxKt9+1axc5OTnlj0cffbQmymwQqhVESikHpdR8pVQycAj4HZh1xuu3KKU2KqU62qhOIcpd0bYT82ffxuaQLTjkdOSjkKdwHuSAKskl+T8LiZs6FWthodFlCjs3YcIE3n///fLn7733HrfeequBFTVcVQ4ipZQD8B2wAPABDgDqnNX+AHoDYy+1QCEqEuTpw3vzZ7KtdRTK4sFXPEPG0Ma4+BWR/cMajo++lsJjx4wuU9ix3r17k5WVxYEDB7BYLHz88ceMHz/e6LIapOpMjDcFGAz8DNymtU5USlnPXEFrHaOUOgIMAZ649DKF+CelFO89PIOZH67AfZsjW7LmENJvFf0O/Uz6gePEjLuBRnPn4jN6lNGlNni/fRpN6smavaAkoKkH/x5XtW8I/joq6t+/P+3ataNJkyaVbtu9e3dMpr8/y3/yyScyVXg1VSeIJgCngXFa6wvdVXgA6FatqoSogudumciq5uvY/uUBTBnj+bxVc24JXErSRkXiY4+R9+efBM+fh8nNzehShZ2ZMGECl19+OcePH6/yabkdO3bQsmXLGqqsYalOELUB1l0khACygcBqbF+IKru5zwC6hTXnxbc+onvyv/g/9yZMHPokWb85k/nll+Rt20ajeXPx7N/f6FIbpKoeqdSWsLAwIiIi+O6771i2bJnR5TRY1blYQQPWi64FjYGCamxfiGpp17gZbzw2lT/C/sSxMJj3Cl+ieJAnvm0LKI6PI27yFOKmTqNE7gUTZ1i2bBm//PIL7u7u/3jNYrFQUFBQ/iiS2YNrRHWC6DjQRSl13rZKKVegM6Wn54SoNS5OzqyaPYcDXY5QbLKwJv1pdnWIJOKqVBzdLGT/8APHh48gc/Vqo0sVdqJFixZERlY4cSjPPvssrq6u5Y+BAweWv9alS5ez7iOaOnVqbZVc71R5qnCl1NPAbGCW1vqFsmVWYIXW+o6y5wuA+cAcrfVzti3ZGDJVeN3z4ndfkPZrOk2yIyjy3MEk58XkHXLg9D43UCbc//UvQhb+B8dAOYNsazJVeMNVW1OFvwQkAc8qpVYppcaULQ9QSg1TSr1LaQjFAm9WY/t2SUZWqHumXz2GK+/oyfZGmzHndGFpxhscad2B5lcn4+xtJfe33zg+YiRpq1bJeHVCGKjKQaS1TgOGAieAG4HPKP3e6BpgNTARiANGaK2zbVapENVwRbvOvDn3Eba03UGuYzY7Tz/Cm2o2wYMyCIrMxZqTRfJTT3PilvEUnTxpdLlCNEjVGllBa70HaA9MBr6l9LugaGAt8AjQXmu911ZFCnEpXByd+OChmbQc15jtQZtxyO7Cm+lLOBzekRbDE3ENgvwdOzh+7RhSli5Fl5QYXbIQDUqVvyNq6CIjI/W2bduMLkNUU25BPmd8Wg8AACAASURBVHcsfYkOJ8Lxzw+h2HMH97ouoiDegaStXugiK87t2tH4mYW4tG1rdLl1lnxH1HDV1ndEQtRZ7i6ufDL1MULGBrAjaDPm7C68lraE6OBOtBwej3tTKDxwgJgbbyL5+efRcrlutcmH3Ianuv/NqzPWnJNSKkgp5XLOcg+l1NNKqW+UUq8ppZpWqyIhasGkflfx0uwH2dD6Twoc89h6ejpvFM8lqE8GoQOyUBST9u5yjo0cRa7MBFtljo6O5OfnG12GqGXFxcU4OFR9nITqHBHNAxI5Y/iesnuKNlB6Wfc1lH53tEkp5V+N7QtRK7xd3fns4Xn4jPBkZ9CfmLK7lh4dBXSm1fCTeLXQFMXEcPL2O0h4bC7WKkwR0NAFBQURHx9PXl6eHBk1EFarleTkZKpzZXF17iPaCIRqrZudsWwspVfP7QFeoTSMrgXmaa3/U+Wq7Jh8R1Q/peZmMWnpS3Q/2Rq/gmAsnju4x/VFLOmKuE0BWHJKcAgJIXjBfDwHDDC63DohKyuLU6dOUVxcbHQpopa4u7sTGhp61mCwf7nQd0TVCaIEYJ/W+sozlr0P3AJ01VrvKTtCOgkknm/HdZUEUf323JovOPzHQbqc6kWBYw6Dvd+mq+kPkvYEk3HQDGYzHgOvIOTpp3Hw8jK6XCHqDFtfrOAHJJ+z7F/AibLLutFaW4HNQDOEqENmXjWGp6bfxy8t1lNozmdj6nRey5+HX6dMIoadxtFdkfPjTxwbdjUZX39tdLlC1AvVCaJioPwkoFIqCGhO6SytZ8oDPKpfmhDGCPHy5X8znqLkSogK2owpuyuvpC/lkFsPWgw9gX9XC5aMdBJnzebEHXdQFBdvdMlC1GnVCaJooO8ZV82NpXRkhXODKAQ4dQm1CWGox6+5icem3cWPLdZRbC7g99TpvJE/H5/WWbS4JgUnPzN5GzdxfNQokp56ipJsGUhEiOqoThB9RukU4RuUUi8BzwFFwJd/raCUMgPdgSO2KFIIo4T7BvL1jKfJvqKIXUGbIasLi9OXcsi5J80HnaBR72KwFJH+4SqODRvG6RUrZGQGIaqoOhcrOAPfAVeULbIAU7XWb5yxzjBKh/55XGv9pI1qtQtysULDFZ2SwCPLX6d3Qhd8CgLBK4pJrs/jYC0geX9jMg6ZoMSCU0Q4gdNn4DVo4EW3KURDYdOr5so2qIB+QCNgh9b62DmvX0HpfERfa62PV71k+yVBJKb+9x3yd52iU0ovch1z+LfPl/Q2/Q9rkSMJO4PJPWEFpXDt1pXgefNkqCAhqIEgaohkPiJxpj2JJ5i9cgl9ErriUxBEpsspRnh+QifzLxRku5CwNYjCUyUoJyc8Bg+m0ZzZOAYEGF22EIaRILIhOSISZ3ro82Uk7j9Gj5QeeBT5kOUazw2eK2hu2kZOsgeJ2/woyS7B5OmJ7803EXD//ZicnY0uW4haVyNBVDaWXH+gMeByntW01vqpau3ATkkQiXPlFOZz/2dvYTl2mm6neuJa4kGu+3HGu/8fIeoA6cd9SdnlgbXQgkOjRgQ+MAXvsWMpPcMtRMNg65EVHIDXgbuAv/5POvf/KF22TGutzVUr175JEInzOZ2bzf0fv4lLXD5dTvXEyeJCscchbnNbgo+OJWV/IGkHncCicW7dmkaPPYb7Zb2MLluIWmHrIHoamAOUUHr13GEg53zra62fqNIO7JwEkbiYuMx0pqx6g4Ak6HQqEpM2Y/LcwwTXN3EtOk3SziByTpjAbMb9ssto9PjjODeTwepF/WbrIDpB6TA/fbXWu21QX50iQSQq69CpJB7+ZCnNTjnTLjUSjRVPr63c5LwUlV1A4rZAClJAubjgNXw4jWY+itnT0+iyhagRtg6ifGCt1nq4LYqraySIRFVtjzvO3M+W0SrVh9anu1JsKibY83fGOL9D0SlI2uZPSY7G5O2N/5134H/HHahqzOkihD2zdRAdAvZqrcfaori6RoJIVNfa6P288PUHdDjdiObpnSg059HS80eGOa0iO8aFlCgvrEUax9BQgmZMx+uqq4wuWQibsXUQPUHpxHfhWuvzfjdUX0kQiUv1+e5t/N8Pn9M1NYymWa3Jd8imu+dX/Nv0FWkHPEk76Apa4dKhA4HTpuLesyfKycnosoW4JLYOImfgF0ovVpiktY6+9BLrDgkiYSvvbF7P/35dQ/fUFgTnRJDvmE5fz0/oUfIzp3b5kn3CCZTC7OODS8cOeA4dhtfgQZirMQOmEEariSF+3IFNQDvgBBAHWCtYVWutB1V5B3ZMgkjY2ou/fMuGP3+nR2p7AvKakO98iivd36dN3mbSj3qTk+BMSW7pusrVFeeICDwG9Mdr5Eicw8MNrV2IyrL1EVEA8BOlY8ld7I48uY9IiErQWjP/uy/Yu3MHPVM7410QSIFLPIPcPqe9w5+QV0RWvBvZse4UpJlK79Qzm3EMCcGtZyReI0fhHtkD5eho9FsRokK2DqJ3gDuAQ8BSSqd6uNB9ROurtAM7J0EkapLFqpn6xXskHzhKj5RueBT5YMVKvksywU4H+Jd5I03VXgqTFVkn3clNcsZaVNrW7OODS4cOeA0disdVQ2Qqc2FXbB1EiZSehmuvtc60QX11igSRqA0FRSVM/u+7JJ44TlChG6G5IQTlhGHWZiyqhGKXeJo57aG3aSO+2cfJS3AiO86VoqzSExDKxQWniAg8+l+O98hRODePMPgdiYbO1kGUA3yvtb7eFsXVNRJEorYlZmazctsfrDu4DVNGPsGFboTmhhKQ2wSFiRJTEVaXGFo57qRbyTZckhPJiXMl75QjWBWYTTg0CsatVy98Ro3ErWdPuU9J1DpbB9FW4LTWeqgtiqtrJIiE0U6kpbN8y2/8eWgnzjnFBBd4EJrTFL/8EACKTfmYXY/QzrSNNhn7MCekkhPviqWgdEJmk7cXLh064jl4EG49InEKbYLJ3d3ItyQaAFsH0URKvxvq3NAu3QYJImF/DiWn8O7mdUQd24dbjoXGBV6EZofhVVg6/1GRQzauzgdpX7yTpkkHsZ7MoSj974salLMTZi9PHBoF4xjaFOeWLXHp2AHntm1xDAxEmevV9UbCIDVx+fazwK3APGCN1jru0kqsOySIhD3TWrMrLol3t/xKdEw0nnmaxvneNMmOwL249P6jIsc0fBz20Sp7P37ZyThmZaLScynJNqEtZ1wIa1aYXJ0x+/jiGNIEp+YtcG7TGtdOnXCKaI7ZQ46iROXZ+ojIUoXVtda6Xp2MliASdYnVqtkcE8uKLb9yIu44PrkmGhf40CS7OS4lfweJFQsljpk4qDQ8LKl4Fp7GKy8Nz6wMnDMycMjIxKEkv/x+DZOTwuTugtk/AMfQcJxbt8OlQ3tcOnbEsVEj+Q5K/IOtg6iiG1fPS2ttqtIOaolS6hNKb8i1AMXAbK312ou1kyASdVmJxcqGo8d4f8uvJCXG4VQEriUm3C2OeJS44lHijkehDx5FPpjP+QxpVYUo0nApScO94DQeuRm4Z6bjnJuBS2E6zoXpmHUJZleF2dMNk4tz+Z2GSgEmVfr0zFnM1F8vKjQaqyq9RcpK6b8WZaYYE8XKgUJlpshkpliZUS6utBo7kaBBA3Fxc0SZZJJBeydThVdAKeWjtc4o+7kbsBYI0FpfMGgliER9lFNYwsmMNPYlJbI7LobjKfFkZWWi84txLAJXixm3Eic8SlzxLHbHo8gXt+IK7lOyZuFcnI5bfhpmSzFamdHKAWvZv6XPzVjP+Fmb/nq9bF3T38svxmzJwUwhjl7+uPi44+rphLuPM55+LngFuOLTyA1PPxdcPBxwcJTvuox0oSCyi+NnpVQoMBOIBLoArkCE1jqmgnWbAi8DV1L6eepnYKrWOrYq+/wrhMrI4F2iQfNwdqBdoyDaNQriui5dKlynoNhCXEYa+5MTiYo9zv5Th8jOyMaaX4Jjsca1xAH3srDyKA7GbHXAYrKglRWrsvz9wIpW+ViVFY219F9lRSuNpnSZVqXHRQoNaBRWTNqKidJjJ9fCHPzTrfhneoPyoSA/j/QsX06bPf5RtzIpHBxNODibcHZ1xNXTETcvZzx9nfAMcMMnyBWvQBdcPZxwcnWQKdwNYBdBBLQExgHbgd+AIRWtpJRyo3TA1ULgNkqP3p8GflVKddZa51Zlp0qpl4FRlAbR2IsdDQnRkLk4mmkZGEjLwEBGduxc4TolFivxmRkcTEokr7AIH3dXPJwdcXdxwdnBEVcHR1wcnXB1cMLF8dL+6GcX5PLKNy+Tt+UTIo5n0GuPxrnYgQxvb5JDG+PQrD8qtDd5uRYKc4spKrSQfbqAjOS8f25MURpWjiac3RwJbedL5LBwPHxdql2fqDy7ODWnlDL9FQJKqbuA/6OCIyKl1EPAS0AbrfWRsmURlE5X/qjW+qWyZT8DXc+zu1Fa6z/O2e5Q4ClKZ50tulCtcmpOCPuTmJnKG589idPuTbQ4lkvnYxoHK6T5QkYzT9oNH0+zG++nMM9CXlYRmcl5ZKTkk326gNyMAvKyiinILaaooISSQguWEo3ZQdGouTc9rgqjaXs/OVK6RHXqO6KLBNFawEVr3fec5esBtNb9L2G/R4AbtNbbL7SeBJEQ9m1f4lHeXzUP/+gDtD5SQJv40uVJwYrcZn50u20GTQaNqrCt1WIlL7OI3eviiN6STG5GIQBeAS6069OYTgOb4OwqA8tWR30KoiTgK631PecsfxO4XmsdWMl9uALBWuvjZc/7AN8BzbXW6RdqK0EkRN2x4eBGfvh0ISHHYmkXXUyTNCgxQ3yoCUtEKH0ffBaf9t0qbGu1WDmy4xRRP5/kdFwOVovGycVMsw7+9BgWRkCoZy2/m7qtPgVREfCS1nrWOcufBmZV9p4lpZQf8C3gSekEf7nAPK31L+dZ/27gboBmzZr1OHHiRJXekxDCWFprvvj9M/Z8u5TGMafodNiCTy7kO0NcmCOOrdsw4JHFuIY0qbB9alw22384Qey+NIryS1AmhX8TdzoPbErrno0wO9jlXSp2RYLIhuSISIi6raikmJVfv0ryhs9pGpNJx6NWXIohwxMSw1zw63k5A2a+UmHbgrxidv8Sx6FNiWSdLgDAzcuJFj2C6DakGZ5yccN51acgSga+vNRTc5dCgkiI+iMzP4cVHzxOyY71hMXk0jZGY9YQ3caRoct/wdkvoMJ2VqsmZk8qUT/FciomC0uJxsHJRHBzb7oNCaNpO1+5uOEc9SmIfgGctNb9zlm+jtL3Uu2LFSpLgkiI+ikhNZ5Vy2fju2Mn/9pZQlyIIuLJF2n576sv2C4jOZftP5wgZs9pCnKKUQq8Al1p37cxHS5vgrOrvdwlY6z6FERTgReB1lrrY2XLwim9fHuW1npRDdY1AhjRsmXLSYcPH66p3QghDGaxWHj1wX9zxfp08lwh/9ZRDHrg2Yu2KyooYd9vCez/PYHMU3loDc5uDjTr4E+3q5oR2MAvbqgTQaSUuq7sx0HAvcD9QAqQ8td040opd2AXkA/MpfSG1qcoveigs9b6vFOW24ocEQnRMLz6xPV0X70XjwI4PLA517/ybaXaaavm5ME0dv4YS9KxTEqKrJgcFAFNPOg0MJRWkY0wmxvexQ11JYjOV8h6rfWAM9ZrxtlD/KyldIifmJquESSIhGhIPvpwId7vrCQiEfZ2dWPM+5swOzlVun1mSh5RP5/k2M4U8rJK75UPbObJmBndG9zYd3UiiOoKCSIhGpaduzewf8F9dD9g5WhzE71e+ZSgVh2qtI2ighKiNyeze91J0hPzCGzmyXUze2BqQEdGFwqihtMLl0gpNUIp9XZmZqbRpQghalG3zpczfOUmfu3jSsRxK/tuv46tny+t0jacXBzo2L8JN82/jPDOAaTEZvPVK1HIgUApCaJK0lp/o7W+29tbBuoWoqHx9vDi/uU7+Onqxnhmg376Fb584rYqb0cpxbB7O9K4lQ8J0Rl8t2R3DVRb90gQCSFEJU1dtJadt11Orgu0+GQLH911eZW3YTKZGPVQVwKbeRCz+zQ/L99XA5XWLRJEQghRBZMefouiJ2ZytKmi6+8pfD6mCwXpp6u0DZODiWund8c32I1Dm5PZ8PGhGqq2bpAgEkKIKrpqyES6vvM//uzqQPv9Ray77t8c/OOHKm3D0cmBsY/2wNPPhT3r4tn89bEaqtb+SRBVklysIIQ4U3jTNkz4YCe/DPCmcZIm9eFprHljTpW24ezmyNiZPXDzcmLb9zHs/KlKE03XGxJElSQXKwghzuXg4MDkpX/y25g2OJRA4Fv/46NpI6q0DXdvZ8Y82gMXNwc2fXGE/b/H11C19kuCSAghLtHkp77k+OTrSPGFzt8f4aPxPSkpuuBkz2fxDnBl1MPdcHA2s25VNEe2JddgtfZHgkgIIWzgpjuewm/RYva2NtF1Ww7fjulO0tEDlW4f0MSTkQ92xeyg+GnFfmL3Ve0CiLpMgkgIIWykZ+RVDFm5jt97OdPyiIW9t49h4xdvV7p9cHNvht7dEQV8/9YeEo81jO+kJYiEEMKGfL0DmfR+FOuGBOGXAab/vMx/n7yj0u3DOgYw6Pb2WEs037waRWpcdg1Wax8kiCpJrpoTQlTF5FfXs/OmXhQ5QOtPNrHy7spPl9aqRyMuv6k1JYUWvnxpJ5mn8mqwUuNJEFWSXDUnhKiqO2e/R8HsBzjRWBG54RSfXN+V3IzKfffT4d9NuGxUCwrzS/j8xR3kZBTWcLXGkSASQogaNGT0/bR/cxXbO5rpvKeQX2+4nPjoPZVq22NoGN2ubEZ+VhGfP7eNgtziGq7WGBJEQghRw1q06sr1H2xlQ193ImKt7Lt7HPs3rqlU23+NaUn7fo3JSS/kv89to7igpIarrX0SREIIUQucXVy5Z9k2NgwOIjgFTs2YysYvl1eq7RXj29KiexCZp/L5/IUdlBRbarja2iVBJIQQtei+19azaXhzvLJBL3yeNcuerlS7qyZ1oGl7P07H5/DlSzuxWKw1XGntkSASQohads9z3xJ1fXcci8H3jQ/5/PnJF22jlOKayZ1p1NyL5ONZfPv6rnozsZ4EUSXJ5dtCCFu6Y96HHL3zKoocIOKDX1j12A0XbWM2mxg9rRv+Tdw5eSCdNW/vrYVKa54EUSXJ5dtCCFu7acpi0qfeRoYndPxyNysfGnbRNg6OZsbM6IF3oCtHd6bwy/uVH0bIXkkQCSGEgUbcPAvH+XOID4LuP8bw/l3/vmgbJ5fSuYw8fJ05sDGR3z87XAuV1hwJIiGEMNi/r5pA6KLXONJM0fP3VD4c3/Oi3/+4ejpx7fTuuHo6suuXk2z99ngtVWt7EkRCCGEHOnUfTLe3PmNvaxPdt+Xw6bhuFBcUXLCNl78rox/ujrOrA1tXH2f3rydrqVrbkiASQgg70Sy8A4NWrGVHJwc67ynk63E9yTp94bmJ/ELcGfFAVxyczPzx2RFOHkirpWptR4JICCHsiJ9fMNet3MKfkc60jy5h/S0DiTu874JtGkV4Mey+TmCCH9/ZR2F+3RoKSIJICCHsjLOLK7d/EMVvfT1oHmNl3z3XsX/z2gu2adrWj96jWlCQW8x3b1ZuLDt7IUFUSXIfkRCitt29bCu/DwqgcRIkPTKFjd+suOD63a5sRpM2PiQczmDHjydqp0gbkCCqJLmPSAhhhHve+I1NV4fhmwmWp5/jh+ULL7j+1fd2xsXDkc1fHeN0XE4tVXlpJIiEEMLO3f3iD0SN7YJLIfi+tpLPFz103nWdXB0YendHtNasfmNXnRiTToJICCHqgNsf/5gjEwdSYobw93/kg3k3nXfdJq196TywKTnphfz87v5arLJ6JIiEEKKOuGnqG6Q9cDPZ7tDpf1G8N+38QwL1HdsS/1APjmw/xaHNSbVYZdVJEAkhRB0y4tZ5qMceIdkfevwQw4p7Lq9wPaUUw6d0xtHZzLoPD5GTbr9TjUsQCSFEHdP/mrto9PzLxIQqLlufwvu39qpwPQ8fFwbc0paSIgurX4+y22kjJIiEEKIO6nLZUDq+uYr9LU303JLNqhu6Ulz4z6Oe1r0a0TIyiNPxuXY7OKoEkRBC1FHhrbpyxfKfiOpgptuuQr66IZK8rH/e63jlxPZ4+ruw59c44g7Z3xBAEkRCCFGH+QU2ZsyHW9nS3YkOB0v4fnxfCvJyz1rH5GDimsmdMZkVa97eR2GBfQ0BJEEkhBB1nLOLK7et2sXmSBfaR1v45ubeFBWcfZrOv7EHl5UNAfT9EvsaAkiCqJJkiB8hhL277f3tbOnuTMeDJXx5c08sJSVnvd7tymY0ae1D/KEMdv4Ua1CV/yRBVEkyxI8Qwt4pk4nx729jWxdHOu0v5rObevwjjK6+r3QIoD+/OsrpBPsYAkiCSAgh6hGzgwM3fbCdHZ0c6bKniE9uicRqsZS/7uTqwJC7OqCtmm9ft48hgCSIhBCinnFwdOS6lZuJ6uBAt12FrJrQ86zXm7b1o9OAULLT7GMIIAkiIYSoh5xdXBn1/kZ2tzPTY0c+K8dHnvV6v+tb4d/EnSPbTxG9xdghgCSIhBCinnJz9+SaFevZ28ZE5LZc3rvt7xEYSocA6mIXQwBJEAkhRD3m4e3Ple+uZX9LE702Z7Pi9j5/v+brQv9b2lBcaOwQQBJEQghRz/n4B3P5su842Fxx2aYMVtzVt/y1Nr2CadmjdAigP/57xJD6JIiEEKIBCGwURp+lXxEdrrjs9zSWnzFq9+Db2+Pp58zuX08aMgSQBJEQQjQQwc1a0X3pZxxppui1PoXl9w8EwFw2BJAyKdb8X+0PASRBJIQQDUjT8A50ePV9YkIVvX5NZPmDVwLg38STy0Y2pyCn9ocAkiASQogGpnnbSFq9/H/EhkDkz3Esn3Y1AN2HhBkyBJAEkRBCNECtO/Wl2fOvkhAEkWuOs2LGSMCYIYAkiIQQooHqEHklwf95nqQA6PbdYd6bNcaQIYAkiIQQogHr0ncEPk8+RaovdFl9gPfn3Vg6BFD/0iGA1q44UOM1SBAJIUQD13PAdbjNm026N3T6chcfPnEr/ca1wq+xO4e3JnN4a80OASRBVEkyH5EQoj7rc9WtmGZOJdMD2n2+lY8WTiofAujXDw6Rm1lzQwBJEFWSzEckhKjv+o+8B8sj95HjCq0//YPvlj1C/5tbU1xo4ZvXdtXYEEASREIIIcoNuu5B8h+cSIEzNP/wF/Zsep4W3QM5HZfDwU2JNbJPCSIhhBBnueqWmWTedwPFDtD0/R/ILVqFp78L8dEZNbI/CSIhhBD/cM3tj3Pq7tFoBU2Wf4lHk58ZdFu7GtmXBJEQQogKjb77GRJuvxqlIfjdj9jx6+c1sh8JIiGEEOc1ZsoiToy/AscSSH5mQY3sw6FGtiqEEKLeGPfwm3ymptD2smE1sn0JIiGEEBd1/bTXa2zbcmpOCCGEoSSIhBBCGEqCSAghhKEkiIQQQhhKgkgIIYShJIiEEEIYSoJICCGEoSSIhBBCGEqCSAghhKFUTU10VF8ppVKAE2cs8gYyq/A8AEitgdLO3Y8t211snfO9XtFye+mvivZlqzbSX1Vvc6H1pL+qtt6l9Ne5y2zZX2Fa68AKX9Fay+MSHsDbVXy+rTbqsGW7i61zvtcrWm4v/VXdPpP+qpk2F1pP+qv2+uvcZbXVX3Jq7tJ9U8XntVWHLdtdbJ3zvV7Rcnvpr+ruS/qrZtpcaD3pr6qtdyn9de6yWukvOTVXy5RS27TWkUbXUVdIf1WN9FfVSH9VTU31lxwR1b63jS6gjpH+qhrpr6qR/qqaGukvOSISQghhKDkiEkIIYSgJIjumlPJVSq1WSkUrpXYppX5USrU0ui57ppSaV9ZfVqXUaKPrsSdKqRZKqd/L+menUkq+G7kI+X2qvEv5eyVBZN80sFhr3Vpr3QVYDbxjcE327idgKLDB6ELs0FLgPa11a+BR4EOllDK4Jnsnv0+VV+2/VxJEVaCUClVKvaaU2qSUylNKaaVU+HnWbaqU+q9SKlMplaWU+kIp1awq+9NaZ2itfz5j0Uagwv3Zo9ruLwCt9Z9a62OXWrs9sGX/KaUCgd7ACgCt9U+AAnrU+BupRbb+natPv08VsWV/XcrfKwmiqmkJjAPSgd/Ot5JSyg34BWgL3AZMAFoBvyql3C9h/1OBry6hfW0zur/qOlv2XzMgUWtdfEbTmLLl9Yn8zlVNTfZX5f9e1dRdxfXxAZjO+PkuSg9FwytY7yHAArQ8Y1kEUAI8fMaynykdLqOiR99ztrmA0k8Ybkb3Qx3pr3XAaKP7wF76j9Ijn+hz2v0IjDH6fdprn9W336da7q8q/b2SI6Iq0FpbK7nqSOBPrfWRM9oeB/4ARp2xbLDWOuA8jz/+Wk8pNRe4Ghimtc6zzbupeUb1V31h4/6LBUKUUo5ntAsvW15v2Pp3rr6rif6qzt8rCaKa0QHYW8HyfUD7qmxIKbUAGAEM0VpXZ1DTusBm/dVAXbT/tNYpwBZgIoBS6kpKvyPaXjsl2h35nauaSvVXdf9eSRDVDD9Kz7meKw3wrexGlFIdgMcBf2C9UipKKbXNJhXaF5v0F4BS6nGlVBzQB3hHKRWnlAq1QY32rLL9dy9wu1IqGngBuEWXnUdpgCrVZw3096kiF+2vS/l75WCjIkUN0Frvo/RTq6gkrfXjlP7PIM6htT4M/MvoOuoS+X2qvEv5eyVHRDUjnYo/yZ/vU0VDJ/11aaT/qk76rGpqtL8kiGrGPkrPqZ6rPbC/lmupC6S/Lo30X9VJn1VNjfaXBFHN+BrorZRq/teCspvE+pa9Js4m/XVppP+qTvqsamq0v2T07SpSSl1X9uMgSr/8vR9IAVK01uvL1nEHdgH5wFxKr81/CvAEOmutc2q7bqNIf10a6b+qkz6rGrvoL6NvqKprj7L/ABU91p2zXjPgcyALyAa+pIIbuhuRsAAABYlJREFUxer7Q/pL+k/6zL4f9tBfckQkhBDCUPIdkRBCCENJEAkhhDCUBJEQQghDSRAJIYQwlASREEIIQ0kQCSGEMJQEkRBCCENJEAkhhDCUBJEQQghDSRAJYTCl1CtKKa2U6m90LUIYQYb4EcJgSqlYwAUI1lpbja5HiNomR0RCGEgp1RNoCnwlISQaKgkiIYw1puzf/xlahRAGkiASohLKvsPRZT/foJTapJTKUUplK6XWKqX6VXPT11I6rP7PlayjUVkt0RW8du9fdSqlWp7zWqey5ZurWacQNUaCSIgqUEo9CawCioBv+f/27j/WqzmO4/jztUrREtmSX1vTHyncS6zJZqWYP6yxYpYYFjMxtLG7EW3+aNOI+RGWYTP8kVmNxdAUxqZJ6f6haWPFCJWSyo+7tz8+n1PH6d6655bvN93X45/z/X4+53zOOX/c++rz6wbfAROBZZLG1WxrNDASWBoRf3bzsq35OKjSVh/gnlLRkMp1s/JxXp1nNGsEB5FZPbcDYyNifERcA5wJLASOAh6q2VYxLPdGdy/IgbWTShABU4ERwCf5+54gkjQUuBZYj4cA7TDkIDKrZ05EfF58yQsMHshfL5LUr0ZbU4DdwNs1n2ELMFBS+ee3DfgJeCJ/L/eIZgL9gfleEGGHIweRWT1vVQsiYhNpyKw/cEJ3GpE0HDgXeC8idtR8hn8Nz0m6FBhDCqEfc92QXNcfuA34GXip5n3MGsJBZFbPhi7Kt+fjgG62U3tYrqQ6T9QG7AAWANty2fH5OB0YCjwVEbt6cC+z/5yDyKyGQzi0NQX4G3izB9duycdBks4HJgELI2Ir8GuuK4bm7ibNKT1dbUTSD5IelDRb0sa8AnChpD6SLpS0QtLvktZIaqlcO1XS+5I2SdolqV3S1aX68/IqvRtKZYMlrZX0kaTuBrb1Ag4iswaTdCIwDvgwIjb3oImiR3QsqTf0F/BYLit6REMkXQKcDbxQvU9ewDAMmAGcnI9PAjfn4zPAs8A0Uu9qfuUZWoDXgeuAycBy4FVJIwHyPNpi4P4cbP3y+f2AKyJidw/e245QfZv9AGa90JWkfwT2ZFgO9gbRGFLP6pWI2JjLtgNB6hHNAjrYN0QAWvPx5YiYnT+/K+kOUrC05B4Wudc1o3xxRMwpPuel48vzOWOBdblqDrCaFGYTSeF1QURswazEQWTWeFNIYbG4h9cXv8jvA0Rpb1BEdEjaQQqpk4BFEfFNJ220AH8ADxcFkvqSFlwsKEIoG8TeIb9iAcStwE3A6aSeWWFn6Vm+lLQIeC4XTejiWayXcxCZNZCk44CLgc8i4vseNlOExKmkzbDtlfptuQ663sDaCqyMiN9KZaNJ+6GWVc5tAdoBJAlYQgq6x4FVwGZgQr7XV5Vr1wPHAPMiYuWBXsx6J88RmTXWZNI8ycFsLC33VjoLmmKe6IPynqeKVtKwWbWsA1jbSfma/HkccBkwPSLmRsQ7OWBGkPZE7QkiSdNIc1grgeslHb3ft7Jey0Fk1g0RoYjQfuqH53O+PUBTB7Nsu7jXa8XzRMSKTurPynUTO7s+Lxw4g32D6BxgXXmZt6RTSHujinNPy8dy4IwGbgTaI6Ijl40n7VtqA67Kbcys+arWSziIzBrrU+DeiPi6ic8wijQE11kQfVEpKxY1FD2iVeQFEJIm5cUNS0i9odUAkkaR5r+ej4hHI2ID8CLQJmngoX4Z+/9zEJk1UETMi4hHmvwYraQ9TNW5pa6G634p5rNygN5CGqJbAlxO+jt3A4DVkoaR/mTRx8CdpXbmAoOBuw7pm9gRwf9Dq5mZNZV7RGZm1lQOIjMzayoHkZmZNZWDyMzMmspBZGZmTeUgMjOzpnIQmZlZUzmIzMysqRxEZmbWVP8AMi/nHtLJZO0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

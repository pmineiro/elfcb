{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discretely many importance weights and rewards, maximum likelihood of sample $\\{ (w_i, r_i) \\}$ from $h$ is \n",
    "\\begin{alignat}{2}\n",
    "&\\!\\max_{Q \\succeq 0} &\\qquad& \\sum_n \\log(Q_{w_n, r_n}),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mle\n",
    "sumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:m\n",
    "lesum}\n",
    "\\end{alignat}\n",
    "Estimate is $\\hat V(\\pi) = \\vec{w}^\\top \\hat{Q} \\vec{r}$. \n",
    "\n",
    "Dual (ignoring constants) is $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta,\\gamma}& -\\beta - \\gamma + \\sum_{n} \\log\\left(w_n \\beta + \\gamma\\right)\\; \\text{ s.t. } \\; \\forall w,r: w \\beta + \\gamma \\geq 0.\n",
    "\\end{aligned}\n",
    "$$ One dual variable can be eliminated by summing the KKT stationarity conditions and leveraging complementary slackness.  Introducing $\\phi \\succeq 0$ as the (matrix of) dual variables associated with $Q \\succeq 0$: $$\n",
    "\\begin{aligned}\n",
    "\\frac{c_{w_i,r_j}}{q_{w_i,r_j}} &= \\phi_{w_i,r_j} + w_i \\beta + \\gamma \\implies n = 0 + \\beta + \\gamma, \\\\\n",
    "\\end{aligned}\n",
    "$$ resulting in the 1-D dual $$\n",
    "\\begin{aligned}\n",
    "\\sup_{\\beta} & \\sum_{n} \\log\\left((w_n - 1) \\beta + n\\right) \\; \\text{ s.t. } \\;\\forall w,r: (w - 1) \\beta + n \\geq 0.\n",
    "\\end{aligned}\n",
    "$$  This can be solved by 1-D bracketed search on the gradient followed by recovery of the primal values.\n",
    "\n",
    "Primary recovery begins with the primal-dual relationship for observed $(w, r)$ pairs: $$\n",
    "\\hat Q_{w,r} = \\sum_n \\frac{\\mathbb{1}_{w=w_n,r=r_n}}{\\beta^* (w_n - 1) + N}.\n",
    "$$  The MLE will sometimes put mass on unobserved importance weights, in which case the distribution over rewards for that importance weight is not determined.  The unobserved mass can be determined by solving the linear feasibility problem $$\n",
    "\\begin{alignat}{2}\n",
    "& &  & w_{\\min} \\hat{q}_{\\min} + w_{\\max} \\hat{q}_{\\max} = 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "&                  &  & \\hat{q}_{\\min} + \\hat{q}_{\\max} = 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N}, \\notag \\\\\n",
    "& & & {\\hat{q}_{\\min} \\geq 0, \\hat{q}_{\\max} \\geq 0},\\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\hat{q}_{\\min}$ and $\\hat{q}_{\\max}$ are associated with\n",
    "$w_{\\min}$ and $w_{\\max}$ respectively.  For robustness we convert this into a non-negative least squares problem $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{q_{\\min} \\geq 0, q_{\\max} \\geq 0} &\\qquad& \\left\\| \\left(\\begin{array}{cc} 1 & 1 \\\\ w_{\\min} & w_{\\max} \\end{array} \\right) \\left(\\begin{array}{c} q_{\\min} \\\\ q_{\\max} \\end{array}\\right) - \\left(\\begin{array}{c} 1 - \\sum_n \\frac{1}{\\beta^* (w_n - 1) + N} \\\\ 1 - \\sum_n \\frac{w_n}{\\beta^* (w_n - 1) + N} \\end{array} \\right) \\right\\|^2. \\notag\n",
    "\\end{alignat}\n",
    "$$\n",
    "When $q_{\\min} + q_{\\max} > 0$, the MLE is actually an interval; the center of this interval is found using $1/2 (r_{\\min} + r_{\\max})$ as the reward for unobserved importance weights.\n",
    "\n",
    "**Using a baseline:** When using a baseline, pass in shifted rewards and then add the correction to the result.  Given reward predictor $\\hat r: \\mathcal{X} \\times A \\to [r_{\\min}, r_{\\max}]$, construct data for the MLE $$\n",
    "\\begin{aligned}\n",
    "(w_n, \\tilde r_n) &\\leftarrow \\left(\\frac{\\pi(a_n|x_n)}{h(a_n|x_n)}, r_n - \\hat\n",
    "r(x_n, a_n) \\right),\n",
    "\\end{aligned}\n",
    "$$ apply the MLE on this data (with modified $\\tilde r_{\\min}$ and $\\tilde r_{\\max}$), and then adjust the result via $$\n",
    "\\begin{aligned}\n",
    "\\hat V^{\\text{(rpmle)}} &= \\hat V^{\\text{(mle)}} + \\sum_n \\sum_a \\pi(a_n|x_n) \\hat r(x_n, a_n).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**With censorship**: Suppose some $r_j = \\varnothing$ implying the reward was exogenously censored, and suppose we want to estimate $$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[r | r \\neq \\varnothing\\right] = \\frac{\\mathbb{E}\\left[r 1_{r \\neq \\varnothing}\\right]}{\\mathbb{E}\\left[1_{r \\neq \\varnothing}\\right]}.\n",
    "\\end{aligned}\n",
    "$$ One possible estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) = \\frac{w^\\top Q (r 1_{r \\neq \\varnothing})}{w^\\top Q 1_{r \\neq \\varnothing}}\n",
    "\\end{aligned}\n",
    "$$ which is straightforward when there is no mass assigned to unobserved importance weights.  When there is mass assigned to unobserved importance weights, the MLE is again an interval and we can choose the center point of the interval as the estimate.\n",
    "\n",
    "In python we represent censored rewards with `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume no duplicates and reduplicate at the end.\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2,\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$\n",
    "Lagrangian:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(Q, \\beta, \\gamma) &= \\beta  (\\vec{w}^\\top Q \\vec{1} -1) + \\gamma (\\vec{1} Q \\vec{1} - 1) + \\sum_{n} \\frac{1}{2} \\left(N Q_{w_n,r_n} - 1\\right)^2. \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\beta w + \\gamma \\right) Q_{w,r} + \\frac{1}{2} c_{w,r} \\left(N Q_{w,r} - 1\\right)^2 \\right). \\\\\n",
    "\\frac{\\partial}{\\partial Q_{w,r}} L(Q, \\beta, \\gamma) &= \\beta w + \\gamma + c_{w,r} N \\left(N Q_{w,r} - 1\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Dual will be unbounded unless $\\forall w: \\beta w + \\gamma \\geq 0$.  $\\beta w + \\gamma = 0$ can only happen everywhere or at $w = w_{\\min}$ or $w = w_{\\max}$ so we will only potentially place undata on an extreme point.  Continuing $\\ldots$\n",
    "<!---\n",
    "1/2 (n q - 1)^2 + (\\[Gamma] + \\[Beta] w) q \n",
    "Solve[D[%, q] == 0, q] // FullSimplify // Collect[#, n]&\n",
    "%% /. %[[1]] // FullSimplify // Collect[#, n]&\n",
    "--->\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &= \\max\\left\\{0, \\frac{1}{N} - \\frac{\\beta w + \\gamma}{N^2}\\right\\} & (c_{w,r} = 1). \\\\\n",
    "\\end{aligned}\n",
    "$$ The $\\max\\{0,\\ldots\\}$ is difficult to deal with so ignore that for the purpose of finding (approximate) closed-form expressions for the dual variables.  This is equivalent to relaxing the feasible region to measures which are signed on observed values but unsigned on unobserved values.  Continuing $\\ldots$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g (\\beta, \\gamma) &= \\inf_{Q \\succeq 0} L(Q, \\beta, \\gamma) \\\\\n",
    "&\\geq -\\beta - \\gamma + \\sum_n \\left( \\left( \\beta w_n + \\gamma \\right) \\left(\\frac{1}{N} - \\frac{\\beta w_n + \\gamma}{N^2} \\right) + \\frac{1}{2} \\left(\\frac{\\beta w_n + \\gamma}{N}\\right)^2 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_n \\left( \\frac{\\beta w_n + \\gamma}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{(\\beta w_n + \\gamma)^2}{2 N^2} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ The unconstrained $\\gamma$ optimum is $\\beta \\frac{1}{N} \\sum_n w_n$ but this is infeasible.  Therefore maximizing $\\gamma$ under the constraint is $$\n",
    "\\gamma^* = \\begin{cases} -\\beta w_{\\min} & \\beta > 0 \\\\ -\\beta w_{\\max} & \\beta \\leq 0 \\end{cases} \\doteq -\\beta w_{\\text{sgn}(\\beta)}\n",
    "$$ Substituting we get $$\n",
    "\\begin{aligned}\n",
    "g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -\\beta  + \\sum_n \\left( \\frac{\\beta w_n}{N} - \\frac{\\beta^2 (w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\right) \\\\\n",
    "&= -\\beta + \\beta \\sum_n \\frac{w_n}{N} - \\beta^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\\n",
    "\\frac{\\partial}{\\partial \\beta} g\\left(\\beta, \\gamma^*(\\beta)\\right) &= -1 + \\sum_n \\frac{w_n}{N} - \\beta \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2} \\\\\n",
    "\\beta^* &= \\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} = \\begin{cases}\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N^2}} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases} \\\\\n",
    "g(\\beta^*, \\gamma^*) &= -\\beta^* \\left(-1 + \\frac{1}{N} \\sum_n w_n\\right) + {\\beta^*}^2 \\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{2 N^2} \\\\ \n",
    "&= \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} - \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}} \\\\\n",
    "&= \\frac{1}{2} \\frac{\\left(-1 + \\frac{1}{N} \\sum_n w_n\\right)^2}{\\sum_n \\frac{(w_n - w_{\\text{sgn}(\\beta)})^2}{N^2}}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "So (approximately)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w,r} &=\n",
    "\\begin{cases}\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\min})^2}{N}}\\right)\\left(w - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\max\\left\\{0, \\frac{1}{N} - \\frac{1}{N} \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\sum_n \\frac{(w_n - w_{\\max})^2}{N}}\\right)\\left(w - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "& (c_{w,r} > 0).\n",
    "\\end{aligned}\n",
    "$$ and the value estimate is $$\n",
    "\\begin{aligned}\n",
    "\\hat V(\\pi) &= \n",
    "\\begin{cases}\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\min})^2}\\right)\\left(w_n - w_{\\min}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n > 1 \\\\\n",
    "\\frac{1}{N} \\sum_n w_n r_n \\max\\left\\{0, 1 - \\left(\\frac{-1 + \\frac{1}{N} \\sum_n w_n}{\\frac{1}{N} \\sum_n (w_n - w_{\\max})^2}\\right)\\left(w_n - w_{\\max}\\right)\\right\\} & \\frac{1}{N} \\sum_n w_n \\leq 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$ Note both denominators can be computed given $\\frac{1}{N} \\sum_n w_n$ and $\\frac{1}{N} \\sum_n w_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cressie-Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume no duplicates and re-duplicate at the end. $$\n",
    "\\begin{alignat}{2}\n",
    "&\\!\\min_{Q \\succeq 0} &\\qquad& \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right),\\label{eq:mle}\\\\\n",
    "&\\text{subject to} &  & \\vec{w}^\\top Q \\vec{1} = 1, \\tag{$\\beta$} \\label{eq:mlesumw} \\\\\n",
    "&                  &  & \\vec{1}^\\top Q \\vec{1} = 1. \\tag{$\\gamma$} \\label{eq:mlesum}\n",
    "\\end{alignat}\n",
    "$$  Dual is $$\n",
    "\\begin{aligned}\n",
    "L (\\beta, \\gamma, Q) &= \\beta \\left(\\vec{w}^\\top Q \\vec{1} - 1\\right) + \\gamma \\left( \\vec{1}^\\top Q \\vec{1} - 1 \\right) + \\frac{2}{\\lambda (1 + \\lambda)} \\sum_n \\left( \\left( N Q_{w_n, r_n} \\right)^{-\\lambda} - 1 \\right) \\\\\n",
    "&= -\\beta - \\gamma + \\sum_{w,r} \\left( \\left( \\gamma + \\beta w \\right) Q_{w,r} + c_{w,r} \\frac{2}{\\lambda (1 + \\lambda)} \\left( \\left( N Q_{w, r} \\right)^{-\\lambda} - 1 \\right) \\right) & \\left( c_{w,r} \\in \\{ 0, 1 \\} \\right).\n",
    "\\end{aligned} \n",
    "$$ This is unbounded unless $\\forall w: \\gamma + \\beta w \\geq 0$. \n",
    "<!--- \n",
    "(\\[Gamma] + \\[Beta] w) Q + (2/(\\[Lambda] (\\[Lambda] + 1)))((N Q)^(-\\[Lambda]) - 1)\n",
    "D[%, Q] == 0\n",
    "Solve[%, Q]\n",
    "%% /. %[[1]] // Simplify // PowerExpand // Simplify\n",
    "(%%%% /. %%[[1]] // Simplify // PowerExpand // FullSimplify // Apart) /. -1 + 1/(1 + \\[Lambda]) -> -\\[Lambda] / (1 + \\[Lambda]) /. 1 - 1 / (1 + \\[Lambda]) -> \\[Lambda] / (1 + \\[Lambda])\n",
    "--->\n",
    "Continuing $\\ldots$ $$\n",
    "\\begin{aligned}\n",
    "0 &= \\gamma + \\beta w - \\frac{2 N}{1 + \\lambda} \\left( N Q_{w, r} \\right)^{-1 - \\lambda} & (c_{w,r} = 1) \\\\\n",
    "\\left( N Q_{w, r} \\right)^{-1 - \\lambda} &= \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\\\\n",
    "Q_{w,r} &= \\frac{1}{N} \\left( \\frac{1 + \\lambda}{2 N} \\left( \\gamma + \\beta w \\right) \\right)^{\\frac{-1}{1 + \\lambda}}  & \\left( \\lambda > -1 \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$ Substitute $\\gamma \\leftarrow \\gamma \\frac{1 + \\lambda}{2 N}$ and $\\beta \\leftarrow \\beta \\frac{1 + \\lambda}{2 N}$ to get\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^*_{w, r} &= \\frac{1}{N} \\left(\\gamma + \\beta w\\right)^{\\frac{-1}{1 + \\lambda}} & \\left( c_{w,r} = 1, \\lambda > -1 \\right) \\\\\n",
    "g (\\beta, \\gamma) &= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\sum_n \\left( \\frac{2}{1 + \\lambda} \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} + \\frac{2}{\\lambda (1 + \\lambda)} N Q_{w_n, r_n} \\left( \\gamma + \\beta w_n \\right) \\right) \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda} \\sum_n \\left(\\gamma + \\beta w_n\\right) N Q_{w_n, r_n} \\\\\n",
    "&= -\\frac{2 N}{1 + \\lambda} \\beta -\\frac{2 N}{1 + \\lambda} \\gamma  - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2}{\\lambda}  \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\\\\n",
    "&= - \\frac{2 N}{\\lambda (1 + \\lambda)} + \\frac{2 N}{(1 + \\lambda)} \\left( - \\beta - \\gamma +  \\frac{1 + \\lambda}{\\lambda N} \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\right) \\\\\n",
    "&= \\frac{2 N}{(1 + \\lambda)} \\left( -\\frac{1}{\\lambda} - \\beta - \\gamma +  \\frac{1 + \\lambda}{\\lambda N} \\sum_n \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} \\right) \\\\\n",
    "&= \\frac{2 N}{(1 + \\lambda)} \\left( 1 - \\beta - \\gamma +  \\frac{1}{N} \\sum_n \\frac{1 + \\lambda}{\\lambda} \\left( \\left(\\gamma + \\beta w_n\\right)^{\\frac{\\lambda}{1 + \\lambda}} - 1 \\right) \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Just hit it with a generic convex solver $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Censorship changes results\n",
    "\n",
    "We learned this the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17414127154917453,\n",
      " {'betastar': -421.93139841688657,\n",
      "  'num': 159912,\n",
      "  'qex': {0: 2.7755671722026296e-17, 380: 0.0005377660516997341},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c1f9e18>,\n",
      "  'vmax': 0.276316821372124,\n",
      "  'vmin': 0.07196572172622505})\n",
      "(0.15222508738880963,\n",
      " {'betastar': -708.0158311345647,\n",
      "  'num': 268338,\n",
      "  'qex': {0: 0.0, 380: 0.00022164515295090473},\n",
      "  'qfunc': <function estimate.<locals>.<lambda> at 0x7f5f3c203048>,\n",
      "  'vmax': 0.22764427578168045,\n",
      "  'vmin': 0.07680589899593883})\n"
     ]
    }
   ],
   "source": [
    "data, wmin, wmax, censored = None, None, None, None\n",
    "for data, wmin, wmax, censored in [\n",
    "    # some data where exogenous censorship is discarded\n",
    "   ([ (c, w, r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]\n",
    "     if w >= 0\n",
    "    ], 0, 380, False),\n",
    "    # same data where exogenous censorship is modeled\n",
    "   ([ (c, -w if w < 0 else w, None if w < 0 else r) for c, w, r in [ \n",
    "      (86, -59.999996, 0.0), (44884, -1.0497237, 0.0), (16331, -1.0447762, 0.0), (31257, -1.0344828, 0.0), \n",
    "      (15868, -1.0, 0.0), (41332, 0.0, 0.0), (1958, 0.0, 1.0), (17763, 1.0, 0.0), (1339, 1.0, 1.0), \n",
    "      (30726, 1.0344828, 0.0), (3867, 1.0344828, 1.0), (2034, 1.0447762, 1.0), (16728, 1.0447762, 0.0), \n",
    "      (40629, 1.0497237, 0.0), (3445, 1.0497237, 1.0), (85, 59.999996, 0.0), (6, 59.999996, 1.0), \n",
    "     ]], 0, 380, True),\n",
    "]:\n",
    "    import MLE.MLE\n",
    "\n",
    "    from pprint import pformat\n",
    "    print(pformat(MLE.MLE.estimate(datagen=lambda: data, \n",
    "                                   wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True, censored=censored)))\n",
    "  \n",
    "del data, wmin, wmax, censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Comparison with CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CVXPY (primal) implementation\n",
    "\n",
    "class MLETest:\n",
    "    @staticmethod\n",
    "    def cvxestimate(data, wmin, wmax, rmin, rmax):\n",
    "        import cvxpy as cp\n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        cdict = defaultdict(int)\n",
    "        n = 0\n",
    "        for (ci, wi, ri) in data:\n",
    "            assert ci >= 0\n",
    "            assert wi >= wmin and wi <= wmax\n",
    "            assert ri >= rmin and ri <= rmax\n",
    "            if ci > 0:\n",
    "                cdict[(wi, ri)] += ci\n",
    "            n += ci\n",
    "        assert n >= 1\n",
    "        cdict[(wmin, rmin)] += 0\n",
    "        cdict[(wmin, rmax)] += 0\n",
    "        cdict[(wmax, rmin)] += 0\n",
    "        cdict[(wmax, rmax)] += 0\n",
    "        cdict.default_factory = None\n",
    "        \n",
    "        wvec = np.array(list(set(w for (w, _), _ in cdict.items())))\n",
    "        wmaxvec = np.max(wvec)\n",
    "        rvec = np.array(list(set(r for (_, r), _ in cdict.items())))\n",
    "        C = np.array([ [ cdict.get((w, r), 0)/n for r in rvec ] for w in wvec ])\n",
    "        Q = cp.Variable((len(wvec), len(rvec)))\n",
    "            \n",
    "        prob = cp.Problem(cp.Maximize(cp.sum(cp.multiply(C, cp.log(Q)))), [\n",
    "                                cp.sum(cp.matmul((wvec/wmaxvec).T, Q)) == 1/wmaxvec,\n",
    "                                cp.sum(Q) == 1\n",
    "                          ])\n",
    "        prob.solve(solver='ECOS')\n",
    "            \n",
    "        vhat = 0\n",
    "        for i, wi in enumerate(wvec):\n",
    "            for j, rj in enumerate(rvec):\n",
    "                if cdict.get((wi, rj), 0) > 0:\n",
    "                    vhat += wi * Q.value[i, j] * rj\n",
    "                else:\n",
    "                    vhat += wi * Q.value[i, j] * 0.5 * (rmax - rmin)\n",
    " \n",
    "        from scipy.special import xlogy\n",
    "    \n",
    "        return vhat, { \n",
    "            'qstar': { (wvec[i], rvec[j]): Q.value[i, j] for i in range(len(wvec)) for j in range(len(rvec)) },\n",
    "            'likelihood': np.sum(xlogy(C, Q.value)),\n",
    "            'sumofone': np.sum(Q.value),\n",
    "            'sumofw': np.sum(wvec.dot(Q.value)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:47<00:00,  7.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def testestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "\n",
    "    wsupport = [ 0, 2, 20 ]\n",
    "    wmax = wsupport[-1]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=5)\n",
    "\n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(datagen = lambda: data, wmin=0, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=0, wmax=wmax, rmin=0, rmax=1)\n",
    " \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "testestimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:48<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "def megatestestimate():\n",
    "    from importlib import reload\n",
    "    from math import ceil\n",
    "    import environments.ControlledRangeVariance\n",
    "    import MLE.MLE\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm as tqdm\n",
    "    import sys\n",
    "\n",
    "    reload(environments.ControlledRangeVariance)\n",
    "    reload(MLE.MLE)\n",
    "    \n",
    "    def getenv():\n",
    "        import numpy\n",
    "        wsupport = numpy.geomspace(0.5, 1000, 10)\n",
    "        env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "        return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "    env = getenv()[0]\n",
    "    wmin, wmax = env.range()\n",
    "    \n",
    "    for ndata in tqdm(map(ceil, np.logspace(1, 7, 14)), file=sys.stderr, total=14):\n",
    "        for i in range(1001):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            vhat, qstar = MLE.MLE.estimate(lambda: data, wmin=wmin, wmax=wmax, rmin=0, rmax=1, raiseonerr=True)\n",
    "            try:\n",
    "                cvxvhat, cvxqstar = MLETest.cvxestimate(data, wmin=wmin, wmax=wmax, rmin=0, rmax=1)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            from pprint import pformat\n",
    "            assert np.allclose(vhat, cvxvhat, atol=1e-4) or not np.isfinite(cvxqstar['likelihood']), pformat(\n",
    "            {\n",
    "                'data': [(c, w, r) for c, w, r in data if c > 0],\n",
    "                'vhat': vhat,\n",
    "                'cvxvhat': cvxvhat,\n",
    "                'qstar': qstar,\n",
    "                'cvxqstar': cvxqstar,\n",
    "            })\n",
    "                                    \n",
    "megatestestimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     43,
     50
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Euclidean ******\n",
      "****** TwoThirds ******\n",
      "****** MinusOneHalf ******\n",
      "****** One ******\n",
      "****** Almost MLE ******\n",
      "****** MLE ******\n",
      "****** Lambda Zero ******\n"
     ]
    }
   ],
   "source": [
    "def produceresults(env, method, minexp=1, maxexp=5, numpts=20, ndataperpt=10000):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    wmin, wmax = env.range()\n",
    "\n",
    "    for ndata in map(ceil, np.logspace(minexp, maxexp, numpts)):\n",
    "        estimates=[]\n",
    "        for i in range(1, ndataperpt+1):\n",
    "            (truevalue, data) = env.sample(ndata)\n",
    "            try:\n",
    "                estimate = None\n",
    "                estimate = method(data=data, wmin=wmin, wmax=wmax)\n",
    "                assert np.isfinite(estimate)\n",
    "            except:\n",
    "                print('truevalue was {}'.format(truevalue))\n",
    "                print('data was {}'.format(data))\n",
    "                print('estimate was {}'.format(estimate))\n",
    "                raise\n",
    "            \n",
    "            essden = sum(c*w*w for (c, w, _) in data)\n",
    "            essnum = sum(c*w for (c, w, _) in data)\n",
    "            ess = 0 if essden == 0 else essnum*(essnum/essden)\n",
    "                                                \n",
    "            estimates.append(\n",
    "                ( truevalue,\n",
    "                  truevalue - estimate,\n",
    "                  (truevalue - estimate)**2,\n",
    "                 ess\n",
    "                )  \n",
    "            )\n",
    "            \n",
    "        yield (ndata,\n",
    "                { \n",
    "                    'bias': np.abs(np.mean([ x[1] for x in estimates])),\n",
    "                    'biasstd': np.std([ x[1] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'mse': np.mean([ x[2] for x in estimates ]),\n",
    "                    'msestd': np.std( [ x[2] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                    'ess': np.mean([ x[3] for x in estimates ]),\n",
    "                    'essstd': np.std([ x[3] for x in estimates ], ddof=1) / np.sqrt(len(estimates) - 1),\n",
    "                },\n",
    "              )\n",
    " \n",
    "class ClippedDR:\n",
    "    @staticmethod\n",
    "    def estimate(data, baseline=0.5, **kwargs):\n",
    "        import numpy as np\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        return baseline if n == 0 else np.clip(sum(c*w*(r-baseline)+c*baseline for c, w, r in data) / n, a_min=0, a_max=1)\n",
    "    \n",
    "class SNIPS:\n",
    "    @staticmethod\n",
    "    def estimate(data, **kwargs):\n",
    "        effn = sum(c*w for c, w, _ in data)\n",
    "        return 0.5 if effn == 0 else sum(c*w*r for c, w, r in data) / effn\n",
    "\n",
    "class Euclidean:\n",
    "    @staticmethod\n",
    "    def estimate(data, wmin, wmax, **kwargs):\n",
    "        n = sum(c for c, _, _ in data)\n",
    "        barw = sum(c*w for c, w, _ in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, _ in data) / n\n",
    "        barwr = sum(c*w*r for c, w, r in data) / n\n",
    "        barwsqr = sum(c*w*w*r for c, w, r in data) / n\n",
    "        barwsq = sum(c*w*w for c, w, r in data) / n\n",
    "        \n",
    "        data = None # sufficient statistics only (!)\n",
    "\n",
    "        wextreme = wmin if barw > 1 else wmax\n",
    "        denom = barwsq - 2 * wextreme * barw + wextreme * wextreme\n",
    "\n",
    "        betastarovern = (barw - 1) / denom\n",
    "        gammastarovern = -betastarovern * wextreme\n",
    "        estimate = max(0, min(1, barwr - gammastarovern * barwr - betastarovern * barwsqr))\n",
    "        missing = 1 - max(0, min(1, barw - gammastarovern * barw - betastarovern * barwsq))\n",
    "\n",
    "#         factor = (barw - 1) / denom\n",
    "#         estimate = sum(c*w*r*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n\n",
    "#         missing = max(0, 1 - sum(c*w*max(0, 1 - factor*(w - wextreme)) for c, w, r in data) / n)\n",
    "\n",
    "        return estimate + 0.5 * missing\n",
    "     \n",
    "from importlib import reload\n",
    "import environments.ControlledRangeVariance\n",
    "import MLE.MLE\n",
    "\n",
    "reload(environments.ControlledRangeVariance)\n",
    "reload(MLE.MLE)\n",
    "\n",
    "from MLE.MLE import CressieRead as CressieRead\n",
    "\n",
    "def getenv():\n",
    "    wsupport = [ 0, 2, 1000 ]\n",
    "    env = environments.ControlledRangeVariance.ControlledRangeVariance(seed=45, wsupport=wsupport, expwsq=100)\n",
    "    return env, env.getpw(), env.range(), env.expectedwsq()\n",
    "\n",
    "allres = []\n",
    "for (name, method) in [ \n",
    "#                         ('Constant 0.5', lambda **kwargs: 0.5),\n",
    "#                         ('ClippedDR', ClippedDR.estimate),\n",
    "#                         ('SNIPS', SNIPS.estimate),\n",
    "                        ('Euclidean', Euclidean.estimate),\n",
    "                        ('TwoThirds', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=2/3)[0]),\n",
    "                        ('MinusOneHalf', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=-1/2)[0]),\n",
    "                        ('One', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=1)[0]),\n",
    "                        ('Almost MLE', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=1e-3)[0]),\n",
    "                        ('MLE', lambda data, **kwargs: MLE.MLE.estimate(datagen=lambda: data, **kwargs)[0]),\n",
    "                        ('Lambda Zero', lambda data, **kwargs: CressieRead.estimate(datagen=lambda: data, **kwargs, lam=0)[0]),\n",
    "                      ]:\n",
    "    print('****** {} ******'.format(name))\n",
    "    res = []\n",
    "    for zzz in produceresults(getenv()[0], method, numpts=14, ndataperpt=10000):\n",
    "        res.append(zzz)\n",
    "#         print('{}'.format(zzz), flush=True)\n",
    "    wmax = getenv()[2][1]\n",
    "    allres.append((name, [(x[0] / wmax, x[1]) for x in res]))\n",
    "    del wmax\n",
    "import pickle\n",
    "pickle.dump( allres, open( \"epsilongreedy_estimate_euclideanres.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "class FlassPlot:\n",
    "    @staticmethod\n",
    "    def pic(x, y, label):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.loglog(x, y, label=label)\n",
    "        plt.legend()\n",
    "        \n",
    "    @staticmethod\n",
    "    def forpaper():\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        LEGEND_SIZE = 12\n",
    "        SMALL_SIZE = 16\n",
    "        MEDIUM_SIZE = 22\n",
    "        BIGGER_SIZE = 24\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=LEGEND_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "        \n",
    "    @staticmethod\n",
    "    def axeslabel(xlabel, ylabel):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    @staticmethod\n",
    "    def title(title):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.title(title)\n",
    "        \n",
    "    @staticmethod\n",
    "    def savefig(filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    @staticmethod\n",
    "    def plt():\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        return plt\n",
    "  \n",
    "import pickle\n",
    "allres = pickle.load(open( \"epsilongreedy_estimate_euclideanres.p\", \"rb\" ) )\n",
    "\n",
    "renameit = { }\n",
    "skip = { 'Almost MLE': 1 }\n",
    "FlassPlot.forpaper()\n",
    "for name, res in allres:\n",
    "    if name in skip:\n",
    "        continue\n",
    "    x = [ x[0] for x in res ]\n",
    "    y = [ x[1]['mse'] for x in res ]\n",
    "    ylo = [ x[1]['mse'] - 1.96 * x[1]['msestd'] for x in res ]\n",
    "    yhi = [ x[1]['mse'] + 1.96 * x[1]['msestd'] for x in res ]\n",
    "    FlassPlot.plt().loglog([ x[0] for x in res ], [ x[1]['mse'] for x in res ], label=renameit.get(name, name))\n",
    "    FlassPlot.plt().fill_between(x, ylo, yhi, alpha=0.7)\n",
    "FlassPlot.plt().legend()\n",
    "\n",
    "FlassPlot.axeslabel('n / $w_{max}$', 'mse')\n",
    "#FlassPlot.plt().savefig(\"epsilongreedy_mse.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
